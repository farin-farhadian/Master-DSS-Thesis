{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ba62ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating the scikit-learn package to the latest version\n",
    "!pip install scikit-learn --upgrade\n",
    "# Importing Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596c464a",
   "metadata": {},
   "source": [
    "# Import Data from all Sites with Different Nominal Power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f64ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_50 = pd.read_excel(r'C:\\Users\\Farin\\Documents\\Tilburg_University\\00_Thesis_start\\DB\\s1_50MW.xlsx').drop(index=0)\n",
    "df_130 = pd.read_excel(r'C:\\Users\\Farin\\Documents\\Tilburg_University\\00_Thesis_start\\DB\\s2_130MW.xlsx').drop(index=0)\n",
    "df_35 = pd.read_excel(r'C:\\Users\\Farin\\Documents\\Tilburg_University\\00_Thesis_start\\DB\\s6_35MW.xlsx').drop(index=0)\n",
    "df_30 = pd.read_excel(r'C:\\Users\\Farin\\Documents\\Tilburg_University\\00_Thesis_start\\DB\\s8_30MW.xlsx').drop(index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c42ec1",
   "metadata": {},
   "source": [
    "# Explanatory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a0e2ed",
   "metadata": {},
   "source": [
    "Rename Columns,\n",
    "Replace <NULL> with NaN,\n",
    "Change Data types,\n",
    "Remove records with missing values for all features,\n",
    "Remove records with values such as -99 and --.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb91e581",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in [df_50, df_130, df_35, df_30]:\n",
    "        dataset.columns=['time','TSI','DNI','GHI','Air_T','Air_P','Air_H','Power(MW)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad3032c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_50.replace('<NULL>', 'NaN', inplace=True)\n",
    "df_130.replace('<NULL>', 'NaN', inplace=True)\n",
    "df_35.replace('<NULL>', 'NaN', inplace=True)\n",
    "df_30.replace('<NULL>', 'NaN', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87c6b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in [df_50, df_130, df_35, df_30]:\n",
    "    for tile in ['TSI','DNI','GHI','Air_T','Air_P','Air_H','Power(MW)']:\n",
    "        dataset[tile]= dataset[tile].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ec25a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_50['time'] = pd.to_datetime(df_50['time'], errors='coerce')\n",
    "df_130['time'] = pd.to_datetime(df_130['time'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0082f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_35.dropna(how='any', inplace=True)# Drop rows where 'TSI' is -99 using the index\n",
    "df_50 = df_50.drop(df_50[df_50['TSI'] == -99].index)\n",
    "df_130 = df_130.drop(df_130[df_130['TSI'] == -99].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112c2ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where 'TSI' is -99 using the index\n",
    "df_50 = df_50.drop(df_50[df_50['TSI'] == -99].index)\n",
    "df_130 = df_130.drop(df_130[df_130['TSI'] == -99].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403c6b6f",
   "metadata": {},
   "source": [
    "# Scatter Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508a8377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# List of datasets\n",
    "datasets = [df_50, df_130, df_30, df_30]  # Assuming you want to display all four datasets\n",
    "dataset_names = ['df_50', 'df_130', 'df_30', 'df_30']\n",
    "\n",
    "# Set up the seaborn style for aesthetics\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Define the number of rows and columns for the subplot grid\n",
    "num_rows = 7  # Number of features (rows)\n",
    "num_columns = 4  # Number of datasets (columns)\n",
    "\n",
    "# Create a large figure with subplots\n",
    "fig, axes = plt.subplots(nrows=num_rows, ncols=num_columns, figsize=(20, 30))\n",
    "\n",
    "# Define colors for each dataset (red, blue, green, yellow)\n",
    "colors = ['red', 'blue', 'green', 'pink']  # Explicit colors for datasets\n",
    "\n",
    "# Loop through the datasets (columns)\n",
    "for col_idx, (dataset, dataset_name) in enumerate(zip(datasets, dataset_names)):\n",
    "    # Get the list of feature columns (assuming all columns except 'Power(MW)' are features)\n",
    "    features = dataset.columns[dataset.columns != 'Power(MW)']\n",
    "    \n",
    "    # Loop through each feature (row)\n",
    "    for row_idx, feature in enumerate(features):\n",
    "        ax = axes[row_idx, col_idx]  # Get the correct axis in the grid\n",
    "        \n",
    "        # Plot the scatter plot for the feature vs Power(MW) for the current dataset\n",
    "        sns.scatterplot(x=dataset[feature], y=dataset['Power(MW)'], \n",
    "                        ax=ax, color=colors[col_idx], s=80, label=dataset_name)\n",
    "        \n",
    "        # Set title and axis labels\n",
    "        ax.set_title(f'{feature} vs Power(MW)', fontsize=10)\n",
    "        ax.set_xlabel(feature, fontsize=8)\n",
    "        ax.set_ylabel('Power(MW)', fontsize=8)\n",
    "        \n",
    "        # Adjust font size for axis ticks\n",
    "        ax.tick_params(axis='both', which='major', labelsize=8)\n",
    "\n",
    "# Adjust layout to avoid overlap and make it readable\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot as a PNG file\n",
    "plt.savefig('scatter_plots_by_dataset2.png', dpi=300)  # Save with 300 dpi for high quality\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618465c6",
   "metadata": {},
   "source": [
    "# Box Plot for Site_50MV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f40175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the features to plot\n",
    "feature_names = ['TSI', 'DNI', 'GHI', 'Air_T', 'Air_P', 'Air_H', 'Power(MW)']\n",
    "\n",
    "# Enhanced Box plots\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Create the boxplot with additional styling\n",
    "boxprops = dict(linestyle='-', linewidth=2, color='navy')\n",
    "medianprops = dict(linestyle='-', linewidth=2, color='firebrick')\n",
    "whiskerprops = dict(linestyle='--', linewidth=2, color='black')\n",
    "capprops = dict(linestyle='-', linewidth=2, color='grey')\n",
    "\n",
    "# Create the boxplot\n",
    "boxplot = plt.boxplot([df_50[feature] for feature in feature_names], \n",
    "                      labels=feature_names, \n",
    "                      notch=True, \n",
    "                      patch_artist=True, \n",
    "                      boxprops=boxprops, \n",
    "                      medianprops=medianprops, \n",
    "                      whiskerprops=whiskerprops, \n",
    "                      capprops=capprops)\n",
    "\n",
    "# Adding grid, title and labels\n",
    "plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7, axis='y')\n",
    "plt.title('Feature Distribution - Boxplot', fontsize=18, fontweight='bold')\n",
    "plt.xlabel('Features', fontsize=16)\n",
    "plt.ylabel('Values', fontsize=16)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "# Remove spines\n",
    "for spine in plt.gca().spines.values():\n",
    "    spine.set_visible(False)\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('Feature_boxplot_50.pdf', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837d041c",
   "metadata": {},
   "source": [
    "# Box Plot for Site_130MV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aab6616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the features to plot\n",
    "feature_names = ['TSI', 'DNI', 'GHI', 'Air_T', 'Air_P', 'Air_H', 'Power(MW)']\n",
    "\n",
    "# Enhanced Box plots\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Create the boxplot with additional styling\n",
    "boxprops = dict(linestyle='-', linewidth=2, color='navy')\n",
    "medianprops = dict(linestyle='-', linewidth=2, color='firebrick')\n",
    "whiskerprops = dict(linestyle='--', linewidth=2, color='black')\n",
    "capprops = dict(linestyle='-', linewidth=2, color='grey')\n",
    "\n",
    "# Create the boxplot\n",
    "boxplot = plt.boxplot([df_130[feature] for feature in feature_names], \n",
    "                      labels=feature_names, \n",
    "                      notch=True, \n",
    "                      patch_artist=True, \n",
    "                      boxprops=boxprops, \n",
    "                      medianprops=medianprops, \n",
    "                      whiskerprops=whiskerprops, \n",
    "                      capprops=capprops)\n",
    "\n",
    "# Adding grid, title and labels\n",
    "plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7, axis='y')\n",
    "plt.title('Feature Distribution - Boxplot', fontsize=18, fontweight='bold')\n",
    "plt.xlabel('Features', fontsize=16)\n",
    "plt.ylabel('Values', fontsize=16)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "# Remove spines\n",
    "for spine in plt.gca().spines.values():\n",
    "    spine.set_visible(False)\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('Feature_boxplot_130.pdf', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ed4c06",
   "metadata": {},
   "source": [
    " # Box Plot for Site_30MV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347b75ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the features to plot\n",
    "feature_names = ['TSI', 'DNI', 'GHI', 'Air_T', 'Air_P', 'Air_H', 'Power(MW)']\n",
    "\n",
    "# Enhanced Box plots\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Create the boxplot with additional styling\n",
    "boxprops = dict(linestyle='-', linewidth=2, color='navy')\n",
    "medianprops = dict(linestyle='-', linewidth=2, color='firebrick')\n",
    "whiskerprops = dict(linestyle='--', linewidth=2, color='black')\n",
    "capprops = dict(linestyle='-', linewidth=2, color='grey')\n",
    "\n",
    "# Create the boxplot\n",
    "boxplot = plt.boxplot([df_30[feature] for feature in feature_names], \n",
    "                      labels=feature_names, \n",
    "                      notch=True, \n",
    "                      patch_artist=True, \n",
    "                      boxprops=boxprops, \n",
    "                      medianprops=medianprops, \n",
    "                      whiskerprops=whiskerprops, \n",
    "                      capprops=capprops)\n",
    "\n",
    "# Adding grid, title and labels\n",
    "plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7, axis='y')\n",
    "plt.title('Feature Distribution - Boxplot', fontsize=18, fontweight='bold')\n",
    "plt.xlabel('Features', fontsize=16)\n",
    "plt.ylabel('Values', fontsize=16)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "# Remove spines\n",
    "for spine in plt.gca().spines.values():\n",
    "    spine.set_visible(False)\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('Feature_boxplot_30.pdf', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c279934b",
   "metadata": {},
   "source": [
    " # Box Plot for Site_35MV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae30b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the features to plot\n",
    "feature_names = ['TSI', 'DNI', 'GHI', 'Air_T', 'Air_P', 'Air_H', 'Power(MW)']\n",
    "\n",
    "# Enhanced Box plots\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Create the boxplot with additional styling\n",
    "boxprops = dict(linestyle='-', linewidth=2, color='navy')\n",
    "medianprops = dict(linestyle='-', linewidth=2, color='firebrick')\n",
    "whiskerprops = dict(linestyle='--', linewidth=2, color='black')\n",
    "capprops = dict(linestyle='-', linewidth=2, color='grey')\n",
    "\n",
    "# Create the boxplot\n",
    "boxplot = plt.boxplot([df_35[feature] for feature in feature_names], \n",
    "                      labels=feature_names, \n",
    "                      notch=True, \n",
    "                      patch_artist=True, \n",
    "                      boxprops=boxprops, \n",
    "                      medianprops=medianprops, \n",
    "                      whiskerprops=whiskerprops, \n",
    "                      capprops=capprops)\n",
    "\n",
    "# Adding grid, title and labels\n",
    "plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7, axis='y')\n",
    "plt.title('Feature Distribution - Boxplot', fontsize=18, fontweight='bold')\n",
    "plt.xlabel('Features', fontsize=16)\n",
    "plt.ylabel('Values', fontsize=16)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "# Remove spines\n",
    "for spine in plt.gca().spines.values():\n",
    "    spine.set_visible(False)\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('Feature_boxplot_35.pdf', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae412bd3",
   "metadata": {},
   "source": [
    "# Correlation_heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aed7fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# List of DataFrames and their respective titles\n",
    "dataframes = [df_50, df_130, df_30, df_35]\n",
    "titles = ['df_50 Correlation Matrix', 'df_130 Correlation Matrix', 'df_30 Correlation Matrix', 'df_35 Correlation Matrix']\n",
    "\n",
    "# List of features to consider for correlation\n",
    "features = ['TSI', 'DNI', 'GHI', 'Air_T', 'Air_P', 'Air_H', 'Power(MW)']\n",
    "\n",
    "# Create subplots for each DataFrame\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axs = axs.flatten()\n",
    "\n",
    "for i, (df, title) in enumerate(zip(dataframes, titles)):\n",
    "    # Calculate the correlation matrix\n",
    "    correlation_matrix = df[features].corr()\n",
    "\n",
    "    # Plot the correlation matrix\n",
    "    cax = axs[i].imshow(correlation_matrix, cmap='coolwarm', vmax=1, vmin=-1)\n",
    "    axs[i].set_title(title)\n",
    "    axs[i].set_xticks(range(len(correlation_matrix.columns)))\n",
    "    axs[i].set_xticklabels(correlation_matrix.columns, rotation=90)\n",
    "    axs[i].set_yticks(range(len(correlation_matrix.columns)))\n",
    "    axs[i].set_yticklabels(correlation_matrix.columns)\n",
    "\n",
    "    # Add color bar\n",
    "    plt.colorbar(cax, ax=axs[i], fraction=0.046, pad=0.04)\n",
    "\n",
    "    # Add correlation values in the matrix\n",
    "    for j in range(len(correlation_matrix.columns)):\n",
    "        for k in range(len(correlation_matrix.columns)):\n",
    "            text = axs[i].text(k, j, np.around(correlation_matrix.iloc[j, k], decimals=2),\n",
    "                               ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.savefig('Correlation_heatmap_before.pdf', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9665d9",
   "metadata": {},
   "source": [
    "# Resamples Data Frames(with Navg=2) for all Sites with Different Nominal Power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c8cd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming your data frames are named df_50, df_130, df_35, df_30\n",
    "dataframes = [df_50, df_130, df_35, df_30]\n",
    "\n",
    "# Function to resample and average the data frames\n",
    "def resample_and_average(df):\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    df.set_index('time', inplace=True)\n",
    "    resampled_df = df.resample('30T').mean()\n",
    "    return resampled_df\n",
    "\n",
    "# Apply the function to each data frame\n",
    "resampled_dfs = [resample_and_average(df) for df in dataframes]\n",
    "\n",
    "# If you want to access each resampled data frame individually\n",
    "resampled_df_50, resampled_df_130, resampled_df_35, resampled_df_30 = resampled_dfs\n",
    "\n",
    "# Example: Print the resampled df_50\n",
    "print(resampled_df_50)\n",
    "\n",
    "resampled_df_50 = resampled_df_50.fillna(method='ffill')\n",
    "resampled_df_130 = resampled_df_50.fillna(method='ffill')\n",
    "resampled_df_35 = resampled_df_50.fillna(method='ffill')\n",
    "resampled_df_30 = resampled_df_50.fillna(method='ffill')\n",
    "\n",
    "\n",
    "# Assuming the dataframes are already defined\n",
    "resampled_df_50.to_csv('resampled_df_50_time.csv', index=True)\n",
    "resampled_df_130.to_csv('resampled_df_130_time.csv', index=True)\n",
    "resampled_df_35.to_csv('resampled_df_35_time.csv', index=True)\n",
    "resampled_df_30.to_csv('resampled_df_30_time.csv', index=True)\n",
    "\n",
    "print(\"Dataframes have been successfully exported to CSV files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefd0bb3",
   "metadata": {},
   "source": [
    "# Spilit Each Resample data for all Sites to Train, Validation and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0e8e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to split the data into train, validation, and test sets\n",
    "def split_data(df, train_size=0.7, val_size=0.1, test_size=0.2):\n",
    "    # Ensure the sizes sum to 1\n",
    "    assert train_size + val_size + test_size == 1, \"Train, validation, and test sizes must sum to 1\"\n",
    "    \n",
    "    # Calculate the number of samples for each set\n",
    "    n = len(df)\n",
    "    train_end = int(train_size * n)\n",
    "    val_end = train_end + int(val_size * n)\n",
    "    \n",
    "    # Split the data\n",
    "    train = df[:train_end]\n",
    "    val = df[train_end:val_end]\n",
    "    test = df[val_end:]\n",
    "    \n",
    "    return train, val, test\n",
    "\n",
    "# Assuming your resampled data frames are named resampled_df_50, resampled_df_130, resampled_df_35, resampled_df_30\n",
    "resampled_dataframes = [resampled_df_50, resampled_df_130, resampled_df_35, resampled_df_30]\n",
    "\n",
    "# Apply the function to each resampled data frame\n",
    "split_resampled_dfs = [split_data(df) for df in resampled_dataframes]\n",
    "\n",
    "# Corrected unpacking of the split data frames\n",
    "(train_resampled_df_50, val_resampled_df_50, test_resampled_df_50), \\\n",
    "(train_resampled_df_130, val_resampled_df_130, test_resampled_df_130), \\\n",
    "(train_resampled_df_35, val_resampled_df_35, test_resampled_df_35), \\\n",
    "(train_resampled_df_30, val_resampled_df_30, test_resampled_df_30) = split_resampled_dfs\n",
    "\n",
    "# Example: Print the shapes of the split resampled_df_50\n",
    "print(\"Train shape:\", train_resampled_df_50.shape)\n",
    "print(\"Validation shape:\", val_resampled_df_50.shape)\n",
    "print(\"Test shape:\", test_resampled_df_50.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40135799",
   "metadata": {},
   "source": [
    "# Scaling Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ba6f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing mean and normalizing deviation of the training data for df_50\n",
    "max_train_50 = train_resampled_df_50.max(axis=0)\n",
    "min_train_50 = train_resampled_df_50.min(axis=0)\n",
    "\n",
    "# Normalizing training, validation, and test data for df_50\n",
    "normalized_train_df_50 = (train_resampled_df_50 - min_train_50) / (max_train_50 - min_train_50)\n",
    "normalized_val_df_50 = (val_resampled_df_50 - min_train_50) / (max_train_50 - min_train_50)\n",
    "normalized_test_df_50 = (test_resampled_df_50 - min_train_50) / (max_train_50 - min_train_50)\n",
    "\n",
    "# Computing mean and normalizing deviation of the training data for df_130\n",
    "max_train_130 = train_resampled_df_130.max(axis=0)\n",
    "min_train_130 = train_resampled_df_130.min(axis=0)\n",
    "\n",
    "# Normalizing training, validation, and test data for df_130\n",
    "normalized_train_df_130 = (train_resampled_df_130 - min_train_130) / (max_train_130 - min_train_130)\n",
    "normalized_val_df_130 = (val_resampled_df_130 - min_train_130) / (max_train_130 - min_train_130)\n",
    "normalized_test_df_130 = (test_resampled_df_130 - min_train_130) / (max_train_130 - min_train_130)\n",
    "\n",
    "# Computing mean and normalizing deviation of the training data for df_30\n",
    "max_train_30 = train_resampled_df_30.max(axis=0)\n",
    "min_train_30 = train_resampled_df_30.min(axis=0)\n",
    "\n",
    "# Normalizing training, validation, and test data for df_30\n",
    "normalized_train_df_30 = (train_resampled_df_30 - min_train_30) / (max_train_30 - min_train_30)\n",
    "normalized_val_df_30 = (val_resampled_df_30 - min_train_30) / (max_train_30 - min_train_30)\n",
    "normalized_test_df_30 = (test_resampled_df_30 - min_train_30) / (max_train_30 - min_train_30)\n",
    "\n",
    "# Computing mean and normalizing deviation of the training data for df_35\n",
    "max_train_35 = train_resampled_df_35.max(axis=0)\n",
    "min_train_35 = train_resampled_df_35.min(axis=0)\n",
    "\n",
    "# Normalizing training, validation, and test data for df_35\n",
    "normalized_train_df_35 = (train_resampled_df_35 - min_train_35) / (max_train_35 - min_train_35)\n",
    "normalized_val_df_35 = (val_resampled_df_35 - min_train_35) / (max_train_35 - min_train_35)\n",
    "normalized_test_df_35 = (test_resampled_df_35 - min_train_35) / (max_train_35 - min_train_35)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2c6e14",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de6642f",
   "metadata": {},
   "source": [
    "# Grid Search for Site_50MV on Train and Validatian Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb59fabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Function to create look-back features for time series forecasting\n",
    "def create_lookback_features(df, target_column, lookback_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - lookback_steps):\n",
    "        X.append(df.iloc[i:i + lookback_steps].values)\n",
    "        y.append(df.iloc[i + lookback_steps][target_column])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Function to calculate and print R squared and RMSE\n",
    "def calculate_metrics(y_true_train, y_pred_train, y_true_val, y_pred_val, title):\n",
    "    r2_train = r2_score(y_true_train, y_pred_train)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_true_train, y_pred_train))\n",
    "    \n",
    "    r2_val = r2_score(y_true_val, y_pred_val)\n",
    "    rmse_val = np.sqrt(mean_squared_error(y_true_val, y_pred_val))\n",
    "    \n",
    "    print(f\"{title} - Train R squared: {r2_train}, RMSE: {rmse_train}\")\n",
    "    print(f\"{title} - Validation R squared: {r2_val}, RMSE: {rmse_val}\")\n",
    "\n",
    "# Function to plot the results of Grid Search\n",
    "def plot_search_results(grid, dataset_name):\n",
    "    \"\"\"\n",
    "    Params: \n",
    "        grid: A trained GridSearchCV object.\n",
    "        dataset_name: A string representing the name of the dataset (e.g., 'Dataset 50').\n",
    "    \"\"\"\n",
    "    # Results from grid search\n",
    "    results = grid.cv_results_\n",
    "    means_test = np.sqrt(-results['mean_test_score'])\n",
    "    stds_test = np.sqrt(results['std_test_score'])\n",
    "    means_train = np.sqrt(-results['mean_train_score'])\n",
    "    stds_train = np.sqrt(results['std_train_score'])\n",
    "\n",
    "    # Getting indexes of values per hyper-parameter\n",
    "    masks = []\n",
    "    masks_names = list(grid.best_params_.keys())\n",
    "    for p_k, p_v in grid.best_params_.items():\n",
    "        masks.append(list(results['param_' + p_k].data == p_v))\n",
    "\n",
    "    params = grid.param_grid\n",
    "\n",
    "    # Plotting results\n",
    "    fig, ax = plt.subplots(1, len(params), sharex='none', sharey='all', figsize=(20, 5))\n",
    "    fig.suptitle(f'Random Forest Hyperparameter Tuning Results ({dataset_name})')\n",
    "    fig.text(0.04, 0.5, 'MEAN RMSE', va='center', rotation='vertical')\n",
    "    for i, p in enumerate(masks_names):\n",
    "        m = np.stack(masks[:i] + masks[i + 1:])\n",
    "        best_params_mask = m.all(axis=0)\n",
    "        best_index = np.where(best_params_mask)[0]\n",
    "        x = np.array(params[p])\n",
    "        y_1 = np.array(means_test[best_index])\n",
    "        e_1 = np.array(stds_test[best_index])\n",
    "        y_2 = np.array(means_train[best_index])\n",
    "        e_2 = np.array(stds_train[best_index])\n",
    "        ax[i].errorbar(x, y_1, e_1, linestyle='--', marker='o', label='Validation')\n",
    "        ax[i].errorbar(x, y_2, e_2, linestyle='-', marker='^', label='Train')\n",
    "        ax[i].set_xlabel(p.upper())\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Function to train and evaluate the Random Forest model with Grid Search\n",
    "def train_evaluate_rf_grid_search(train_df, val_df, target_column='Power(MW)', lookback_steps=8, dataset_name='Dataset'):\n",
    "    # Create look-back features\n",
    "    X_train, y_train = create_lookback_features(train_df, target_column, lookback_steps)\n",
    "    X_val, y_val = create_lookback_features(val_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape X to 2D array for RandomForestRegressor\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    X_val = X_val.reshape(X_val.shape[0], -1)\n",
    "    \n",
    "    # Define the parameter grid for Grid Search\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [10, 20, 30],\n",
    "        'min_samples_leaf': [2, 4, 6],\n",
    "        'bootstrap': [True, False]\n",
    "    }\n",
    "    \n",
    "    # Initialize the Random Forest model\n",
    "    rf_model = RandomForestRegressor()\n",
    "    \n",
    "    # Perform Grid Search with cross-validation\n",
    "    grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid,\n",
    "                               cv=3, n_jobs=-1, scoring='neg_mean_squared_error', return_train_score=True)\n",
    "    \n",
    "    # Fit the Grid Search model\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best parameters from Grid Search\n",
    "    best_params = grid_search.best_params_\n",
    "    print(f\"Best parameters from Grid Search for {dataset_name}: {best_params}\")\n",
    "    \n",
    "    # Train the Random Forest model with the best parameters\n",
    "    best_rf_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Predict on train and validation sets\n",
    "    y_pred_train = best_rf_model.predict(X_train)\n",
    "    y_pred_val = best_rf_model.predict(X_val)\n",
    "    \n",
    "    # Calculate and print R squared and RMSE\n",
    "    calculate_metrics(y_train, y_pred_train, y_val, y_pred_val, title=f'Random Forest with Grid Search ({dataset_name})')\n",
    "    \n",
    "    # Plot the results of Grid Search\n",
    "    plot_search_results(grid_search, dataset_name)\n",
    "\n",
    "# List of split data frames (train_df, val_df) and their corresponding dataset names\n",
    "split_resampled_dfs = [\n",
    "    (normalized_train_df_50, normalized_val_df_50, 'Dataset 50')\n",
    "]\n",
    "\n",
    "# Loop through each pair of train and validation dataframes\n",
    "for i, (train_df, val_df, dataset_name) in enumerate(split_resampled_dfs):\n",
    "    print(f\"Evaluating Random Forest model with Grid Search for {dataset_name}\")\n",
    "    train_evaluate_rf_grid_search(train_df=train_df,\n",
    "                                  val_df=val_df,\n",
    "                                  target_column='Power(MW)',\n",
    "                                  lookback_steps=8,\n",
    "                                  dataset_name=dataset_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e38b4a",
   "metadata": {},
   "source": [
    "# Build Random Forest model with best combination of Hyperparameters, then forcast On Test Data for all sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762c5586",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Function to create look-back features for time series forecasting\n",
    "def create_lookback_features(df, target_column, lookback_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - lookback_steps):\n",
    "        X.append(df.iloc[i:i + lookback_steps].values)\n",
    "        y.append(df.iloc[i + lookback_steps][target_column])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Function to calculate and print R squared and RMSE for train and test\n",
    "def calculate_metrics(y_true_train, y_pred_train, y_true_test, y_pred_test, title):\n",
    "    r2_train = r2_score(y_true_train, y_pred_train)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_true_train, y_pred_train))\n",
    "    \n",
    "    r2_test = r2_score(y_true_test, y_pred_test)\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_true_test, y_pred_test))\n",
    "    \n",
    "    print(f\"{title} - Train R squared: {r2_train}, RMSE: {rmse_train}\")\n",
    "    print(f\"{title} - Test R squared: {r2_test}, RMSE: {rmse_test}\")\n",
    "\n",
    "# Function to train and evaluate the Random Forest model\n",
    "def train_evaluate_rf(train_df, test_df, target_column='Power(MW)', lookback_steps=8):\n",
    "    # Create look-back features\n",
    "    X_train, y_train = create_lookback_features(train_df, target_column, lookback_steps)\n",
    "    X_test, y_test = create_lookback_features(test_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape X to 2D array for RandomForestRegressor\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "    \n",
    "    # Train the Random Forest model with specified hyperparameters\n",
    "    rf_model = RandomForestRegressor(n_estimators=100, bootstrap=True, max_depth=20, min_samples_leaf=2)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on train and test sets\n",
    "    y_pred_train = rf_model.predict(X_train)\n",
    "    y_pred_test = rf_model.predict(X_test)\n",
    "    \n",
    "    # Calculate and print R squared and RMSE for train and test\n",
    "    calculate_metrics(y_train, y_pred_train, y_test, y_pred_test, title='Random Forest')\n",
    "\n",
    "# List of split data frames (train and test)\n",
    "split_resampled_dfs = [\n",
    "    (normalized_train_df_50, normalized_test_df_50),\n",
    "    (normalized_train_df_130, normalized_test_df_130),\n",
    "    (normalized_train_df_35, normalized_test_df_35),\n",
    "    (normalized_train_df_30, normalized_test_df_30)\n",
    "]\n",
    "\n",
    "# Apply the Random Forest model to each set of split data frames and calculate metrics\n",
    "for i, (train_df, test_df) in enumerate(split_resampled_dfs):\n",
    "    print(f\"Evaluating Random Forest model for resampled dataset {i+1}\")\n",
    "    train_evaluate_rf(train_df=train_df,\n",
    "                      test_df=test_df,\n",
    "                      target_column='Power(MW)',\n",
    "                      lookback_steps=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d553d868",
   "metadata": {},
   "source": [
    "# Feature Importance for All Sites with Different Nominal Power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925c9847",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Function to create look-back features for time series forecasting\n",
    "def create_lookback_features(df, target_column, lookback_steps):\n",
    "    feature_names = df.columns.tolist()\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - lookback_steps):\n",
    "        X.append(df.iloc[i:i + lookback_steps].values)\n",
    "        y.append(df.iloc[i + lookback_steps][target_column])\n",
    "    \n",
    "    # Create labels for each feature to indicate time step\n",
    "    labels = [f\"{feature} (t-{lookback_steps+1-j})\" for j in range(1, lookback_steps+1) for feature in feature_names]\n",
    "    return np.array(X), np.array(y), labels\n",
    "\n",
    "# Function to calculate and print R squared and RMSE for train and test\n",
    "def calculate_metrics(y_true_train, y_pred_train, y_true_test, y_pred_test, title):\n",
    "    r2_train = r2_score(y_true_train, y_pred_train)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_true_train, y_pred_train))\n",
    "    \n",
    "    r2_test = r2_score(y_true_test, y_pred_test)\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_true_test, y_pred_test))\n",
    "    \n",
    "    print(f\"{title} - Train R squared: {r2_train}, RMSE: {rmse_train}\")\n",
    "    print(f\"{title} - Test R squared: {r2_test}, RMSE: {rmse_test}\")\n",
    "\n",
    "# Function to train and evaluate the Random Forest model\n",
    "def train_evaluate_rf(train_df, test_df, target_column='Power(MW)', lookback_steps=8):\n",
    "    # Create look-back features\n",
    "    X_train, y_train, feature_labels = create_lookback_features(train_df, target_column, lookback_steps)\n",
    "    X_test, y_test, _ = create_lookback_features(test_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape X to 2D array for RandomForestRegressor\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "    \n",
    "    # Train the Random Forest model with specified hyperparameters\n",
    "    rf_model = RandomForestRegressor(n_estimators=100, bootstrap=True, max_depth=20, min_samples_leaf=2)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on train and test sets\n",
    "    y_pred_train = rf_model.predict(X_train)\n",
    "    y_pred_test = rf_model.predict(X_test)\n",
    "    \n",
    "    # Calculate and print R squared and RMSE for train and test\n",
    "    calculate_metrics(y_train, y_pred_train, y_test, y_pred_test, title='Random Forest')\n",
    "    \n",
    "    # Plot feature importance with actual feature names indicating time step\n",
    "    importances = rf_model.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    # Create a color map for different time steps\n",
    "    cmap = plt.get_cmap('tab20')  # Use a colormap with distinct colors\n",
    "    num_features_per_step = len(train_df.columns)\n",
    "    colors = [cmap(i) for i in range(lookback_steps)]\n",
    "    bar_colors = np.array([colors[j // num_features_per_step] for j in indices])\n",
    "    \n",
    "    plt.figure(figsize=(20, 8))\n",
    "    plt.title(\"Feature Importances\")\n",
    "    bars = plt.bar(range(X_train.shape[1]), importances[indices], color=bar_colors)\n",
    "    \n",
    "    # Add a legend to indicate time steps\n",
    "    handles = [plt.Rectangle((0, 0), 1, 1, color=cmap(i)) for i in range(lookback_steps)]\n",
    "    labels = [f't-{lookback_steps+1-j}' for j in range(lookback_steps)]\n",
    "    plt.legend(handles, labels)\n",
    "    \n",
    "    plt.xticks(range(X_train.shape[1]), [feature_labels[i] for i in indices], rotation=90)\n",
    "    plt.xlim([-1, X_train.shape[1]])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# List of split data frames (train and test)\n",
    "split_resampled_dfs = [\n",
    "    (normalized_train_df_50, normalized_test_df_50),\n",
    "    (normalized_train_df_130, normalized_test_df_130),\n",
    "    (normalized_train_df_35, normalized_test_df_35),\n",
    "    (normalized_train_df_30, normalized_test_df_30)\n",
    "]\n",
    "\n",
    "# Apply the Random Forest model to each set of split data frames and calculate metrics\n",
    "for i, (train_df, test_df) in enumerate(split_resampled_dfs):\n",
    "    print(f\"Evaluating Random Forest model for resampled dataset {i+1}\")\n",
    "    train_evaluate_rf(train_df, test_df, target_column='Power(MW)', lookback_steps=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00670fbf",
   "metadata": {},
   "source": [
    "# Residual Plot for Actual And Predicted For All Sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcab2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Function to create look-back features for time series forecasting\n",
    "def create_lookback_features(df, target_column, lookback_steps):\n",
    "    feature_names = df.columns.tolist()\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - lookback_steps):\n",
    "        X.append(df.iloc[i:i + lookback_steps].values)\n",
    "        y.append(df.iloc[i + lookback_steps][target_column])\n",
    "    \n",
    "    # Create labels for each feature to indicate time step\n",
    "    labels = [f\"{feature} (t-{lookback_steps+1-j})\" for j in range(1, lookback_steps+1) for feature in feature_names]\n",
    "    return np.array(X), np.array(y), labels\n",
    "\n",
    "# Function to calculate and print R squared and RMSE for train and test\n",
    "def calculate_metrics(y_true_train, y_pred_train, y_true_test, y_pred_test, title):\n",
    "    r2_train = r2_score(y_true_train, y_pred_train)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_true_train, y_pred_train))\n",
    "    \n",
    "    r2_test = r2_score(y_true_test, y_pred_test)\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_true_test, y_pred_test))\n",
    "    \n",
    "    print(f\"{title} - Train R squared: {r2_train}, RMSE: {rmse_train}\")\n",
    "    print(f\"{title} - Test R squared: {r2_test}, RMSE: {rmse_test}\")\n",
    "\n",
    "# Function to train and evaluate the Random Forest model\n",
    "def train_evaluate_rf(train_df, test_df, target_column='Power(MW)', lookback_steps=8):\n",
    "    # Create look-back features\n",
    "    X_train, y_train, feature_labels = create_lookback_features(train_df, target_column, lookback_steps)\n",
    "    X_test, y_test, _ = create_lookback_features(test_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape X to 2D array for RandomForestRegressor\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "    \n",
    "    # Train the Random Forest model with specified hyperparameters\n",
    "    rf_model = RandomForestRegressor(n_estimators=100, bootstrap=True, max_depth=20, min_samples_leaf=2)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on train and test sets\n",
    "    y_pred_train = rf_model.predict(X_train)\n",
    "    y_pred_test = rf_model.predict(X_test)\n",
    "    \n",
    "    # Calculate and print R squared and RMSE for train and test\n",
    "    calculate_metrics(y_train, y_pred_train, y_test, y_pred_test, title='Random Forest')\n",
    "    \n",
    "    # Plot feature importance with actual feature names indicating time step\n",
    "    importances = rf_model.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    # Create a color map for different time steps\n",
    "    cmap = plt.get_cmap('tab20')  # Use a colormap with distinct colors\n",
    "    num_features_per_step = len(train_df.columns)\n",
    "    colors = [cmap(i) for i in range(lookback_steps)]\n",
    "    bar_colors = np.array([colors[j // num_features_per_step] for j in indices])\n",
    "    \n",
    " \n",
    "    \n",
    "    # Plot Actual vs Predicted on Test Data\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(y_test, y_pred_test, alpha=0.5, color='b', label='Test Data')\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', label='Perfect Prediction')\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.title('Actual vs Predicted on Test Data - Site_50MV')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Apply the Random Forest model to each set of split data frames and calculate metrics\n",
    "split_resampled_data = [\n",
    "    (normalized_train_df_50, normalized_test_df_50),\n",
    "    (normalized_train_df_130, normalized_test_df_130),\n",
    "    (normalized_train_df_35, normalized_test_df_35),\n",
    "    (normalized_train_df_30, normalized_test_df_30)\n",
    "]\n",
    "\n",
    "\n",
    "for i, (train_df, test_df) in enumerate(split_resampled_data):\n",
    "    print(f\"Evaluating Random Forest model for resampled dataset {i+1}\")\n",
    "    train_evaluate_rf(train_df, test_df, target_column='Power(MW)', lookback_steps=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab25c68a",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39142e6",
   "metadata": {},
   "source": [
    "# Evaluating LSTM on Validation set without Batch Normalization for site_50MV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1937ee40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Function to create look-back features for time series forecasting\n",
    "def create_lookback_features(df, target_column, lookback_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - lookback_steps):\n",
    "        X.append(df.iloc[i:i + lookback_steps].values)\n",
    "        y.append(df.iloc[i + lookback_steps][target_column])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Function to reverse normalization\n",
    "def reverse_normalization(normalized_values, min_val, max_val):\n",
    "    return normalized_values * (max_val - min_val) + min_val\n",
    "\n",
    "# Function to calculate and print R squared and RMSE\n",
    "def calculate_metrics(y_true_train, y_pred_train, y_true_val, y_pred_val, title, best_epoch=None):\n",
    "    r2_train = r2_score(y_true_train, y_pred_train)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_true_train, y_pred_train))\n",
    "    \n",
    "    r2_val = r2_score(y_true_val, y_pred_val)\n",
    "    rmse_val = np.sqrt(mean_squared_error(y_true_val, y_pred_val))\n",
    "    \n",
    "    print(f\"{title} - Train R squared: {r2_train}, RMSE: {rmse_train}\")\n",
    "    print(f\"{title} - Validation R squared: {r2_val}, RMSE: {rmse_val}\")\n",
    "    \n",
    "    if best_epoch is not None:\n",
    "        print(f\"Best Epoch: {best_epoch}\")\n",
    "\n",
    "# Function to train and evaluate the LSTM model\n",
    "def train_evaluate_lstm(train_df, val_df, target_column='Power(MW)', lookback_steps=8, epochs=100, batch_size=64, site_name='Site', min_train=None, max_train=None):\n",
    "    # Create look-back features\n",
    "    X_train, y_train = create_lookback_features(train_df, target_column, lookback_steps)\n",
    "    X_val, y_val = create_lookback_features(val_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape data for LSTM: [samples, time steps, features]\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2])\n",
    "    X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], X_val.shape[2])\n",
    "    \n",
    "    # Build the LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                   kernel_regularizer=regularizers.l2(0.001)))  # LSTM Layer with L2 regularization\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(LSTM(units=32, return_sequences=False, kernel_regularizer=regularizers.l2(0.001)))  # Second LSTM Layer\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    # Adding Dense layers with ReLU activation\n",
    "    model.add(Dense(24, activation='relu', kernel_regularizer=regularizers.l2(0.001)))  # Hidden Dense layer with ReLU\n",
    "    model.add(Dropout(0.2))  # Dropout layer after Dense layer\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))  # Output layer with Linear activation (default for regression)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    # Initialize lists to store metrics for each epoch\n",
    "    r2_train_list = []\n",
    "    rmse_train_list = []\n",
    "    r2_val_list = []  # List to store R² for validation set\n",
    "    rmse_val_list = []  # List to store RMSE for validation set\n",
    "    \n",
    "    # Initialize variables to track the best epoch\n",
    "    best_epoch_r2 = -np.inf\n",
    "    best_epoch_rmse = np.inf\n",
    "    best_epoch_r2_index = None\n",
    "    best_epoch_rmse_index = None\n",
    "    \n",
    "    # Train the model and track history\n",
    "    for epoch in range(epochs):\n",
    "        # Train for one epoch\n",
    "        history = model.fit(X_train, y_train, epochs=1, batch_size=batch_size, verbose=0,shuffle=False)\n",
    "        \n",
    "        # Get predictions for train and validation data\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_val = model.predict(X_val)\n",
    "        \n",
    "        # Reverse normalization for both predictions and true values\n",
    "        y_pred_train = reverse_normalization(y_pred_train, min_train, max_train)\n",
    "        y_pred_val = reverse_normalization(y_pred_val, min_train, max_train)\n",
    "        y_train_orig = reverse_normalization(y_train, min_train, max_train)\n",
    "        y_val_orig = reverse_normalization(y_val, min_train, max_train)\n",
    "        \n",
    "        # Calculate R-squared and RMSE for the current epoch for both train and validation data\n",
    "        r2_train = r2_score(y_train_orig, y_pred_train)\n",
    "        rmse_train = np.sqrt(mean_squared_error(y_train_orig, y_pred_train))\n",
    "        \n",
    "        r2_val = r2_score(y_val_orig, y_pred_val)\n",
    "        rmse_val = np.sqrt(mean_squared_error(y_val_orig, y_pred_val))\n",
    "        \n",
    "        # Append metrics to lists\n",
    "        r2_train_list.append(r2_train)\n",
    "        rmse_train_list.append(rmse_train)\n",
    "        r2_val_list.append(r2_val)  # Append to validation list\n",
    "        rmse_val_list.append(rmse_val)  # Append to validation list\n",
    "        \n",
    "        # Track best epoch for R² (max)\n",
    "        if r2_val > best_epoch_r2:\n",
    "            best_epoch_r2 = r2_val\n",
    "            best_epoch_r2_index = epoch\n",
    "        \n",
    "        # Track best epoch for RMSE (min)\n",
    "        if rmse_val < best_epoch_rmse:\n",
    "            best_epoch_rmse = rmse_val\n",
    "            best_epoch_rmse_index = epoch\n",
    "    \n",
    "    # Predict on the training and validation sets after training\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    \n",
    "    # Reverse normalization for both predictions and true values\n",
    "    y_pred_train = reverse_normalization(y_pred_train, min_train, max_train)\n",
    "    y_pred_val = reverse_normalization(y_pred_val, min_train, max_train)\n",
    "    y_train_orig = reverse_normalization(y_train, min_train, max_train)\n",
    "    y_val_orig = reverse_normalization(y_val, min_train, max_train)\n",
    "    \n",
    "    # Calculate and print final R squared and RMSE for train and validation sets\n",
    "    calculate_metrics(y_train_orig, y_pred_train, y_val_orig, y_pred_val, title=f'LSTM Model - {site_name}', best_epoch=best_epoch_r2_index)\n",
    "    \n",
    "    # Plot the loss, R squared, and RMSE metrics for train and validation sets\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(10, 12))\n",
    "    \n",
    "    # Plot R2 for train and validation sets\n",
    "    axs[0].plot(range(epochs), r2_train_list, label='Train R²', color='blue', linestyle='-', marker='o')\n",
    "    axs[0].plot(range(epochs), r2_val_list, label='Validation R²', color='green', linestyle='--', marker='s')\n",
    "    axs[0].set_title(f'R² Score per Epoch - {site_name}')\n",
    "    axs[0].set_xlabel('Epochs')\n",
    "    axs[0].set_ylabel('R²')\n",
    "    axs[0].legend()\n",
    "    \n",
    "    # Mark best epoch for R² and display R² value\n",
    "    axs[0].scatter(best_epoch_r2_index, best_epoch_r2, color='red', zorder=5, label=f'Best Epoch: {best_epoch_r2_index}')\n",
    "    axs[0].text(best_epoch_r2_index, best_epoch_r2, f' R²: {best_epoch_r2:.3f}', color='red', fontsize=10, verticalalignment='bottom')\n",
    "    \n",
    "    # Plot RMSE for train and validation sets\n",
    "    axs[1].plot(range(epochs), rmse_train_list, label='Train RMSE', color='red', linestyle='-', marker='x')\n",
    "    axs[1].plot(range(epochs), rmse_val_list, label='Validation RMSE', color='purple', linestyle='--', marker='v')\n",
    "    axs[1].set_title(f'RMSE per Epoch - {site_name}')\n",
    "    axs[1].set_xlabel('Epochs')\n",
    "    axs[1].set_ylabel('RMSE')\n",
    "    axs[1].legend()\n",
    "    \n",
    "    # Mark best epoch for RMSE and display RMSE value\n",
    "    axs[1].scatter(best_epoch_rmse_index, best_epoch_rmse, color='blue', zorder=5, label=f'Best Epoch: {best_epoch_rmse_index}')\n",
    "    axs[1].text(best_epoch_rmse_index, best_epoch_rmse, f' RMSE: {best_epoch_rmse:.3f}', color='blue', fontsize=10, verticalalignment='top')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming your split data frames are named as follows:\n",
    "# (train_resampled_df_50, val_resampled_df_50)\n",
    "# (train_resampled_df_130, val_resampled_df_130)\n",
    "# (train_resampled_df_35, val_resampled_df_35)\n",
    "# (train_resampled_df_30, val_resampled_df_30)\n",
    "\n",
    "# List of split data frames and site names\n",
    "split_resampled_dfs = [\n",
    "    (normalized_train_df_50, normalized_val_df_50, 'Site_50MV')\n",
    "]\n",
    "\n",
    "# Define the min and max values used for normalization\n",
    "min_train_50 = normalized_train_df_50.min().min()  # Get the minimum value from the training data\n",
    "max_train_50 = normalized_train_df_50.max().max()  # Get the maximum value from the training data\n",
    "\n",
    "# Apply the LSTM model to each set of split data frames and calculate metrics\n",
    "for i, (train_df, val_df, site_name) in enumerate(split_resampled_dfs):\n",
    "    print(f\"Evaluating LSTM model for resampled dataset {site_name} ({i+1})\")\n",
    "    train_evaluate_lstm(\n",
    "        train_df=train_df,\n",
    "        val_df=val_df,\n",
    "        target_column='Power(MW)',\n",
    "        lookback_steps=8,\n",
    "        epochs=128,\n",
    "        batch_size=64,\n",
    "        site_name=site_name,\n",
    "        min_train=min_train_50,  # Provide the min value for reversing normalization\n",
    "        max_train=max_train_50   # Provide the max value for reversing normalization\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe8e72e",
   "metadata": {},
   "source": [
    "# Evaluating LSTM on Validation set with Batch Normalization for site_50MV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490be3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "# Function to create look-back features for time series forecasting\n",
    "def create_lookback_features(df, target_column, lookback_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - lookback_steps):\n",
    "        X.append(df.iloc[i:i + lookback_steps].values)\n",
    "        y.append(df.iloc[i + lookback_steps][target_column])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Function to reverse normalization\n",
    "def reverse_normalization(normalized_values, min_val, max_val):\n",
    "    return normalized_values * (max_val - min_val) + min_val\n",
    "\n",
    "# Function to calculate and print R squared and RMSE\n",
    "def calculate_metrics(y_true_train, y_pred_train, y_true_val, y_pred_val, title, best_epoch=None):\n",
    "    r2_train = r2_score(y_true_train, y_pred_train)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_true_train, y_pred_train))\n",
    "    \n",
    "    r2_val = r2_score(y_true_val, y_pred_val)\n",
    "    rmse_val = np.sqrt(mean_squared_error(y_true_val, y_pred_val))\n",
    "    \n",
    "    print(f\"{title} - Train R squared: {r2_train}, RMSE: {rmse_train}\")\n",
    "    print(f\"{title} - Validation R squared: {r2_val}, RMSE: {rmse_val}\")\n",
    "    \n",
    "    if best_epoch is not None:\n",
    "        print(f\"Best Epoch: {best_epoch}\")\n",
    "\n",
    "# Function to train and evaluate the LSTM model\n",
    "def train_evaluate_lstm(train_df, val_df, target_column='Power(MW)', lookback_steps=8, epochs=100, batch_size=64, site_name='Site', min_train=None, max_train=None):\n",
    "    # Create look-back features\n",
    "    X_train, y_train = create_lookback_features(train_df, target_column, lookback_steps)\n",
    "    X_val, y_val = create_lookback_features(val_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape data for LSTM: [samples, time steps, features]\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2])\n",
    "    X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], X_val.shape[2])\n",
    "    \n",
    "    # Build the LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                   kernel_regularizer=regularizers.l2(0.001)))  # LSTM Layer with L2 regularization\n",
    "    model.add(BatchNormalization())  # BatchNormalization after second LSTM\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(LSTM(units=32, return_sequences=False, kernel_regularizer=regularizers.l2(0.001)))  # Second LSTM Layer\n",
    "    model.add(BatchNormalization())  # BatchNormalization after second LSTM\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    # Adding Dense layers with ReLU activation\n",
    "    model.add(Dense(24, activation='relu', kernel_regularizer=regularizers.l2(0.001)))  # Hidden Dense layer with ReLU\n",
    "    model.add(Dropout(0.2))  # Dropout layer after Dense layer\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))  # Output layer with Linear activation (default for regression)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    # Initialize lists to store metrics for each epoch\n",
    "    r2_train_list = []\n",
    "    rmse_train_list = []\n",
    "    r2_val_list = []  # List to store R² for validation set\n",
    "    rmse_val_list = []  # List to store RMSE for validation set\n",
    "    \n",
    "    # Initialize variables to track the best epoch\n",
    "    best_epoch_r2 = -np.inf\n",
    "    best_epoch_rmse = np.inf\n",
    "    best_epoch_r2_index = None\n",
    "    best_epoch_rmse_index = None\n",
    "    \n",
    "    # Train the model and track history\n",
    "    for epoch in range(epochs):\n",
    "        # Train for one epoch\n",
    "        history = model.fit(X_train, y_train, epochs=1, batch_size=batch_size, verbose=2,shuffle=False)\n",
    "        \n",
    "        # Get predictions for train and validation data\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_val = model.predict(X_val)\n",
    "        \n",
    "        # Reverse normalization for both predictions and true values\n",
    "        y_pred_train = reverse_normalization(y_pred_train, min_train, max_train)\n",
    "        y_pred_val = reverse_normalization(y_pred_val, min_train, max_train)\n",
    "        y_train_orig = reverse_normalization(y_train, min_train, max_train)\n",
    "        y_val_orig = reverse_normalization(y_val, min_train, max_train)\n",
    "        \n",
    "        # Calculate R-squared and RMSE for the current epoch for both train and validation data\n",
    "        r2_train = r2_score(y_train_orig, y_pred_train)\n",
    "        rmse_train = np.sqrt(mean_squared_error(y_train_orig, y_pred_train))\n",
    "        \n",
    "        r2_val = r2_score(y_val_orig, y_pred_val)\n",
    "        rmse_val = np.sqrt(mean_squared_error(y_val_orig, y_pred_val))\n",
    "        \n",
    "        # Append metrics to lists\n",
    "        r2_train_list.append(r2_train)\n",
    "        rmse_train_list.append(rmse_train)\n",
    "        r2_val_list.append(r2_val)  # Append to validation list\n",
    "        rmse_val_list.append(rmse_val)  # Append to validation list\n",
    "        \n",
    "        # Track best epoch for R² (max)\n",
    "        if r2_val > best_epoch_r2:\n",
    "            best_epoch_r2 = r2_val\n",
    "            best_epoch_r2_index = epoch\n",
    "        \n",
    "        # Track best epoch for RMSE (min)\n",
    "        if rmse_val < best_epoch_rmse:\n",
    "            best_epoch_rmse = rmse_val\n",
    "            best_epoch_rmse_index = epoch\n",
    "    \n",
    "    # Predict on the training and validation sets after training\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    \n",
    "    # Reverse normalization for both predictions and true values\n",
    "    y_pred_train = reverse_normalization(y_pred_train, min_train, max_train)\n",
    "    y_pred_val = reverse_normalization(y_pred_val, min_train, max_train)\n",
    "    y_train_orig = reverse_normalization(y_train, min_train, max_train)\n",
    "    y_val_orig = reverse_normalization(y_val, min_train, max_train)\n",
    "    \n",
    "    # Calculate and print final R squared and RMSE for train and validation sets\n",
    "    calculate_metrics(y_train_orig, y_pred_train, y_val_orig, y_pred_val, title=f'LSTM Model - {site_name}', best_epoch=best_epoch_r2_index)\n",
    "    \n",
    "    # Plot the loss, R squared, and RMSE metrics for train and validation sets\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(10, 12))\n",
    "    \n",
    "    # Plot R2 for train and validation sets\n",
    "    axs[0].plot(range(epochs), r2_train_list, label='Train R²', color='blue', linestyle='-', marker='o')\n",
    "    axs[0].plot(range(epochs), r2_val_list, label='Validation R²', color='green', linestyle='--', marker='s')\n",
    "    axs[0].set_title(f'R² Score per Epoch - {site_name}')\n",
    "    axs[0].set_xlabel('Epochs')\n",
    "    axs[0].set_ylabel('R²')\n",
    "    axs[0].legend()\n",
    "    \n",
    "    # Mark best epoch for R² and display R² value\n",
    "    axs[0].scatter(best_epoch_r2_index, best_epoch_r2, color='red', zorder=5, label=f'Best Epoch: {best_epoch_r2_index}')\n",
    "    axs[0].text(best_epoch_r2_index, best_epoch_r2, f' R²: {best_epoch_r2:.3f}', color='red', fontsize=10, verticalalignment='bottom')\n",
    "    \n",
    "    # Plot RMSE for train and validation sets\n",
    "    axs[1].plot(range(epochs), rmse_train_list, label='Train RMSE', color='red', linestyle='-', marker='x')\n",
    "    axs[1].plot(range(epochs), rmse_val_list, label='Validation RMSE', color='purple', linestyle='--', marker='v')\n",
    "    axs[1].set_title(f'RMSE per Epoch - {site_name}')\n",
    "    axs[1].set_xlabel('Epochs')\n",
    "    axs[1].set_ylabel('RMSE')\n",
    "    axs[1].legend()\n",
    "    \n",
    "    # Mark best epoch for RMSE and display RMSE value\n",
    "    axs[1].scatter(best_epoch_rmse_index, best_epoch_rmse, color='blue', zorder=5, label=f'Best Epoch: {best_epoch_rmse_index}')\n",
    "    axs[1].text(best_epoch_rmse_index, best_epoch_rmse, f' RMSE: {best_epoch_rmse:.3f}', color='blue', fontsize=10, verticalalignment='top')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming your split data frames are named as follows:\n",
    "# (train_resampled_df_50, val_resampled_df_50)\n",
    "# (train_resampled_df_130, val_resampled_df_130)\n",
    "# (train_resampled_df_35, val_resampled_df_35)\n",
    "# (train_resampled_df_30, val_resampled_df_30)\n",
    "\n",
    "# List of split data frames and site names\n",
    "split_resampled_dfs = [\n",
    "    (normalized_train_df_50, normalized_val_df_50, 'Site_50MV')\n",
    "]\n",
    "\n",
    "# Define the min and max values used for normalization\n",
    "min_train_50 = normalized_train_df_50.min().min()  # Get the minimum value from the training data\n",
    "max_train_50 = normalized_train_df_50.max().max()  # Get the maximum value from the training data\n",
    "\n",
    "# Apply the LSTM model to each set of split data frames and calculate metrics\n",
    "for i, (train_df, val_df, site_name) in enumerate(split_resampled_dfs):\n",
    "    print(f\"Evaluating LSTM model for resampled dataset {site_name} ({i+1})\")\n",
    "    train_evaluate_lstm(\n",
    "        train_df=train_df,\n",
    "        val_df=val_df,\n",
    "        target_column='Power(MW)',\n",
    "        lookback_steps=8,\n",
    "        epochs=128,\n",
    "        batch_size=64,\n",
    "        site_name=site_name,\n",
    "        min_train=min_train_50,  # Provide the min value for reversing normalization\n",
    "        max_train=max_train_50   # Provide the max value for reversing normalization\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f7f4da",
   "metadata": {},
   "source": [
    " # Evaluating LSTM on Validation for site_50MV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c89a50",
   "metadata": {},
   "source": [
    "Consider different combination of number of units and epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7686eeac",
   "metadata": {},
   "source": [
    "Number of Units: 64-32-24, Epochs=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc45dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "# Function to create look-back features for time series forecasting\n",
    "def create_lookback_features(df, target_column, lookback_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - lookback_steps):\n",
    "        X.append(df.iloc[i:i + lookback_steps].values)\n",
    "        y.append(df.iloc[i + lookback_steps][target_column])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Function to reverse normalization\n",
    "def reverse_normalization(normalized_values, min_val, max_val):\n",
    "    return normalized_values * (max_val - min_val) + min_val\n",
    "\n",
    "# Function to calculate and print R squared and RMSE\n",
    "def calculate_metrics(y_true_train, y_pred_train, y_true_val, y_pred_val, title, best_epoch=None):\n",
    "    r2_train = r2_score(y_true_train, y_pred_train)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_true_train, y_pred_train))\n",
    "    \n",
    "    r2_val = r2_score(y_true_val, y_pred_val)\n",
    "    rmse_val = np.sqrt(mean_squared_error(y_true_val, y_pred_val))\n",
    "    \n",
    "    print(f\"{title} - Train R squared: {r2_train}, RMSE: {rmse_train}\")\n",
    "    print(f\"{title} - Validation R squared: {r2_val}, RMSE: {rmse_val}\")\n",
    "    \n",
    "    if best_epoch is not None:\n",
    "        print(f\"Best Epoch: {best_epoch}\")\n",
    "\n",
    "# Function to train and evaluate the LSTM model\n",
    "def train_evaluate_lstm(train_df, val_df, target_column='Power(MW)', lookback_steps=8, epochs=100, batch_size=64, site_name='Site', min_train=None, max_train=None):\n",
    "    # Create look-back features\n",
    "    X_train, y_train = create_lookback_features(train_df, target_column, lookback_steps)\n",
    "    X_val, y_val = create_lookback_features(val_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape data for LSTM: [samples, time steps, features]\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2])\n",
    "    X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], X_val.shape[2])\n",
    "    \n",
    "    # Build the LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                   kernel_regularizer=regularizers.l2(0.001)))  # LSTM Layer with L2 regularization\n",
    "    model.add(BatchNormalization())  # BatchNormalization after second LSTM\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(LSTM(units=32, return_sequences=False, kernel_regularizer=regularizers.l2(0.001)))  # Second LSTM Layer\n",
    "    model.add(BatchNormalization())  # BatchNormalization after second LSTM\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    # Adding Dense layers with ReLU activation\n",
    "    model.add(Dense(24, activation='relu', kernel_regularizer=regularizers.l2(0.001)))  # Hidden Dense layer with ReLU\n",
    "    model.add(Dropout(0.2))  # Dropout layer after Dense layer\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))  # Output layer with Linear activation (default for regression)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    # Initialize lists to store metrics for each epoch\n",
    "    r2_train_list = []\n",
    "    rmse_train_list = []\n",
    "    r2_val_list = []  # List to store R² for validation set\n",
    "    rmse_val_list = []  # List to store RMSE for validation set\n",
    "    \n",
    "    # Initialize variables to track the best epoch\n",
    "    best_epoch_r2 = -np.inf\n",
    "    best_epoch_rmse = np.inf\n",
    "    best_epoch_r2_index = None\n",
    "    best_epoch_rmse_index = None\n",
    "    \n",
    "    # Train the model and track history\n",
    "    for epoch in range(epochs):\n",
    "        # Train for one epoch\n",
    "        history = model.fit(X_train, y_train, epochs=1, batch_size=batch_size, verbose=2,shuffle=False)\n",
    "        \n",
    "        # Get predictions for train and validation data\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_val = model.predict(X_val)\n",
    "        \n",
    "        # Reverse normalization for both predictions and true values\n",
    "        y_pred_train = reverse_normalization(y_pred_train, min_train, max_train)\n",
    "        y_pred_val = reverse_normalization(y_pred_val, min_train, max_train)\n",
    "        y_train_orig = reverse_normalization(y_train, min_train, max_train)\n",
    "        y_val_orig = reverse_normalization(y_val, min_train, max_train)\n",
    "        \n",
    "        # Calculate R-squared and RMSE for the current epoch for both train and validation data\n",
    "        r2_train = r2_score(y_train_orig, y_pred_train)\n",
    "        rmse_train = np.sqrt(mean_squared_error(y_train_orig, y_pred_train))\n",
    "        \n",
    "        r2_val = r2_score(y_val_orig, y_pred_val)\n",
    "        rmse_val = np.sqrt(mean_squared_error(y_val_orig, y_pred_val))\n",
    "        \n",
    "        # Append metrics to lists\n",
    "        r2_train_list.append(r2_train)\n",
    "        rmse_train_list.append(rmse_train)\n",
    "        r2_val_list.append(r2_val)  # Append to validation list\n",
    "        rmse_val_list.append(rmse_val)  # Append to validation list\n",
    "        \n",
    "        # Track best epoch for R² (max)\n",
    "        if r2_val > best_epoch_r2:\n",
    "            best_epoch_r2 = r2_val\n",
    "            best_epoch_r2_index = epoch\n",
    "        \n",
    "        # Track best epoch for RMSE (min)\n",
    "        if rmse_val < best_epoch_rmse:\n",
    "            best_epoch_rmse = rmse_val\n",
    "            best_epoch_rmse_index = epoch\n",
    "    \n",
    "    # Predict on the training and validation sets after training\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    \n",
    "    # Reverse normalization for both predictions and true values\n",
    "    y_pred_train = reverse_normalization(y_pred_train, min_train, max_train)\n",
    "    y_pred_val = reverse_normalization(y_pred_val, min_train, max_train)\n",
    "    y_train_orig = reverse_normalization(y_train, min_train, max_train)\n",
    "    y_val_orig = reverse_normalization(y_val, min_train, max_train)\n",
    "    \n",
    "    # Calculate and print final R squared and RMSE for train and validation sets\n",
    "    calculate_metrics(y_train_orig, y_pred_train, y_val_orig, y_pred_val, title=f'LSTM Model - {site_name}', best_epoch=best_epoch_r2_index)\n",
    "    \n",
    "    # Plot the loss, R squared, and RMSE metrics for train and validation sets\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(10, 12))\n",
    "    \n",
    "    # Plot R2 for train and validation sets\n",
    "    axs[0].plot(range(epochs), r2_train_list, label='Train R²', color='blue', linestyle='-', marker='o')\n",
    "    axs[0].plot(range(epochs), r2_val_list, label='Validation R²', color='green', linestyle='--', marker='s')\n",
    "    axs[0].set_title(f'R² Score per Epoch (LSTM: LB=8, Epoch=64, Units=64-32-24) - {site_name}')\n",
    "    axs[0].set_xlabel('Epochs')\n",
    "    axs[0].set_ylabel('R²')\n",
    "    axs[0].legend()\n",
    "    \n",
    "    # Mark best epoch for R² and display R² value\n",
    "    axs[0].scatter(best_epoch_r2_index, best_epoch_r2, color='red', zorder=5, label=f'Best Epoch: {best_epoch_r2_index}')\n",
    "    axs[0].text(best_epoch_r2_index, best_epoch_r2, f' R²: {best_epoch_r2:.3f}', color='red', fontsize=10, verticalalignment='bottom')\n",
    "    \n",
    "    # Plot RMSE for train and validation sets\n",
    "    axs[1].plot(range(epochs), rmse_train_list, label='Train RMSE', color='red', linestyle='-', marker='x')\n",
    "    axs[1].plot(range(epochs), rmse_val_list, label='Validation RMSE', color='purple', linestyle='--', marker='v')\n",
    "    axs[1].set_title(f'RMSE per Epoch (LSTM: LB=8, Epoch=64, Units=64-32-24) - {site_name}')\n",
    "    axs[1].set_xlabel('Epochs')\n",
    "    axs[1].set_ylabel('RMSE')\n",
    "    axs[1].legend()\n",
    "    \n",
    "    # Mark best epoch for RMSE and display RMSE value\n",
    "    axs[1].scatter(best_epoch_rmse_index, best_epoch_rmse, color='blue', zorder=5, label=f'Best Epoch: {best_epoch_rmse_index}')\n",
    "    axs[1].text(best_epoch_rmse_index, best_epoch_rmse, f' RMSE: {best_epoch_rmse:.3f}', color='blue', fontsize=10, verticalalignment='top')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming your split data frames are named as follows:\n",
    "# (train_resampled_df_50, val_resampled_df_50)\n",
    "# (train_resampled_df_130, val_resampled_df_130)\n",
    "# (train_resampled_df_35, val_resampled_df_35)\n",
    "# (train_resampled_df_30, val_resampled_df_30)\n",
    "\n",
    "# List of split data frames and site names\n",
    "split_resampled_dfs = [\n",
    "    (normalized_train_df_50, normalized_val_df_50, 'Site_50MV')\n",
    "]\n",
    "\n",
    "# Define the min and max values used for normalization\n",
    "min_train_50 = normalized_train_df_50.min().min()  # Get the minimum value from the training data\n",
    "max_train_50 = normalized_train_df_50.max().max()  # Get the maximum value from the training data\n",
    "\n",
    "# Apply the LSTM model to each set of split data frames and calculate metrics\n",
    "for i, (train_df, val_df, site_name) in enumerate(split_resampled_dfs):\n",
    "    print(f\"Evaluating LSTM model for resampled dataset {site_name} ({i+1})\")\n",
    "    train_evaluate_lstm(\n",
    "        train_df=train_df,\n",
    "        val_df=val_df,\n",
    "        target_column='Power(MW)',\n",
    "        lookback_steps=8,\n",
    "        epochs=64,\n",
    "        batch_size=64,\n",
    "        site_name=site_name,\n",
    "        min_train=min_train_50,  # Provide the min value for reversing normalization\n",
    "        max_train=max_train_50   # Provide the max value for reversing normalization\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772619a4",
   "metadata": {},
   "source": [
    "Number of Units: 32-16-20, Epochs=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822b25a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "# Function to create look-back features for time series forecasting\n",
    "def create_lookback_features(df, target_column, lookback_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - lookback_steps):\n",
    "        X.append(df.iloc[i:i + lookback_steps].values)\n",
    "        y.append(df.iloc[i + lookback_steps][target_column])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Function to reverse normalization\n",
    "def reverse_normalization(normalized_values, min_val, max_val):\n",
    "    return normalized_values * (max_val - min_val) + min_val\n",
    "\n",
    "# Function to calculate and print R squared and RMSE\n",
    "def calculate_metrics(y_true_train, y_pred_train, y_true_val, y_pred_val, title, best_epoch=None):\n",
    "    r2_train = r2_score(y_true_train, y_pred_train)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_true_train, y_pred_train))\n",
    "    \n",
    "    r2_val = r2_score(y_true_val, y_pred_val)\n",
    "    rmse_val = np.sqrt(mean_squared_error(y_true_val, y_pred_val))\n",
    "    \n",
    "    print(f\"{title} - Train R squared: {r2_train}, RMSE: {rmse_train}\")\n",
    "    print(f\"{title} - Validation R squared: {r2_val}, RMSE: {rmse_val}\")\n",
    "    \n",
    "    if best_epoch is not None:\n",
    "        print(f\"Best Epoch: {best_epoch}\")\n",
    "\n",
    "# Function to train and evaluate the LSTM model\n",
    "def train_evaluate_lstm(train_df, val_df, target_column='Power(MW)', lookback_steps=8, epochs=100, batch_size=64, site_name='Site', min_train=None, max_train=None):\n",
    "    # Create look-back features\n",
    "    X_train, y_train = create_lookback_features(train_df, target_column, lookback_steps)\n",
    "    X_val, y_val = create_lookback_features(val_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape data for LSTM: [samples, time steps, features]\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2])\n",
    "    X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], X_val.shape[2])\n",
    "    \n",
    "    # Build the LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=32, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                   kernel_regularizer=regularizers.l2(0.001)))  # LSTM Layer with L2 regularization\n",
    "    model.add(BatchNormalization())  # BatchNormalization after second LSTM\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(LSTM(units=16, return_sequences=False, kernel_regularizer=regularizers.l2(0.001)))  # Second LSTM Layer\n",
    "    model.add(BatchNormalization())  # BatchNormalization after second LSTM\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    # Adding Dense layers with ReLU activation\n",
    "    model.add(Dense(20, activation='relu', kernel_regularizer=regularizers.l2(0.001)))  # Hidden Dense layer with ReLU\n",
    "    model.add(Dropout(0.2))  # Dropout layer after Dense layer\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))  # Output layer with Linear activation (default for regression)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    # Initialize lists to store metrics for each epoch\n",
    "    r2_train_list = []\n",
    "    rmse_train_list = []\n",
    "    r2_val_list = []  # List to store R² for validation set\n",
    "    rmse_val_list = []  # List to store RMSE for validation set\n",
    "    \n",
    "    # Initialize variables to track the best epoch\n",
    "    best_epoch_r2 = -np.inf\n",
    "    best_epoch_rmse = np.inf\n",
    "    best_epoch_r2_index = None\n",
    "    best_epoch_rmse_index = None\n",
    "    \n",
    "    # Train the model and track history\n",
    "    for epoch in range(epochs):\n",
    "        # Train for one epoch\n",
    "        history = model.fit(X_train, y_train, epochs=1, batch_size=batch_size, verbose=2,shuffle=False)\n",
    "        \n",
    "        # Get predictions for train and validation data\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_val = model.predict(X_val)\n",
    "        \n",
    "        # Reverse normalization for both predictions and true values\n",
    "        y_pred_train = reverse_normalization(y_pred_train, min_train, max_train)\n",
    "        y_pred_val = reverse_normalization(y_pred_val, min_train, max_train)\n",
    "        y_train_orig = reverse_normalization(y_train, min_train, max_train)\n",
    "        y_val_orig = reverse_normalization(y_val, min_train, max_train)\n",
    "        \n",
    "        # Calculate R-squared and RMSE for the current epoch for both train and validation data\n",
    "        r2_train = r2_score(y_train_orig, y_pred_train)\n",
    "        rmse_train = np.sqrt(mean_squared_error(y_train_orig, y_pred_train))\n",
    "        \n",
    "        r2_val = r2_score(y_val_orig, y_pred_val)\n",
    "        rmse_val = np.sqrt(mean_squared_error(y_val_orig, y_pred_val))\n",
    "        \n",
    "        # Append metrics to lists\n",
    "        r2_train_list.append(r2_train)\n",
    "        rmse_train_list.append(rmse_train)\n",
    "        r2_val_list.append(r2_val)  # Append to validation list\n",
    "        rmse_val_list.append(rmse_val)  # Append to validation list\n",
    "        \n",
    "        # Track best epoch for R² (max)\n",
    "        if r2_val > best_epoch_r2:\n",
    "            best_epoch_r2 = r2_val\n",
    "            best_epoch_r2_index = epoch\n",
    "        \n",
    "        # Track best epoch for RMSE (min)\n",
    "        if rmse_val < best_epoch_rmse:\n",
    "            best_epoch_rmse = rmse_val\n",
    "            best_epoch_rmse_index = epoch\n",
    "    \n",
    "    # Predict on the training and validation sets after training\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    \n",
    "    # Reverse normalization for both predictions and true values\n",
    "    y_pred_train = reverse_normalization(y_pred_train, min_train, max_train)\n",
    "    y_pred_val = reverse_normalization(y_pred_val, min_train, max_train)\n",
    "    y_train_orig = reverse_normalization(y_train, min_train, max_train)\n",
    "    y_val_orig = reverse_normalization(y_val, min_train, max_train)\n",
    "    \n",
    "    # Calculate and print final R squared and RMSE for train and validation sets\n",
    "    calculate_metrics(y_train_orig, y_pred_train, y_val_orig, y_pred_val, title=f'LSTM Model - {site_name}', best_epoch=best_epoch_r2_index)\n",
    "    \n",
    "    # Plot the loss, R squared, and RMSE metrics for train and validation sets\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(10, 12))\n",
    "    \n",
    "    # Plot R2 for train and validation sets\n",
    "    axs[0].plot(range(epochs), r2_train_list, label='Train R²', color='blue', linestyle='-', marker='o')\n",
    "    axs[0].plot(range(epochs), r2_val_list, label='Validation R²', color='green', linestyle='--', marker='s')\n",
    "    axs[0].set_title(f'R² Score per Epoch (LSTM: LB=8, Epoch=128, Units=32-16-20) - {site_name}')\n",
    "    axs[0].set_xlabel('Epochs')\n",
    "    axs[0].set_ylabel('R²')\n",
    "    axs[0].legend()\n",
    "    \n",
    "    # Mark best epoch for R² and display R² value\n",
    "    axs[0].scatter(best_epoch_r2_index, best_epoch_r2, color='red', zorder=5, label=f'Best Epoch: {best_epoch_r2_index}')\n",
    "    axs[0].text(best_epoch_r2_index, best_epoch_r2, f' R²: {best_epoch_r2:.3f}', color='red', fontsize=10, verticalalignment='bottom')\n",
    "    \n",
    "    # Plot RMSE for train and validation sets\n",
    "    axs[1].plot(range(epochs), rmse_train_list, label='Train RMSE', color='red', linestyle='-', marker='x')\n",
    "    axs[1].plot(range(epochs), rmse_val_list, label='Validation RMSE', color='purple', linestyle='--', marker='v')\n",
    "    axs[1].set_title(f'RMSE per Epoch (LSTM: LB=8, Epoch=128, Units=32-16-20) - {site_name}')\n",
    "    axs[1].set_xlabel('Epochs')\n",
    "    axs[1].set_ylabel('RMSE')\n",
    "    axs[1].legend()\n",
    "    \n",
    "    # Mark best epoch for RMSE and display RMSE value\n",
    "    axs[1].scatter(best_epoch_rmse_index, best_epoch_rmse, color='blue', zorder=5, label=f'Best Epoch: {best_epoch_rmse_index}')\n",
    "    axs[1].text(best_epoch_rmse_index, best_epoch_rmse, f' RMSE: {best_epoch_rmse:.3f}', color='blue', fontsize=10, verticalalignment='top')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming your split data frames are named as follows:\n",
    "# (train_resampled_df_50, val_resampled_df_50)\n",
    "# (train_resampled_df_130, val_resampled_df_130)\n",
    "# (train_resampled_df_35, val_resampled_df_35)\n",
    "# (train_resampled_df_30, val_resampled_df_30)\n",
    "\n",
    "# List of split data frames and site names\n",
    "split_resampled_dfs = [\n",
    "    (normalized_train_df_50, normalized_val_df_50, 'Site_50MV')\n",
    "]\n",
    "\n",
    "# Define the min and max values used for normalization\n",
    "min_train_50 = normalized_train_df_50.min().min()  # Get the minimum value from the training data\n",
    "max_train_50 = normalized_train_df_50.max().max()  # Get the maximum value from the training data\n",
    "\n",
    "# Apply the LSTM model to each set of split data frames and calculate metrics\n",
    "for i, (train_df, val_df, site_name) in enumerate(split_resampled_dfs):\n",
    "    print(f\"Evaluating LSTM model for resampled dataset {site_name} ({i+1})\")\n",
    "    train_evaluate_lstm(\n",
    "        train_df=train_df,\n",
    "        val_df=val_df,\n",
    "        target_column='Power(MW)',\n",
    "        lookback_steps=8,\n",
    "        epochs=128,\n",
    "        batch_size=64,\n",
    "        site_name=site_name,\n",
    "        min_train=min_train_50,  # Provide the min value for reversing normalization\n",
    "        max_train=max_train_50   # Provide the max value for reversing normalization\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25936982",
   "metadata": {},
   "source": [
    "Number of Units: 64-32-24, Epochs=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dfa4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "# Function to create look-back features for time series forecasting\n",
    "def create_lookback_features(df, target_column, lookback_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - lookback_steps):\n",
    "        X.append(df.iloc[i:i + lookback_steps].values)\n",
    "        y.append(df.iloc[i + lookback_steps][target_column])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Function to reverse normalization\n",
    "def reverse_normalization(normalized_values, min_val, max_val):\n",
    "    return normalized_values * (max_val - min_val) + min_val\n",
    "\n",
    "# Function to calculate and print R squared and RMSE\n",
    "def calculate_metrics(y_true_train, y_pred_train, y_true_val, y_pred_val, title, best_epoch=None):\n",
    "    r2_train = r2_score(y_true_train, y_pred_train)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_true_train, y_pred_train))\n",
    "    \n",
    "    r2_val = r2_score(y_true_val, y_pred_val)\n",
    "    rmse_val = np.sqrt(mean_squared_error(y_true_val, y_pred_val))\n",
    "    \n",
    "    print(f\"{title} - Train R squared: {r2_train}, RMSE: {rmse_train}\")\n",
    "    print(f\"{title} - Validation R squared: {r2_val}, RMSE: {rmse_val}\")\n",
    "    \n",
    "    if best_epoch is not None:\n",
    "        print(f\"Best Epoch: {best_epoch}\")\n",
    "\n",
    "# Function to train and evaluate the LSTM model\n",
    "def train_evaluate_lstm(train_df, val_df, target_column='Power(MW)', lookback_steps=8, epochs=100, batch_size=64, site_name='Site', min_train=None, max_train=None):\n",
    "    # Create look-back features\n",
    "    X_train, y_train = create_lookback_features(train_df, target_column, lookback_steps)\n",
    "    X_val, y_val = create_lookback_features(val_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape data for LSTM: [samples, time steps, features]\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2])\n",
    "    X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], X_val.shape[2])\n",
    "    \n",
    "    # Build the LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                   kernel_regularizer=regularizers.l2(0.001)))  # LSTM Layer with L2 regularization\n",
    "    model.add(BatchNormalization())  # BatchNormalization after second LSTM\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(LSTM(units=32, return_sequences=False, kernel_regularizer=regularizers.l2(0.001)))  # Second LSTM Layer\n",
    "    model.add(BatchNormalization())  # BatchNormalization after second LSTM\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    # Adding Dense layers with ReLU activation\n",
    "    model.add(Dense(24, activation='relu', kernel_regularizer=regularizers.l2(0.001)))  # Hidden Dense layer with ReLU\n",
    "    model.add(Dropout(0.2))  # Dropout layer after Dense layer\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))  # Output layer with Linear activation (default for regression)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    # Initialize lists to store metrics for each epoch\n",
    "    r2_train_list = []\n",
    "    rmse_train_list = []\n",
    "    r2_val_list = []  # List to store R² for validation set\n",
    "    rmse_val_list = []  # List to store RMSE for validation set\n",
    "    \n",
    "    # Initialize variables to track the best epoch\n",
    "    best_epoch_r2 = -np.inf\n",
    "    best_epoch_rmse = np.inf\n",
    "    best_epoch_r2_index = None\n",
    "    best_epoch_rmse_index = None\n",
    "    \n",
    "    # Train the model and track history\n",
    "    for epoch in range(epochs):\n",
    "        # Train for one epoch\n",
    "        history = model.fit(X_train, y_train, epochs=1, batch_size=batch_size, verbose=2,shuffle=False)\n",
    "        \n",
    "        # Get predictions for train and validation data\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_val = model.predict(X_val)\n",
    "        \n",
    "        # Reverse normalization for both predictions and true values\n",
    "        y_pred_train = reverse_normalization(y_pred_train, min_train, max_train)\n",
    "        y_pred_val = reverse_normalization(y_pred_val, min_train, max_train)\n",
    "        y_train_orig = reverse_normalization(y_train, min_train, max_train)\n",
    "        y_val_orig = reverse_normalization(y_val, min_train, max_train)\n",
    "        \n",
    "        # Calculate R-squared and RMSE for the current epoch for both train and validation data\n",
    "        r2_train = r2_score(y_train_orig, y_pred_train)\n",
    "        rmse_train = np.sqrt(mean_squared_error(y_train_orig, y_pred_train))\n",
    "        \n",
    "        r2_val = r2_score(y_val_orig, y_pred_val)\n",
    "        rmse_val = np.sqrt(mean_squared_error(y_val_orig, y_pred_val))\n",
    "        \n",
    "        # Append metrics to lists\n",
    "        r2_train_list.append(r2_train)\n",
    "        rmse_train_list.append(rmse_train)\n",
    "        r2_val_list.append(r2_val)  # Append to validation list\n",
    "        rmse_val_list.append(rmse_val)  # Append to validation list\n",
    "        \n",
    "        # Track best epoch for R² (max)\n",
    "        if r2_val > best_epoch_r2:\n",
    "            best_epoch_r2 = r2_val\n",
    "            best_epoch_r2_index = epoch\n",
    "        \n",
    "        # Track best epoch for RMSE (min)\n",
    "        if rmse_val < best_epoch_rmse:\n",
    "            best_epoch_rmse = rmse_val\n",
    "            best_epoch_rmse_index = epoch\n",
    "    \n",
    "    # Predict on the training and validation sets after training\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    \n",
    "    # Reverse normalization for both predictions and true values\n",
    "    y_pred_train = reverse_normalization(y_pred_train, min_train, max_train)\n",
    "    y_pred_val = reverse_normalization(y_pred_val, min_train, max_train)\n",
    "    y_train_orig = reverse_normalization(y_train, min_train, max_train)\n",
    "    y_val_orig = reverse_normalization(y_val, min_train, max_train)\n",
    "    \n",
    "    # Calculate and print final R squared and RMSE for train and validation sets\n",
    "    calculate_metrics(y_train_orig, y_pred_train, y_val_orig, y_pred_val, title=f'LSTM Model - {site_name}', best_epoch=best_epoch_r2_index)\n",
    "    \n",
    "    # Plot the loss, R squared, and RMSE metrics for train and validation sets\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(10, 12))\n",
    "    \n",
    "    # Plot R2 for train and validation sets\n",
    "    axs[0].plot(range(epochs), r2_train_list, label='Train R²', color='blue', linestyle='-', marker='o')\n",
    "    axs[0].plot(range(epochs), r2_val_list, label='Validation R²', color='green', linestyle='--', marker='s')\n",
    "    axs[0].set_title(f'R² Score per Epoch (LSTM: LB=8, Epoch=64, Units=64-32-24) - {site_name}')\n",
    "    axs[0].set_xlabel('Epochs')\n",
    "    axs[0].set_ylabel('R²')\n",
    "    axs[0].legend()\n",
    "    \n",
    "    # Mark best epoch for R² and display R² value\n",
    "    axs[0].scatter(best_epoch_r2_index, best_epoch_r2, color='red', zorder=5, label=f'Best Epoch: {best_epoch_r2_index}')\n",
    "    axs[0].text(best_epoch_r2_index, best_epoch_r2, f' R²: {best_epoch_r2:.3f}', color='red', fontsize=10, verticalalignment='bottom')\n",
    "    \n",
    "    # Plot RMSE for train and validation sets\n",
    "    axs[1].plot(range(epochs), rmse_train_list, label='Train RMSE', color='red', linestyle='-', marker='x')\n",
    "    axs[1].plot(range(epochs), rmse_val_list, label='Validation RMSE', color='purple', linestyle='--', marker='v')\n",
    "    axs[1].set_title(f'RMSE per Epoch (LSTM: LB=8, Epoch=64, Units=64-32-24) - {site_name}')\n",
    "    axs[1].set_xlabel('Epochs')\n",
    "    axs[1].set_ylabel('RMSE')\n",
    "    axs[1].legend()\n",
    "    \n",
    "    # Mark best epoch for RMSE and display RMSE value\n",
    "    axs[1].scatter(best_epoch_rmse_index, best_epoch_rmse, color='blue', zorder=5, label=f'Best Epoch: {best_epoch_rmse_index}')\n",
    "    axs[1].text(best_epoch_rmse_index, best_epoch_rmse, f' RMSE: {best_epoch_rmse:.3f}', color='blue', fontsize=10, verticalalignment='top')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming your split data frames are named as follows:\n",
    "# (train_resampled_df_50, val_resampled_df_50)\n",
    "# (train_resampled_df_130, val_resampled_df_130)\n",
    "# (train_resampled_df_35, val_resampled_df_35)\n",
    "# (train_resampled_df_30, val_resampled_df_30)\n",
    "\n",
    "# List of split data frames and site names\n",
    "split_resampled_dfs = [\n",
    "    (normalized_train_df_50, normalized_val_df_50, 'Site_50MV')\n",
    "]\n",
    "\n",
    "# Define the min and max values used for normalization\n",
    "min_train_50 = normalized_train_df_50.min().min()  # Get the minimum value from the training data\n",
    "max_train_50 = normalized_train_df_50.max().max()  # Get the maximum value from the training data\n",
    "\n",
    "# Apply the LSTM model to each set of split data frames and calculate metrics\n",
    "for i, (train_df, val_df, site_name) in enumerate(split_resampled_dfs):\n",
    "    print(f\"Evaluating LSTM model for resampled dataset {site_name} ({i+1})\")\n",
    "    train_evaluate_lstm(\n",
    "        train_df=train_df,\n",
    "        val_df=val_df,\n",
    "        target_column='Power(MW)',\n",
    "        lookback_steps=8,\n",
    "        epochs=64,\n",
    "        batch_size=64,\n",
    "        site_name=site_name,\n",
    "        min_train=min_train_50,  # Provide the min value for reversing normalization\n",
    "        max_train=max_train_50   # Provide the max value for reversing normalization\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6fb974",
   "metadata": {},
   "source": [
    "Number of Units: 32-16-20, Epochs=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed90351",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "# Function to create look-back features for time series forecasting\n",
    "def create_lookback_features(df, target_column, lookback_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - lookback_steps):\n",
    "        X.append(df.iloc[i:i + lookback_steps].values)\n",
    "        y.append(df.iloc[i + lookback_steps][target_column])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Function to reverse normalization\n",
    "def reverse_normalization(normalized_values, min_val, max_val):\n",
    "    return normalized_values * (max_val - min_val) + min_val\n",
    "\n",
    "# Function to calculate and print R squared and RMSE\n",
    "def calculate_metrics(y_true_train, y_pred_train, y_true_val, y_pred_val, title, best_epoch=None):\n",
    "    r2_train = r2_score(y_true_train, y_pred_train)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_true_train, y_pred_train))\n",
    "    \n",
    "    r2_val = r2_score(y_true_val, y_pred_val)\n",
    "    rmse_val = np.sqrt(mean_squared_error(y_true_val, y_pred_val))\n",
    "    \n",
    "    print(f\"{title} - Train R squared: {r2_train}, RMSE: {rmse_train}\")\n",
    "    print(f\"{title} - Validation R squared: {r2_val}, RMSE: {rmse_val}\")\n",
    "    \n",
    "    if best_epoch is not None:\n",
    "        print(f\"Best Epoch: {best_epoch}\")\n",
    "\n",
    "# Function to train and evaluate the LSTM model\n",
    "def train_evaluate_lstm(train_df, val_df, target_column='Power(MW)', lookback_steps=8, epochs=100, batch_size=64, site_name='Site', min_train=None, max_train=None):\n",
    "    # Create look-back features\n",
    "    X_train, y_train = create_lookback_features(train_df, target_column, lookback_steps)\n",
    "    X_val, y_val = create_lookback_features(val_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape data for LSTM: [samples, time steps, features]\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2])\n",
    "    X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], X_val.shape[2])\n",
    "    \n",
    "    # Build the LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=32, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                   kernel_regularizer=regularizers.l2(0.001)))  # LSTM Layer with L2 regularization\n",
    "    model.add(BatchNormalization())  # BatchNormalization after second LSTM\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(LSTM(units=16, return_sequences=False, kernel_regularizer=regularizers.l2(0.001)))  # Second LSTM Layer\n",
    "    model.add(BatchNormalization())  # BatchNormalization after second LSTM\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    # Adding Dense layers with ReLU activation\n",
    "    model.add(Dense(20, activation='relu', kernel_regularizer=regularizers.l2(0.001)))  # Hidden Dense layer with ReLU\n",
    "    model.add(Dropout(0.2))  # Dropout layer after Dense layer\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))  # Output layer with Linear activation (default for regression)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    # Initialize lists to store metrics for each epoch\n",
    "    r2_train_list = []\n",
    "    rmse_train_list = []\n",
    "    r2_val_list = []  # List to store R² for validation set\n",
    "    rmse_val_list = []  # List to store RMSE for validation set\n",
    "    \n",
    "    # Initialize variables to track the best epoch\n",
    "    best_epoch_r2 = -np.inf\n",
    "    best_epoch_rmse = np.inf\n",
    "    best_epoch_r2_index = None\n",
    "    best_epoch_rmse_index = None\n",
    "    \n",
    "    # Train the model and track history\n",
    "    for epoch in range(epochs):\n",
    "        # Train for one epoch\n",
    "        history = model.fit(X_train, y_train, epochs=1, batch_size=batch_size, verbose=2,shuffle=False)\n",
    "        \n",
    "        # Get predictions for train and validation data\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_val = model.predict(X_val)\n",
    "        \n",
    "        # Reverse normalization for both predictions and true values\n",
    "        y_pred_train = reverse_normalization(y_pred_train, min_train, max_train)\n",
    "        y_pred_val = reverse_normalization(y_pred_val, min_train, max_train)\n",
    "        y_train_orig = reverse_normalization(y_train, min_train, max_train)\n",
    "        y_val_orig = reverse_normalization(y_val, min_train, max_train)\n",
    "        \n",
    "        # Calculate R-squared and RMSE for the current epoch for both train and validation data\n",
    "        r2_train = r2_score(y_train_orig, y_pred_train)\n",
    "        rmse_train = np.sqrt(mean_squared_error(y_train_orig, y_pred_train))\n",
    "        \n",
    "        r2_val = r2_score(y_val_orig, y_pred_val)\n",
    "        rmse_val = np.sqrt(mean_squared_error(y_val_orig, y_pred_val))\n",
    "        \n",
    "        # Append metrics to lists\n",
    "        r2_train_list.append(r2_train)\n",
    "        rmse_train_list.append(rmse_train)\n",
    "        r2_val_list.append(r2_val)  # Append to validation list\n",
    "        rmse_val_list.append(rmse_val)  # Append to validation list\n",
    "        \n",
    "        # Track best epoch for R² (max)\n",
    "        if r2_val > best_epoch_r2:\n",
    "            best_epoch_r2 = r2_val\n",
    "            best_epoch_r2_index = epoch\n",
    "        \n",
    "        # Track best epoch for RMSE (min)\n",
    "        if rmse_val < best_epoch_rmse:\n",
    "            best_epoch_rmse = rmse_val\n",
    "            best_epoch_rmse_index = epoch\n",
    "    \n",
    "    # Predict on the training and validation sets after training\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    \n",
    "    # Reverse normalization for both predictions and true values\n",
    "    y_pred_train = reverse_normalization(y_pred_train, min_train, max_train)\n",
    "    y_pred_val = reverse_normalization(y_pred_val, min_train, max_train)\n",
    "    y_train_orig = reverse_normalization(y_train, min_train, max_train)\n",
    "    y_val_orig = reverse_normalization(y_val, min_train, max_train)\n",
    "    \n",
    "    # Calculate and print final R squared and RMSE for train and validation sets\n",
    "    calculate_metrics(y_train_orig, y_pred_train, y_val_orig, y_pred_val, title=f'LSTM Model - {site_name}', best_epoch=best_epoch_r2_index)\n",
    "    \n",
    "    # Plot the loss, R squared, and RMSE metrics for train and validation sets\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(10, 12))\n",
    "    \n",
    "    # Plot R2 for train and validation sets\n",
    "    axs[0].plot(range(epochs), r2_train_list, label='Train R²', color='blue', linestyle='-', marker='o')\n",
    "    axs[0].plot(range(epochs), r2_val_list, label='Validation R²', color='green', linestyle='--', marker='s')\n",
    "    axs[0].set_title(f'R² Score per Epoch (LSTM: LB=8, Epoch=128, Units=32-16-20) - {site_name}')\n",
    "    axs[0].set_xlabel('Epochs')\n",
    "    axs[0].set_ylabel('R²')\n",
    "    axs[0].legend()\n",
    "    \n",
    "    # Mark best epoch for R² and display R² value\n",
    "    axs[0].scatter(best_epoch_r2_index, best_epoch_r2, color='red', zorder=5, label=f'Best Epoch: {best_epoch_r2_index}')\n",
    "    axs[0].text(best_epoch_r2_index, best_epoch_r2, f' R²: {best_epoch_r2:.3f}', color='red', fontsize=10, verticalalignment='bottom')\n",
    "    \n",
    "    # Plot RMSE for train and validation sets\n",
    "    axs[1].plot(range(epochs), rmse_train_list, label='Train RMSE', color='red', linestyle='-', marker='x')\n",
    "    axs[1].plot(range(epochs), rmse_val_list, label='Validation RMSE', color='purple', linestyle='--', marker='v')\n",
    "    axs[1].set_title(f'RMSE per Epoch (LSTM: LB=8, Epoch=128, Units=32-16-20) - {site_name}')\n",
    "    axs[1].set_xlabel('Epochs')\n",
    "    axs[1].set_ylabel('RMSE')\n",
    "    axs[1].legend()\n",
    "    \n",
    "    # Mark best epoch for RMSE and display RMSE value\n",
    "    axs[1].scatter(best_epoch_rmse_index, best_epoch_rmse, color='blue', zorder=5, label=f'Best Epoch: {best_epoch_rmse_index}')\n",
    "    axs[1].text(best_epoch_rmse_index, best_epoch_rmse, f' RMSE: {best_epoch_rmse:.3f}', color='blue', fontsize=10, verticalalignment='top')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming your split data frames are named as follows:\n",
    "# (train_resampled_df_50, val_resampled_df_50)\n",
    "# (train_resampled_df_130, val_resampled_df_130)\n",
    "# (train_resampled_df_35, val_resampled_df_35)\n",
    "# (train_resampled_df_30, val_resampled_df_30)\n",
    "\n",
    "# List of split data frames and site names\n",
    "split_resampled_dfs = [\n",
    "    (normalized_train_df_50, normalized_val_df_50, 'Site_50MV')\n",
    "]\n",
    "\n",
    "# Define the min and max values used for normalization\n",
    "min_train_50 = normalized_train_df_50.min().min()  # Get the minimum value from the training data\n",
    "max_train_50 = normalized_train_df_50.max().max()  # Get the maximum value from the training data\n",
    "\n",
    "# Apply the LSTM model to each set of split data frames and calculate metrics\n",
    "for i, (train_df, val_df, site_name) in enumerate(split_resampled_dfs):\n",
    "    print(f\"Evaluating LSTM model for resampled dataset {site_name} ({i+1})\")\n",
    "    train_evaluate_lstm(\n",
    "        train_df=train_df,\n",
    "        val_df=val_df,\n",
    "        target_column='Power(MW)',\n",
    "        lookback_steps=8,\n",
    "        epochs=64,\n",
    "        batch_size=64,\n",
    "        site_name=site_name,\n",
    "        min_train=min_train_50,  # Provide the min value for reversing normalization\n",
    "        max_train=max_train_50   # Provide the max value for reversing normalization\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d322d5",
   "metadata": {},
   "source": [
    "# Build LSTM model with best combination of Hyperparameters, then forcast On Test Data for all sites and plot actual vs predicted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86e71de",
   "metadata": {},
   "source": [
    "# Site_50MV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e65cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "# Function to create look-back features for time series forecasting\n",
    "def create_lookback_features(df, target_column, lookback_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - lookback_steps):\n",
    "        X.append(df.iloc[i:i + lookback_steps].values)\n",
    "        y.append(df.iloc[i + lookback_steps][target_column])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Function to calculate and print R squared and RMSE\n",
    "def calculate_metrics(y_true, y_pred, title):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    \n",
    "    print(f\"{title} - R squared: {r2}, RMSE: {rmse}\")\n",
    "\n",
    "# Function to reverse normalization\n",
    "def reverse_normalization(normalized_values, min_val, max_val):\n",
    "    return normalized_values * (max_val - min_val) + min_val\n",
    "\n",
    "# Function to train the LSTM model and predict on test data\n",
    "def train_evaluate_lstm(train_df, test_df, target_column='Power(MW)', lookback_steps=8, epochs=100, batch_size=64, site_name='Site', min_val=None, max_val=None):\n",
    "    # Create look-back features for training data\n",
    "    X_train, y_train = create_lookback_features(train_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape data for GRU: [samples, time steps, features]\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2])\n",
    "    \n",
    "    # Build the LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                   kernel_regularizer=regularizers.l2(0.001)))  # LSTM Layer with L2 regularization\n",
    "    model.add(BatchNormalization())  # BatchNormalization after first LSTM\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(LSTM(units=32, return_sequences=False, kernel_regularizer=regularizers.l2(0.001)))  # Second LSTM Layer\n",
    "    model.add(BatchNormalization())  # BatchNormalization after second LSTM\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    # Adding Dense layers with ReLU activation\n",
    "    model.add(Dense(24, activation='relu', kernel_regularizer=regularizers.l2(0.001)))  # Hidden Dense layer with ReLU\n",
    "    model.add(BatchNormalization())  # BatchNormalization after Dense layer\n",
    "    model.add(Dropout(0.2))  # Dropout layer after Dense layer\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))  # Output layer with Linear activation (default for regression)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1, shuffle=False)\n",
    "    \n",
    "    # Create look-back features for test data\n",
    "    X_test, y_test = create_lookback_features(test_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape test data for LSTM\n",
    "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2])\n",
    "    \n",
    "    # Predict on the test data\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Reverse normalization on both actual and predicted values\n",
    "    y_test_reversed = reverse_normalization(y_test, min_val, max_val)\n",
    "    y_pred_test_reversed = reverse_normalization(y_pred_test, min_val, max_val)\n",
    "    \n",
    "    # Calculate and print R squared and RMSE for the test set\n",
    "    calculate_metrics(y_test_reversed, y_pred_test_reversed, title=f'LSTM Model - {site_name} (Test Data)')\n",
    "\n",
    "    # Define the range of days you want to plot (90th to 100th days)\n",
    "    start_day = 90\n",
    "    end_day = 95\n",
    "    records_per_day = 48  # 24 hours * 2 records per hour (30-minute intervals)\n",
    "\n",
    "    # Calculate the indices corresponding to the 90th and 100th days\n",
    "    start_index = (start_day - 1) * records_per_day  # Start index for 90th day\n",
    "    end_index = end_day * records_per_day  # End index for 100th day\n",
    "\n",
    "    # Slice the test data and predictions for the range from the 90th to 100th day\n",
    "    y_test_range = y_test_reversed[start_index:end_index]\n",
    "    y_pred_test_range = y_pred_test_reversed[start_index:end_index]\n",
    "\n",
    "    # Plot the comparison between actual and predicted values for the 90th to 100th days\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(start_index, end_index), y_test_range, label='Actual', color='blue', linestyle='-', marker='o')\n",
    "    plt.plot(range(start_index, end_index), y_pred_test_range, label='Predicted', color='red', linestyle='--', marker='x')\n",
    "    plt.title(f'Actual vs Predicted Power (LSTM: LB=8, Epoch=128, Unit=64-32-24) - {site_name}')\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('Power (MW)')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming your split data frames are named as follows:\n",
    "# (train_resampled_df_50, test_resampled_df_50)\n",
    "\n",
    "# Define the min and max values used for normalization\n",
    "min_train_50 = normalized_train_df_50.min().min()  # Get the minimum value from the training data\n",
    "max_train_50 = normalized_train_df_50.max().max()  # Get the maximum value from the training data\n",
    "\n",
    "# List of split data frames and site names\n",
    "split_resampled_dfs = [\n",
    "    (normalized_train_df_50, normalized_test_df_50, 'Site_50MV')\n",
    "]\n",
    "\n",
    "# Apply the GRU model to each set of split data frames and calculate metrics\n",
    "for i, (train_df, test_df, site_name) in enumerate(split_resampled_dfs):\n",
    "    print(f\"Evaluating LSTM model for resampled dataset {site_name} ({i+1})\")\n",
    "    train_evaluate_lstm(train_df=train_df,\n",
    "                        test_df=test_df,\n",
    "                        target_column='Power(MW)',\n",
    "                        lookback_steps=8, epochs=128, batch_size=64,\n",
    "                        site_name=site_name,\n",
    "                        min_val=min_train_50,\n",
    "                        max_val=max_train_50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9093c785",
   "metadata": {},
   "source": [
    "# Site_130MV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6748f9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "# Function to create look-back features for time series forecasting\n",
    "def create_lookback_features(df, target_column, lookback_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - lookback_steps):\n",
    "        X.append(df.iloc[i:i + lookback_steps].values)\n",
    "        y.append(df.iloc[i + lookback_steps][target_column])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Function to calculate and print R squared and RMSE\n",
    "def calculate_metrics(y_true, y_pred, title):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    \n",
    "    print(f\"{title} - R squared: {r2}, RMSE: {rmse}\")\n",
    "\n",
    "# Function to reverse normalization\n",
    "def reverse_normalization(normalized_values, min_val, max_val):\n",
    "    return normalized_values * (max_val - min_val) + min_val\n",
    "\n",
    "# Function to train the LSTM model and predict on test data\n",
    "def train_evaluate_lstm(train_df, test_df, target_column='Power(MW)', lookback_steps=8, epochs=100, batch_size=64, site_name='Site', min_val=None, max_val=None):\n",
    "    # Create look-back features for training data\n",
    "    X_train, y_train = create_lookback_features(train_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape data for GRU: [samples, time steps, features]\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2])\n",
    "    \n",
    "    # Build the LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                   kernel_regularizer=regularizers.l2(0.001)))  # LSTM Layer with L2 regularization\n",
    "    model.add(BatchNormalization())  # BatchNormalization after first LSTM\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(LSTM(units=32, return_sequences=False, kernel_regularizer=regularizers.l2(0.001)))  # Second LSTM Layer\n",
    "    model.add(BatchNormalization())  # BatchNormalization after second LSTM\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    # Adding Dense layers with ReLU activation\n",
    "    model.add(Dense(24, activation='relu', kernel_regularizer=regularizers.l2(0.001)))  # Hidden Dense layer with ReLU\n",
    "    model.add(BatchNormalization())  # BatchNormalization after Dense layer\n",
    "    model.add(Dropout(0.2))  # Dropout layer after Dense layer\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))  # Output layer with Linear activation (default for regression)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1, shuffle=False)\n",
    "    \n",
    "    # Create look-back features for test data\n",
    "    X_test, y_test = create_lookback_features(test_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape test data for LSTM\n",
    "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2])\n",
    "    \n",
    "    # Predict on the test data\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Reverse normalization on both actual and predicted values\n",
    "    y_test_reversed = reverse_normalization(y_test, min_val, max_val)\n",
    "    y_pred_test_reversed = reverse_normalization(y_pred_test, min_val, max_val)\n",
    "    \n",
    "    # Calculate and print R squared and RMSE for the test set\n",
    "    calculate_metrics(y_test_reversed, y_pred_test_reversed, title=f'LSTM Model - {site_name} (Test Data)')\n",
    "\n",
    "    # Define the range of days you want to plot (90th to 100th days)\n",
    "    start_day = 90\n",
    "    end_day = 95\n",
    "    records_per_day = 48  # 24 hours * 2 records per hour (30-minute intervals)\n",
    "\n",
    "    # Calculate the indices corresponding to the 90th and 100th days\n",
    "    start_index = (start_day - 1) * records_per_day  # Start index for 90th day\n",
    "    end_index = end_day * records_per_day  # End index for 100th day\n",
    "\n",
    "    # Slice the test data and predictions for the range from the 90th to 100th day\n",
    "    y_test_range = y_test_reversed[start_index:end_index]\n",
    "    y_pred_test_range = y_pred_test_reversed[start_index:end_index]\n",
    "\n",
    "    # Plot the comparison between actual and predicted values for the 90th to 100th days\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(start_index, end_index), y_test_range, label='Actual', color='blue', linestyle='-', marker='o')\n",
    "    plt.plot(range(start_index, end_index), y_pred_test_range, label='Predicted', color='red', linestyle='--', marker='x')\n",
    "    plt.title(f'Actual vs Predicted Power (LSTM: LB=8, Epoch=128, Unit=64-32-24) - {site_name}')\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('Power (MW)')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming your split data frames are named as follows:\n",
    "# (train_resampled_df_50, test_resampled_df_50)\n",
    "\n",
    "# Define the min and max values used for normalization\n",
    "min_train_130 = normalized_train_df_130.min().min()  # Get the minimum value from the training data\n",
    "max_train_130 = normalized_train_df_130.max().max()  # Get the maximum value from the training data\n",
    "\n",
    "# List of split data frames and site names\n",
    "split_resampled_dfs = [\n",
    "    (normalized_train_df_130, normalized_test_df_130, 'Site_130MV')\n",
    "]\n",
    "\n",
    "# Apply the GRU model to each set of split data frames and calculate metrics\n",
    "for i, (train_df, test_df, site_name) in enumerate(split_resampled_dfs):\n",
    "    print(f\"Evaluating LSTM model for resampled dataset {site_name} ({i+1})\")\n",
    "    train_evaluate_lstm(train_df=train_df,\n",
    "                        test_df=test_df,\n",
    "                        target_column='Power(MW)',\n",
    "                        lookback_steps=8, epochs=128, batch_size=64,\n",
    "                        site_name=site_name,\n",
    "                        min_val=min_train_130,\n",
    "                        max_val=max_train_130)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc1ebff",
   "metadata": {},
   "source": [
    "# Site_30MV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91da3e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "# Function to create look-back features for time series forecasting\n",
    "def create_lookback_features(df, target_column, lookback_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - lookback_steps):\n",
    "        X.append(df.iloc[i:i + lookback_steps].values)\n",
    "        y.append(df.iloc[i + lookback_steps][target_column])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Function to calculate and print R squared and RMSE\n",
    "def calculate_metrics(y_true, y_pred, title):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    \n",
    "    print(f\"{title} - R squared: {r2}, RMSE: {rmse}\")\n",
    "\n",
    "# Function to reverse normalization\n",
    "def reverse_normalization(normalized_values, min_val, max_val):\n",
    "    return normalized_values * (max_val - min_val) + min_val\n",
    "\n",
    "# Function to train the LSTM model and predict on test data\n",
    "def train_evaluate_lstm(train_df, test_df, target_column='Power(MW)', lookback_steps=8, epochs=100, batch_size=64, site_name='Site', min_val=None, max_val=None):\n",
    "    # Create look-back features for training data\n",
    "    X_train, y_train = create_lookback_features(train_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape data for GRU: [samples, time steps, features]\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2])\n",
    "    \n",
    "    # Build the LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                   kernel_regularizer=regularizers.l2(0.001)))  # LSTM Layer with L2 regularization\n",
    "    model.add(BatchNormalization())  # BatchNormalization after first LSTM\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(LSTM(units=32, return_sequences=False, kernel_regularizer=regularizers.l2(0.001)))  # Second LSTM Layer\n",
    "    model.add(BatchNormalization())  # BatchNormalization after second LSTM\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    # Adding Dense layers with ReLU activation\n",
    "    model.add(Dense(24, activation='relu', kernel_regularizer=regularizers.l2(0.001)))  # Hidden Dense layer with ReLU\n",
    "    model.add(BatchNormalization())  # BatchNormalization after Dense layer\n",
    "    model.add(Dropout(0.2))  # Dropout layer after Dense layer\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))  # Output layer with Linear activation (default for regression)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1, shuffle=False)\n",
    "    \n",
    "    # Create look-back features for test data\n",
    "    X_test, y_test = create_lookback_features(test_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape test data for LSTM\n",
    "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2])\n",
    "    \n",
    "    # Predict on the test data\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Reverse normalization on both actual and predicted values\n",
    "    y_test_reversed = reverse_normalization(y_test, min_val, max_val)\n",
    "    y_pred_test_reversed = reverse_normalization(y_pred_test, min_val, max_val)\n",
    "    \n",
    "    # Calculate and print R squared and RMSE for the test set\n",
    "    calculate_metrics(y_test_reversed, y_pred_test_reversed, title=f'LSTM Model - {site_name} (Test Data)')\n",
    "\n",
    "    # Define the range of days you want to plot (90th to 100th days)\n",
    "    start_day = 90\n",
    "    end_day = 95\n",
    "    records_per_day = 48  # 24 hours * 2 records per hour (30-minute intervals)\n",
    "\n",
    "    # Calculate the indices corresponding to the 90th and 100th days\n",
    "    start_index = (start_day - 1) * records_per_day  # Start index for 90th day\n",
    "    end_index = end_day * records_per_day  # End index for 100th day\n",
    "\n",
    "    # Slice the test data and predictions for the range from the 90th to 100th day\n",
    "    y_test_range = y_test_reversed[start_index:end_index]\n",
    "    y_pred_test_range = y_pred_test_reversed[start_index:end_index]\n",
    "\n",
    "    # Plot the comparison between actual and predicted values for the 90th to 100th days\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(start_index, end_index), y_test_range, label='Actual', color='blue', linestyle='-', marker='o')\n",
    "    plt.plot(range(start_index, end_index), y_pred_test_range, label='Predicted', color='red', linestyle='--', marker='x')\n",
    "    plt.title(f'Actual vs Predicted Power (LSTM: LB=8, Epoch=128, Unit=64-32-24) - {site_name}')\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('Power (MW)')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming your split data frames are named as follows:\n",
    "# (train_resampled_df_50, test_resampled_df_50)\n",
    "\n",
    "# Define the min and max values used for normalization\n",
    "min_train_30 = normalized_train_df_30.min().min()  # Get the minimum value from the training data\n",
    "max_train_30 = normalized_train_df_30.max().max()  # Get the maximum value from the training data\n",
    "\n",
    "# List of split data frames and site names\n",
    "split_resampled_dfs = [\n",
    "    (normalized_train_df_30, normalized_test_df_30, 'Site_30MV')\n",
    "]\n",
    "\n",
    "# Apply the GRU model to each set of split data frames and calculate metrics\n",
    "for i, (train_df, test_df, site_name) in enumerate(split_resampled_dfs):\n",
    "    print(f\"Evaluating LSTM model for resampled dataset {site_name} ({i+1})\")\n",
    "    train_evaluate_lstm(train_df=train_df,\n",
    "                        test_df=test_df,\n",
    "                        target_column='Power(MW)',\n",
    "                        lookback_steps=8, epochs=128, batch_size=64,\n",
    "                        site_name=site_name,\n",
    "                        min_val=min_train_30,\n",
    "                        max_val=max_train_30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f372b8ea",
   "metadata": {},
   "source": [
    "# Site_35MV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecda41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "# Function to create look-back features for time series forecasting\n",
    "def create_lookback_features(df, target_column, lookback_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - lookback_steps):\n",
    "        X.append(df.iloc[i:i + lookback_steps].values)\n",
    "        y.append(df.iloc[i + lookback_steps][target_column])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Function to calculate and print R squared and RMSE\n",
    "def calculate_metrics(y_true, y_pred, title):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    \n",
    "    print(f\"{title} - R squared: {r2}, RMSE: {rmse}\")\n",
    "\n",
    "# Function to reverse normalization\n",
    "def reverse_normalization(normalized_values, min_val, max_val):\n",
    "    return normalized_values * (max_val - min_val) + min_val\n",
    "\n",
    "# Function to train the LSTM model and predict on test data\n",
    "def train_evaluate_lstm(train_df, test_df, target_column='Power(MW)', lookback_steps=8, epochs=100, batch_size=64, site_name='Site', min_val=None, max_val=None):\n",
    "    # Create look-back features for training data\n",
    "    X_train, y_train = create_lookback_features(train_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape data for GRU: [samples, time steps, features]\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2])\n",
    "    \n",
    "    # Build the LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                   kernel_regularizer=regularizers.l2(0.001)))  # LSTM Layer with L2 regularization\n",
    "    model.add(BatchNormalization())  # BatchNormalization after first LSTM\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(LSTM(units=32, return_sequences=False, kernel_regularizer=regularizers.l2(0.001)))  # Second LSTM Layer\n",
    "    model.add(BatchNormalization())  # BatchNormalization after second LSTM\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    # Adding Dense layers with ReLU activation\n",
    "    model.add(Dense(24, activation='relu', kernel_regularizer=regularizers.l2(0.001)))  # Hidden Dense layer with ReLU\n",
    "    model.add(BatchNormalization())  # BatchNormalization after Dense layer\n",
    "    model.add(Dropout(0.2))  # Dropout layer after Dense layer\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))  # Output layer with Linear activation (default for regression)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1, shuffle=False)\n",
    "    \n",
    "    # Create look-back features for test data\n",
    "    X_test, y_test = create_lookback_features(test_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape test data for LSTM\n",
    "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2])\n",
    "    \n",
    "    # Predict on the test data\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Reverse normalization on both actual and predicted values\n",
    "    y_test_reversed = reverse_normalization(y_test, min_val, max_val)\n",
    "    y_pred_test_reversed = reverse_normalization(y_pred_test, min_val, max_val)\n",
    "    \n",
    "    # Calculate and print R squared and RMSE for the test set\n",
    "    calculate_metrics(y_test_reversed, y_pred_test_reversed, title=f'LSTM Model - {site_name} (Test Data)')\n",
    "\n",
    "    # Define the range of days you want to plot (90th to 100th days)\n",
    "    start_day = 90\n",
    "    end_day = 95\n",
    "    records_per_day = 48  # 24 hours * 2 records per hour (30-minute intervals)\n",
    "\n",
    "    # Calculate the indices corresponding to the 90th and 100th days\n",
    "    start_index = (start_day - 1) * records_per_day  # Start index for 90th day\n",
    "    end_index = end_day * records_per_day  # End index for 100th day\n",
    "\n",
    "    # Slice the test data and predictions for the range from the 90th to 100th day\n",
    "    y_test_range = y_test_reversed[start_index:end_index]\n",
    "    y_pred_test_range = y_pred_test_reversed[start_index:end_index]\n",
    "\n",
    "    # Plot the comparison between actual and predicted values for the 90th to 100th days\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(start_index, end_index), y_test_range, label='Actual', color='blue', linestyle='-', marker='o')\n",
    "    plt.plot(range(start_index, end_index), y_pred_test_range, label='Predicted', color='red', linestyle='--', marker='x')\n",
    "    plt.title(f'Actual vs Predicted Power (LSTM: LB=8, Epoch=128, Unit=64-32-24) - {site_name}')\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('Power (MW)')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming your split data frames are named as follows:\n",
    "# (train_resampled_df_50, test_resampled_df_50)\n",
    "\n",
    "# Define the min and max values used for normalization\n",
    "min_train_35 = normalized_train_df_35.min().min()  # Get the minimum value from the training data\n",
    "max_train_35 = normalized_train_df_35.max().max()  # Get the maximum value from the training data\n",
    "\n",
    "# List of split data frames and site names\n",
    "split_resampled_dfs = [\n",
    "    (normalized_train_df_35, normalized_test_df_35, 'Site_35MV')\n",
    "]\n",
    "\n",
    "# Apply the GRU model to each set of split data frames and calculate metrics\n",
    "for i, (train_df, test_df, site_name) in enumerate(split_resampled_dfs):\n",
    "    print(f\"Evaluating LSTM model for resampled dataset {site_name} ({i+1})\")\n",
    "    train_evaluate_lstm(train_df=train_df,\n",
    "                        test_df=test_df,\n",
    "                        target_column='Power(MW)',\n",
    "                        lookback_steps=8, epochs=128, batch_size=64,\n",
    "                        site_name=site_name,\n",
    "                        min_val=min_train_35,\n",
    "                        max_val=max_train_35)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3b3abf",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af62c4c",
   "metadata": {},
   "source": [
    "# Build GRU model with best combination of Hyperparameters of LSTM, then forcast On Test Data for all sites and plot Actual vs Predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8bc97f",
   "metadata": {},
   "source": [
    "# Site_50MV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea461702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "# Function to create look-back features for time series forecasting\n",
    "def create_lookback_features(df, target_column, lookback_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - lookback_steps):\n",
    "        X.append(df.iloc[i:i + lookback_steps].values)\n",
    "        y.append(df.iloc[i + lookback_steps][target_column])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Function to calculate and print R squared and RMSE\n",
    "def calculate_metrics(y_true, y_pred, title):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    \n",
    "    print(f\"{title} - R squared: {r2}, RMSE: {rmse}\")\n",
    "\n",
    "# Function to reverse normalization\n",
    "def reverse_normalization(normalized_values, min_val, max_val):\n",
    "    return normalized_values * (max_val - min_val) + min_val\n",
    "\n",
    "# Function to train the GRU model and predict on test data\n",
    "def train_evaluate_gru(train_df, test_df, target_column='Power(MW)', lookback_steps=8, epochs=100, batch_size=64, site_name='Site', min_val=None, max_val=None):\n",
    "    # Create look-back features for training data\n",
    "    X_train, y_train = create_lookback_features(train_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape data for GRU: [samples, time steps, features]\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2])\n",
    "    \n",
    "    # Build the GRU model\n",
    "    model = Sequential()\n",
    "    model.add(GRU(units=64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                   kernel_regularizer=regularizers.l2(0.001)))  # GRU Layer with L2 regularization\n",
    "    model.add(BatchNormalization())  # BatchNormalization after first GRU\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(GRU(units=32, return_sequences=False, kernel_regularizer=regularizers.l2(0.001)))  # Second GRU Layer\n",
    "    model.add(BatchNormalization())  # BatchNormalization after second GRU\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    # Adding Dense layers with ReLU activation\n",
    "    model.add(Dense(24, activation='relu', kernel_regularizer=regularizers.l2(0.001)))  # Hidden Dense layer with ReLU\n",
    "    model.add(BatchNormalization())  # BatchNormalization after Dense layer\n",
    "    model.add(Dropout(0.2))  # Dropout layer after Dense layer\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))  # Output layer with Linear activation (default for regression)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1, shuffle=False)\n",
    "    \n",
    "    # Create look-back features for test data\n",
    "    X_test, y_test = create_lookback_features(test_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape test data for GRU\n",
    "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2])\n",
    "    \n",
    "    # Predict on the test data\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Reverse normalization on both actual and predicted values\n",
    "    y_test_reversed = reverse_normalization(y_test, min_val, max_val)\n",
    "    y_pred_test_reversed = reverse_normalization(y_pred_test, min_val, max_val)\n",
    "    \n",
    "    # Calculate and print R squared and RMSE for the test set\n",
    "    calculate_metrics(y_test_reversed, y_pred_test_reversed, title=f'GRU Model - {site_name} (Test Data)')\n",
    "\n",
    "    # Define the range of days you want to plot (90th to 100th days)\n",
    "    start_day = 90\n",
    "    end_day = 95\n",
    "    records_per_day = 48  # 24 hours * 2 records per hour (30-minute intervals)\n",
    "\n",
    "    # Calculate the indices corresponding to the 90th and 100th days\n",
    "    start_index = (start_day - 1) * records_per_day  # Start index for 95th day\n",
    "    end_index = end_day * records_per_day  # End index for 100th day\n",
    "\n",
    "    # Slice the test data and predictions for the range from the 90th to 95th day\n",
    "    y_test_range = y_test_reversed[start_index:end_index]\n",
    "    y_pred_test_range = y_pred_test_reversed[start_index:end_index]\n",
    "\n",
    "    # Plot the comparison between actual and predicted values for the 90th to 95th days\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(start_index, end_index), y_test_range, label='Actual', color='blue', linestyle='-', marker='o')\n",
    "    plt.plot(range(start_index, end_index), y_pred_test_range, label='Predicted', color='red', linestyle='--', marker='x')\n",
    "    plt.title(f'Actual vs Predicted Power (GRU: LB=8, Epoch=128, Unit=64-32-24) - {site_name}')\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('Power (MW)')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming your split data frames are named as follows:\n",
    "# (train_resampled_df_50, test_resampled_df_50)\n",
    "\n",
    "# Define the min and max values used for normalization\n",
    "min_train_50 = normalized_train_df_50.min().min()  # Get the minimum value from the training data\n",
    "max_train_50 = normalized_train_df_50.max().max()  # Get the maximum value from the training data\n",
    "\n",
    "# List of split data frames and site names\n",
    "split_resampled_dfs = [\n",
    "    (normalized_train_df_50, normalized_test_df_50, 'Site_50MV')\n",
    "]\n",
    "\n",
    "# Apply the GRU model to each set of split data frames and calculate metrics\n",
    "for i, (train_df, test_df, site_name) in enumerate(split_resampled_dfs):\n",
    "    print(f\"Evaluating GRU model for resampled dataset {site_name} ({i+1})\")\n",
    "    train_evaluate_gru(train_df=train_df,\n",
    "                        test_df=test_df,\n",
    "                        target_column='Power(MW)',\n",
    "                        lookback_steps=8, epochs=128, batch_size=64,\n",
    "                        site_name=site_name,\n",
    "                        min_val=min_train_50,\n",
    "                        max_val=max_train_50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0bce81",
   "metadata": {},
   "source": [
    "# Site_130MV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202d32d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "# Function to create look-back features for time series forecasting\n",
    "def create_lookback_features(df, target_column, lookback_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - lookback_steps):\n",
    "        X.append(df.iloc[i:i + lookback_steps].values)\n",
    "        y.append(df.iloc[i + lookback_steps][target_column])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Function to calculate and print R squared and RMSE\n",
    "def calculate_metrics(y_true, y_pred, title):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    \n",
    "    print(f\"{title} - R squared: {r2}, RMSE: {rmse}\")\n",
    "\n",
    "# Function to reverse normalization\n",
    "def reverse_normalization(normalized_values, min_val, max_val):\n",
    "    return normalized_values * (max_val - min_val) + min_val\n",
    "\n",
    "# Function to train the GRU model and predict on test data\n",
    "def train_evaluate_gru(train_df, test_df, target_column='Power(MW)', lookback_steps=8, epochs=100, batch_size=64, site_name='Site', min_val=None, max_val=None):\n",
    "    # Create look-back features for training data\n",
    "    X_train, y_train = create_lookback_features(train_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape data for GRU: [samples, time steps, features]\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2])\n",
    "    \n",
    "    # Build the GRU model\n",
    "    model = Sequential()\n",
    "    model.add(GRU(units=64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                   kernel_regularizer=regularizers.l2(0.001)))  # GRU Layer with L2 regularization\n",
    "    model.add(BatchNormalization())  # BatchNormalization after first GRU\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(GRU(units=32, return_sequences=False, kernel_regularizer=regularizers.l2(0.001)))  # Second GRU Layer\n",
    "    model.add(BatchNormalization())  # BatchNormalization after second GRU\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    # Adding Dense layers with ReLU activation\n",
    "    model.add(Dense(24, activation='relu', kernel_regularizer=regularizers.l2(0.001)))  # Hidden Dense layer with ReLU\n",
    "    model.add(BatchNormalization())  # BatchNormalization after Dense layer\n",
    "    model.add(Dropout(0.2))  # Dropout layer after Dense layer\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))  # Output layer with Linear activation (default for regression)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1, shuffle=False)\n",
    "    \n",
    "    # Create look-back features for test data\n",
    "    X_test, y_test = create_lookback_features(test_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape test data for GRU\n",
    "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2])\n",
    "    \n",
    "    # Predict on the test data\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Reverse normalization on both actual and predicted values\n",
    "    y_test_reversed = reverse_normalization(y_test, min_val, max_val)\n",
    "    y_pred_test_reversed = reverse_normalization(y_pred_test, min_val, max_val)\n",
    "    \n",
    "    # Calculate and print R squared and RMSE for the test set\n",
    "    calculate_metrics(y_test_reversed, y_pred_test_reversed, title=f'GRU Model - {site_name} (Test Data)')\n",
    "\n",
    "    # Define the range of days you want to plot (90th to 100th days)\n",
    "    start_day = 90\n",
    "    end_day = 95\n",
    "    records_per_day = 48  # 24 hours * 2 records per hour (30-minute intervals)\n",
    "\n",
    "    # Calculate the indices corresponding to the 90th and 100th days\n",
    "    start_index = (start_day - 1) * records_per_day  # Start index for 95th day\n",
    "    end_index = end_day * records_per_day  # End index for 100th day\n",
    "\n",
    "    # Slice the test data and predictions for the range from the 90th to 95th day\n",
    "    y_test_range = y_test_reversed[start_index:end_index]\n",
    "    y_pred_test_range = y_pred_test_reversed[start_index:end_index]\n",
    "\n",
    "    # Plot the comparison between actual and predicted values for the 90th to 95th days\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(start_index, end_index), y_test_range, label='Actual', color='blue', linestyle='-', marker='o')\n",
    "    plt.plot(range(start_index, end_index), y_pred_test_range, label='Predicted', color='red', linestyle='--', marker='x')\n",
    "    plt.title(f'Actual vs Predicted Power (GRU: LB=8, Epoch=128, Unit=64-32-24) - {site_name}')\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('Power (MW)')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming your split data frames are named as follows:\n",
    "# (train_resampled_df_50, test_resampled_df_50)\n",
    "\n",
    "# Define the min and max values used for normalization\n",
    "min_train_130 = normalized_train_df_130.min().min()  # Get the minimum value from the training data\n",
    "max_train_130 = normalized_train_df_130.max().max()  # Get the maximum value from the training data\n",
    "\n",
    "# List of split data frames and site names\n",
    "split_resampled_dfs = [\n",
    "    (normalized_train_df_130, normalized_test_df_130, 'Site_130MV')\n",
    "]\n",
    "\n",
    "# Apply the GRU model to each set of split data frames and calculate metrics\n",
    "for i, (train_df, test_df, site_name) in enumerate(split_resampled_dfs):\n",
    "    print(f\"Evaluating GRU model for resampled dataset {site_name} ({i+1})\")\n",
    "    train_evaluate_gru(train_df=train_df,\n",
    "                        test_df=test_df,\n",
    "                        target_column='Power(MW)',\n",
    "                        lookback_steps=8, epochs=128, batch_size=64,\n",
    "                        site_name=site_name,\n",
    "                        min_val=min_train_130,\n",
    "                        max_val=max_train_130)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5de8c7",
   "metadata": {},
   "source": [
    "# Site_30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a66e722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "# Function to create look-back features for time series forecasting\n",
    "def create_lookback_features(df, target_column, lookback_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - lookback_steps):\n",
    "        X.append(df.iloc[i:i + lookback_steps].values)\n",
    "        y.append(df.iloc[i + lookback_steps][target_column])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Function to calculate and print R squared and RMSE\n",
    "def calculate_metrics(y_true, y_pred, title):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    \n",
    "    print(f\"{title} - R squared: {r2}, RMSE: {rmse}\")\n",
    "\n",
    "# Function to reverse normalization\n",
    "def reverse_normalization(normalized_values, min_val, max_val):\n",
    "    return normalized_values * (max_val - min_val) + min_val\n",
    "\n",
    "# Function to train the GRU model and predict on test data\n",
    "def train_evaluate_gru(train_df, test_df, target_column='Power(MW)', lookback_steps=8, epochs=100, batch_size=64, site_name='Site', min_val=None, max_val=None):\n",
    "    # Create look-back features for training data\n",
    "    X_train, y_train = create_lookback_features(train_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape data for GRU: [samples, time steps, features]\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2])\n",
    "    \n",
    "    # Build the GRU model\n",
    "    model = Sequential()\n",
    "    model.add(GRU(units=64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                   kernel_regularizer=regularizers.l2(0.001)))  # GRU Layer with L2 regularization\n",
    "    model.add(BatchNormalization())  # BatchNormalization after first GRU\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(GRU(units=32, return_sequences=False, kernel_regularizer=regularizers.l2(0.001)))  # Second GRU Layer\n",
    "    model.add(BatchNormalization())  # BatchNormalization after second GRU\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    # Adding Dense layers with ReLU activation\n",
    "    model.add(Dense(24, activation='relu', kernel_regularizer=regularizers.l2(0.001)))  # Hidden Dense layer with ReLU\n",
    "    model.add(BatchNormalization())  # BatchNormalization after Dense layer\n",
    "    model.add(Dropout(0.2))  # Dropout layer after Dense layer\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))  # Output layer with Linear activation (default for regression)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1, shuffle=False)\n",
    "    \n",
    "    # Create look-back features for test data\n",
    "    X_test, y_test = create_lookback_features(test_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape test data for GRU\n",
    "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2])\n",
    "    \n",
    "    # Predict on the test data\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Reverse normalization on both actual and predicted values\n",
    "    y_test_reversed = reverse_normalization(y_test, min_val, max_val)\n",
    "    y_pred_test_reversed = reverse_normalization(y_pred_test, min_val, max_val)\n",
    "    \n",
    "    # Calculate and print R squared and RMSE for the test set\n",
    "    calculate_metrics(y_test_reversed, y_pred_test_reversed, title=f'GRU Model - {site_name} (Test Data)')\n",
    "\n",
    "    # Define the range of days you want to plot (90th to 100th days)\n",
    "    start_day = 90\n",
    "    end_day = 95\n",
    "    records_per_day = 48  # 24 hours * 2 records per hour (30-minute intervals)\n",
    "\n",
    "    # Calculate the indices corresponding to the 90th and 100th days\n",
    "    start_index = (start_day - 1) * records_per_day  # Start index for 95th day\n",
    "    end_index = end_day * records_per_day  # End index for 100th day\n",
    "\n",
    "    # Slice the test data and predictions for the range from the 90th to 95th day\n",
    "    y_test_range = y_test_reversed[start_index:end_index]\n",
    "    y_pred_test_range = y_pred_test_reversed[start_index:end_index]\n",
    "\n",
    "    # Plot the comparison between actual and predicted values for the 90th to 95th days\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(start_index, end_index), y_test_range, label='Actual', color='blue', linestyle='-', marker='o')\n",
    "    plt.plot(range(start_index, end_index), y_pred_test_range, label='Predicted', color='red', linestyle='--', marker='x')\n",
    "    plt.title(f'Actual vs Predicted Power (GRU: LB=8, Epoch=128, Unit=64-32-24) - {site_name}')\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('Power (MW)')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming your split data frames are named as follows:\n",
    "# (train_resampled_df_50, test_resampled_df_50)\n",
    "\n",
    "# Define the min and max values used for normalization\n",
    "min_train_30 = normalized_train_df_30.min().min()  # Get the minimum value from the training data\n",
    "max_train_30 = normalized_train_df_30.max().max()  # Get the maximum value from the training data\n",
    "\n",
    "# List of split data frames and site names\n",
    "split_resampled_dfs = [\n",
    "    (normalized_train_df_30, normalized_test_df_30, 'Site_30MV')\n",
    "]\n",
    "\n",
    "# Apply the GRU model to each set of split data frames and calculate metrics\n",
    "for i, (train_df, test_df, site_name) in enumerate(split_resampled_dfs):\n",
    "    print(f\"Evaluating GRU model for resampled dataset {site_name} ({i+1})\")\n",
    "    train_evaluate_gru(train_df=train_df,\n",
    "                        test_df=test_df,\n",
    "                        target_column='Power(MW)',\n",
    "                        lookback_steps=8, epochs=128, batch_size=64,\n",
    "                        site_name=site_name,\n",
    "                        min_val=min_train_30,\n",
    "                        max_val=max_train_30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad391ae",
   "metadata": {},
   "source": [
    "# Site_35MV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0272454a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "# Function to create look-back features for time series forecasting\n",
    "def create_lookback_features(df, target_column, lookback_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - lookback_steps):\n",
    "        X.append(df.iloc[i:i + lookback_steps].values)\n",
    "        y.append(df.iloc[i + lookback_steps][target_column])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Function to calculate and print R squared and RMSE\n",
    "def calculate_metrics(y_true, y_pred, title):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    \n",
    "    print(f\"{title} - R squared: {r2}, RMSE: {rmse}\")\n",
    "\n",
    "# Function to reverse normalization\n",
    "def reverse_normalization(normalized_values, min_val, max_val):\n",
    "    return normalized_values * (max_val - min_val) + min_val\n",
    "\n",
    "# Function to train the GRU model and predict on test data\n",
    "def train_evaluate_gru(train_df, test_df, target_column='Power(MW)', lookback_steps=8, epochs=100, batch_size=64, site_name='Site', min_val=None, max_val=None):\n",
    "    # Create look-back features for training data\n",
    "    X_train, y_train = create_lookback_features(train_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape data for GRU: [samples, time steps, features]\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2])\n",
    "    \n",
    "    # Build the GRU model\n",
    "    model = Sequential()\n",
    "    model.add(GRU(units=64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                   kernel_regularizer=regularizers.l2(0.001)))  # GRU Layer with L2 regularization\n",
    "    model.add(BatchNormalization())  # BatchNormalization after first GRU\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(GRU(units=32, return_sequences=False, kernel_regularizer=regularizers.l2(0.001)))  # Second GRU Layer\n",
    "    model.add(BatchNormalization())  # BatchNormalization after second GRU\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    # Adding Dense layers with ReLU activation\n",
    "    model.add(Dense(24, activation='relu', kernel_regularizer=regularizers.l2(0.001)))  # Hidden Dense layer with ReLU\n",
    "    model.add(BatchNormalization())  # BatchNormalization after Dense layer\n",
    "    model.add(Dropout(0.2))  # Dropout layer after Dense layer\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))  # Output layer with Linear activation (default for regression)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1, shuffle=False)\n",
    "    \n",
    "    # Create look-back features for test data\n",
    "    X_test, y_test = create_lookback_features(test_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape test data for GRU\n",
    "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2])\n",
    "    \n",
    "    # Predict on the test data\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Reverse normalization on both actual and predicted values\n",
    "    y_test_reversed = reverse_normalization(y_test, min_val, max_val)\n",
    "    y_pred_test_reversed = reverse_normalization(y_pred_test, min_val, max_val)\n",
    "    \n",
    "    # Calculate and print R squared and RMSE for the test set\n",
    "    calculate_metrics(y_test_reversed, y_pred_test_reversed, title=f'GRU Model - {site_name} (Test Data)')\n",
    "\n",
    "    # Define the range of days you want to plot (90th to 100th days)\n",
    "    start_day = 90\n",
    "    end_day = 95\n",
    "    records_per_day = 48  # 24 hours * 2 records per hour (30-minute intervals)\n",
    "\n",
    "    # Calculate the indices corresponding to the 90th and 100th days\n",
    "    start_index = (start_day - 1) * records_per_day  # Start index for 95th day\n",
    "    end_index = end_day * records_per_day  # End index for 100th day\n",
    "\n",
    "    # Slice the test data and predictions for the range from the 90th to 95th day\n",
    "    y_test_range = y_test_reversed[start_index:end_index]\n",
    "    y_pred_test_range = y_pred_test_reversed[start_index:end_index]\n",
    "\n",
    "    # Plot the comparison between actual and predicted values for the 90th to 95th days\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(start_index, end_index), y_test_range, label='Actual', color='blue', linestyle='-', marker='o')\n",
    "    plt.plot(range(start_index, end_index), y_pred_test_range, label='Predicted', color='red', linestyle='--', marker='x')\n",
    "    plt.title(f'Actual vs Predicted Power (GRU: LB=8, Epoch=128, Unit=64-32-24) - {site_name}')\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('Power (MW)')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming your split data frames are named as follows:\n",
    "# (train_resampled_df_50, test_resampled_df_50)\n",
    "\n",
    "# Define the min and max values used for normalization\n",
    "min_train_35 = normalized_train_df_35.min().min()  # Get the minimum value from the training data\n",
    "max_train_35 = normalized_train_df_35.max().max()  # Get the maximum value from the training data\n",
    "\n",
    "# List of split data frames and site names\n",
    "split_resampled_dfs = [\n",
    "    (normalized_train_df_35, normalized_test_df_35, 'Site_35MV')\n",
    "]\n",
    "\n",
    "# Apply the GRU model to each set of split data frames and calculate metrics\n",
    "for i, (train_df, test_df, site_name) in enumerate(split_resampled_dfs):\n",
    "    print(f\"Evaluating GRU model for resampled dataset {site_name} ({i+1})\")\n",
    "    train_evaluate_gru(train_df=train_df,\n",
    "                        test_df=test_df,\n",
    "                        target_column='Power(MW)',\n",
    "                        lookback_steps=8, epochs=128, batch_size=64,\n",
    "                        site_name=site_name,\n",
    "                        min_val=min_train_35,\n",
    "                        max_val=max_train_35)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200f07f4",
   "metadata": {},
   "source": [
    "# LSTM Vs GRU On Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d245087f",
   "metadata": {},
   "source": [
    "# Site_50MV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c7adfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Function to create look-back features for time series forecasting\n",
    "def create_lookback_features(df, target_column, lookback_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - lookback_steps):\n",
    "        X.append(df.iloc[i:i + lookback_steps].values)  # Slice features\n",
    "        y.append(df.iloc[i + lookback_steps][target_column])  # Target is the next value\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Function to reverse normalization\n",
    "def reverse_normalization(normalized_values, min_val, max_val):\n",
    "    return normalized_values * (max_val - min_val) + min_val\n",
    "\n",
    "# Function to calculate and print R squared and RMSE\n",
    "def calculate_metrics(y_true_train, y_pred_train, y_true_val, y_pred_val, title, best_epoch=None):\n",
    "    r2_train = r2_score(y_true_train, y_pred_train)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_true_train, y_pred_train))\n",
    "    \n",
    "    r2_val = r2_score(y_true_val, y_pred_val)\n",
    "    rmse_val = np.sqrt(mean_squared_error(y_true_val, y_pred_val))\n",
    "    \n",
    "    print(f\"{title} - Train R squared: {r2_train}, RMSE: {rmse_train}\")\n",
    "    print(f\"{title} - Validation R squared: {r2_val}, RMSE: {rmse_val}\")\n",
    "    \n",
    "    if best_epoch is not None:\n",
    "        print(f\"Best Epoch: {best_epoch}\")\n",
    "\n",
    "# Function to build and train LSTM model\n",
    "def build_train_lstm(X_train, y_train, X_val, y_val, lookback_steps, epochs, batch_size, min_train, max_train):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                   kernel_regularizer=regularizers.l2(0.001)))  # LSTM Layer with L2 regularization\n",
    "    model.add(BatchNormalization())  # BatchNormalization after LSTM\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(LSTM(units=32, return_sequences=False, kernel_regularizer=regularizers.l2(0.001)))  # Second LSTM Layer\n",
    "    model.add(BatchNormalization())  # BatchNormalization after second LSTM\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(Dense(24, activation='relu', kernel_regularizer=regularizers.l2(0.001)))  # Dense layer with ReLU\n",
    "    model.add(Dropout(0.2))  # Dropout layer after Dense layer\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))  # Output layer\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    r2_train_list, rmse_train_list, r2_val_list, rmse_val_list = [], [], [], []\n",
    "    best_epoch_r2, best_epoch_rmse = -np.inf, np.inf\n",
    "    best_epoch_r2_index, best_epoch_rmse_index = None, None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.fit(X_train, y_train, epochs=1, batch_size=batch_size, verbose=0,shuffle=False)\n",
    "        \n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_val = model.predict(X_val)\n",
    "        \n",
    "        # Reverse normalization for both predictions and true values\n",
    "        y_pred_train = reverse_normalization(y_pred_train, min_train, max_train)\n",
    "        y_pred_val = reverse_normalization(y_pred_val, min_train, max_train)\n",
    "        y_train_orig = reverse_normalization(y_train, min_train, max_train)\n",
    "        y_val_orig = reverse_normalization(y_val, min_train, max_train)\n",
    "        \n",
    "        # Calculate R² and RMSE\n",
    "        r2_train = r2_score(y_train_orig, y_pred_train)\n",
    "        rmse_train = np.sqrt(mean_squared_error(y_train_orig, y_pred_train))\n",
    "        \n",
    "        r2_val = r2_score(y_val_orig, y_pred_val)\n",
    "        rmse_val = np.sqrt(mean_squared_error(y_val_orig, y_pred_val))\n",
    "        \n",
    "        r2_train_list.append(r2_train)\n",
    "        rmse_train_list.append(rmse_train)\n",
    "        r2_val_list.append(r2_val)\n",
    "        rmse_val_list.append(rmse_val)\n",
    "        \n",
    "        # Track best epoch for R² (max) and RMSE (min)\n",
    "        if r2_val > best_epoch_r2:\n",
    "            best_epoch_r2 = r2_val\n",
    "            best_epoch_r2_index = epoch\n",
    "        \n",
    "        if rmse_val < best_epoch_rmse:\n",
    "            best_epoch_rmse = rmse_val\n",
    "            best_epoch_rmse_index = epoch\n",
    "    \n",
    "    return r2_train_list, rmse_train_list, r2_val_list, rmse_val_list, best_epoch_r2_index, best_epoch_rmse_index\n",
    "\n",
    "# Function to build and train GRU model\n",
    "def build_train_gru(X_train, y_train, X_val, y_val, lookback_steps, epochs, batch_size, min_train, max_train):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(units=64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                  kernel_regularizer=regularizers.l2(0.001)))  # GRU Layer with L2 regularization\n",
    "    model.add(BatchNormalization())  # BatchNormalization after GRU\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(GRU(units=32, return_sequences=False, kernel_regularizer=regularizers.l2(0.001)))  # Second GRU Layer\n",
    "    model.add(BatchNormalization())  # BatchNormalization after second GRU\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(Dense(24, activation='relu', kernel_regularizer=regularizers.l2(0.001)))  # Dense layer with ReLU\n",
    "    model.add(Dropout(0.2))  # Dropout layer after Dense layer\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))  # Output layer\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    r2_train_list, rmse_train_list, r2_val_list, rmse_val_list = [], [], [], []\n",
    "    best_epoch_r2, best_epoch_rmse = -np.inf, np.inf\n",
    "    best_epoch_r2_index, best_epoch_rmse_index = None, None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.fit(X_train, y_train, epochs=1, batch_size=batch_size, verbose=0,shuffle=False)\n",
    "        \n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_val = model.predict(X_val)\n",
    "        \n",
    "        # Reverse normalization for both predictions and true values\n",
    "        y_pred_train = reverse_normalization(y_pred_train, min_train, max_train)\n",
    "        y_pred_val = reverse_normalization(y_pred_val, min_train, max_train)\n",
    "        y_train_orig = reverse_normalization(y_train, min_train, max_train)\n",
    "        y_val_orig = reverse_normalization(y_val, min_train, max_train)\n",
    "        \n",
    "        # Calculate R² and RMSE\n",
    "        r2_train = r2_score(y_train_orig, y_pred_train)\n",
    "        rmse_train = np.sqrt(mean_squared_error(y_train_orig, y_pred_train))\n",
    "        \n",
    "        r2_val = r2_score(y_val_orig, y_pred_val)\n",
    "        rmse_val = np.sqrt(mean_squared_error(y_val_orig, y_pred_val))\n",
    "        \n",
    "        r2_train_list.append(r2_train)\n",
    "        rmse_train_list.append(rmse_train)\n",
    "        r2_val_list.append(r2_val)\n",
    "        rmse_val_list.append(rmse_val)\n",
    "        \n",
    "        # Track best epoch for R² (max) and RMSE (min)\n",
    "        if r2_val > best_epoch_r2:\n",
    "            best_epoch_r2 = r2_val\n",
    "            best_epoch_r2_index = epoch\n",
    "        \n",
    "        if rmse_val < best_epoch_rmse:\n",
    "            best_epoch_rmse = rmse_val\n",
    "            best_epoch_rmse_index = epoch\n",
    "    \n",
    "    return r2_train_list, rmse_train_list, r2_val_list, rmse_val_list, best_epoch_r2_index, best_epoch_rmse_index\n",
    "\n",
    "# Function to plot metrics for both LSTM and GRU models with different reds for RMSE\n",
    "def plot_metrics(r2_train_lstm, rmse_train_lstm, r2_val_lstm, rmse_val_lstm, \n",
    "                 r2_train_gru, rmse_train_gru, r2_val_gru, rmse_val_gru, \n",
    "                 best_epoch_r2_lstm, best_epoch_rmse_lstm,\n",
    "                 best_epoch_r2_gru, best_epoch_rmse_gru, epochs):\n",
    "    \n",
    "    fig, axs = plt.subplots(2, 1, figsize=(10, 12))\n",
    "    \n",
    "    # Plot R² for train and validation sets (keeping previous colors)\n",
    "    axs[0].plot(range(epochs), r2_train_lstm, label='LSTM Train R²', color='blue', linestyle='-', marker='o')\n",
    "    axs[0].plot(range(epochs), r2_val_lstm, label='LSTM Test R²', color='blue', linestyle='--')\n",
    "    axs[0].plot(range(epochs), r2_train_gru, label='GRU Train R²', color='green', linestyle='-', marker='o')\n",
    "    axs[0].plot(range(epochs), r2_val_gru, label='GRU Test R²', color='green', linestyle='--')\n",
    "    axs[0].set_title('R² for LSTM and GRU Models (LB=8, Epochs=128, Units=64-32-24) - Site_50MV')\n",
    "    axs[0].set_xlabel('Epochs')\n",
    "    axs[0].set_ylabel('R²')\n",
    "    axs[0].legend(loc='lower right')\n",
    "    axs[0].grid(True)\n",
    "    \n",
    "    # Plot RMSE for train and validation sets with different red and orange shades\n",
    "    axs[1].plot(range(epochs), rmse_train_lstm, label='LSTM Train RMSE', color='darkorange', linestyle='-', marker='o')\n",
    "    axs[1].plot(range(epochs), rmse_val_lstm, label='LSTM Test RMSE', color='orangered', linestyle='--')\n",
    "    axs[1].plot(range(epochs), rmse_train_gru, label='GRU Train RMSE', color='darkred', linestyle='-', marker='o')\n",
    "    axs[1].plot(range(epochs), rmse_val_gru, label='GRU Test RMSE', color='firebrick', linestyle='--')\n",
    "    axs[1].set_title('RMSE for LSTM and GRU Models (LB=8, Epochs=128, Units=64-32-24) - Site_50MV')\n",
    "    axs[1].set_xlabel('Epochs')\n",
    "    axs[1].set_ylabel('RMSE')\n",
    "    axs[1].legend(loc='upper right')\n",
    "    axs[1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Load your training and validation data (assuming they are already normalized and available)\n",
    "# Train and validate LSTM model\n",
    "lookback_steps = 8\n",
    "epochs = 128\n",
    "batch_size = 64\n",
    "\n",
    "train_df = normalized_train_df_50 # This is the training DataFrame\n",
    "val_df = normalized_test_df_50 \n",
    "X_train, y_train = create_lookback_features(train_df, 'Power(MW)', lookback_steps)\n",
    "X_val, y_val = create_lookback_features(val_df, 'Power(MW)', lookback_steps)\n",
    "\n",
    "# Normalize features and target for LSTM/GRU\n",
    "min_train, max_train = np.min(y_train), np.max(y_train)\n",
    "X_train = (X_train - X_train.min(axis=0)) / (X_train.max(axis=0) - X_train.min(axis=0))\n",
    "X_val = (X_val - X_val.min(axis=0)) / (X_val.max(axis=0) - X_val.min(axis=0))\n",
    "\n",
    "# Train LSTM\n",
    "r2_train_lstm, rmse_train_lstm, r2_val_lstm, rmse_val_lstm, best_epoch_r2_lstm, best_epoch_rmse_lstm = build_train_lstm(\n",
    "    X_train, y_train, X_val, y_val, lookback_steps, epochs, batch_size, min_train, max_train\n",
    ")\n",
    "\n",
    "# Train GRU\n",
    "r2_train_gru, rmse_train_gru, r2_val_gru, rmse_val_gru, best_epoch_r2_gru, best_epoch_rmse_gru = build_train_gru(\n",
    "    X_train, y_train, X_val, y_val, lookback_steps, epochs, batch_size, min_train, max_train\n",
    ")\n",
    "\n",
    "# Plot metrics for both models\n",
    "plot_metrics(r2_train_lstm, rmse_train_lstm, r2_val_lstm, rmse_val_lstm,\n",
    "             r2_train_gru, rmse_train_gru, r2_val_gru, rmse_val_gru,\n",
    "             best_epoch_r2_lstm, best_epoch_rmse_lstm,\n",
    "             best_epoch_r2_gru, best_epoch_rmse_gru, epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e76d944",
   "metadata": {},
   "source": [
    "# Site_130MV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a51955",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Function to create look-back features for time series forecasting\n",
    "def create_lookback_features(df, target_column, lookback_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - lookback_steps):\n",
    "        X.append(df.iloc[i:i + lookback_steps].values)  # Slice features\n",
    "        y.append(df.iloc[i + lookback_steps][target_column])  # Target is the next value\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Function to reverse normalization\n",
    "def reverse_normalization(normalized_values, min_val, max_val):\n",
    "    return normalized_values * (max_val - min_val) + min_val\n",
    "\n",
    "# Function to calculate and print R squared and RMSE\n",
    "def calculate_metrics(y_true_train, y_pred_train, y_true_val, y_pred_val, title, best_epoch=None):\n",
    "    r2_train = r2_score(y_true_train, y_pred_train)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_true_train, y_pred_train))\n",
    "    \n",
    "    r2_val = r2_score(y_true_val, y_pred_val)\n",
    "    rmse_val = np.sqrt(mean_squared_error(y_true_val, y_pred_val))\n",
    "    \n",
    "    print(f\"{title} - Train R squared: {r2_train}, RMSE: {rmse_train}\")\n",
    "    print(f\"{title} - Validation R squared: {r2_val}, RMSE: {rmse_val}\")\n",
    "    \n",
    "    if best_epoch is not None:\n",
    "        print(f\"Best Epoch: {best_epoch}\")\n",
    "\n",
    "# Function to build and train LSTM model\n",
    "def build_train_lstm(X_train, y_train, X_val, y_val, lookback_steps, epochs, batch_size, min_train, max_train):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                   kernel_regularizer=regularizers.l2(0.001)))  # LSTM Layer with L2 regularization\n",
    "    model.add(BatchNormalization())  # BatchNormalization after LSTM\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(LSTM(units=32, return_sequences=False, kernel_regularizer=regularizers.l2(0.001)))  # Second LSTM Layer\n",
    "    model.add(BatchNormalization())  # BatchNormalization after second LSTM\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(Dense(24, activation='relu', kernel_regularizer=regularizers.l2(0.001)))  # Dense layer with ReLU\n",
    "    model.add(Dropout(0.2))  # Dropout layer after Dense layer\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))  # Output layer\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    r2_train_list, rmse_train_list, r2_val_list, rmse_val_list = [], [], [], []\n",
    "    best_epoch_r2, best_epoch_rmse = -np.inf, np.inf\n",
    "    best_epoch_r2_index, best_epoch_rmse_index = None, None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.fit(X_train, y_train, epochs=1, batch_size=batch_size, verbose=0,shuffle=False)\n",
    "        \n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_val = model.predict(X_val)\n",
    "        \n",
    "        # Reverse normalization for both predictions and true values\n",
    "        y_pred_train = reverse_normalization(y_pred_train, min_train, max_train)\n",
    "        y_pred_val = reverse_normalization(y_pred_val, min_train, max_train)\n",
    "        y_train_orig = reverse_normalization(y_train, min_train, max_train)\n",
    "        y_val_orig = reverse_normalization(y_val, min_train, max_train)\n",
    "        \n",
    "        # Calculate R² and RMSE\n",
    "        r2_train = r2_score(y_train_orig, y_pred_train)\n",
    "        rmse_train = np.sqrt(mean_squared_error(y_train_orig, y_pred_train))\n",
    "        \n",
    "        r2_val = r2_score(y_val_orig, y_pred_val)\n",
    "        rmse_val = np.sqrt(mean_squared_error(y_val_orig, y_pred_val))\n",
    "        \n",
    "        r2_train_list.append(r2_train)\n",
    "        rmse_train_list.append(rmse_train)\n",
    "        r2_val_list.append(r2_val)\n",
    "        rmse_val_list.append(rmse_val)\n",
    "        \n",
    "        # Track best epoch for R² (max) and RMSE (min)\n",
    "        if r2_val > best_epoch_r2:\n",
    "            best_epoch_r2 = r2_val\n",
    "            best_epoch_r2_index = epoch\n",
    "        \n",
    "        if rmse_val < best_epoch_rmse:\n",
    "            best_epoch_rmse = rmse_val\n",
    "            best_epoch_rmse_index = epoch\n",
    "    \n",
    "    return r2_train_list, rmse_train_list, r2_val_list, rmse_val_list, best_epoch_r2_index, best_epoch_rmse_index\n",
    "\n",
    "# Function to build and train GRU model\n",
    "def build_train_gru(X_train, y_train, X_val, y_val, lookback_steps, epochs, batch_size, min_train, max_train):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(units=64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                  kernel_regularizer=regularizers.l2(0.001)))  # GRU Layer with L2 regularization\n",
    "    model.add(BatchNormalization())  # BatchNormalization after GRU\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(GRU(units=32, return_sequences=False, kernel_regularizer=regularizers.l2(0.001)))  # Second GRU Layer\n",
    "    model.add(BatchNormalization())  # BatchNormalization after second GRU\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(Dense(24, activation='relu', kernel_regularizer=regularizers.l2(0.001)))  # Dense layer with ReLU\n",
    "    model.add(Dropout(0.2))  # Dropout layer after Dense layer\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))  # Output layer\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    r2_train_list, rmse_train_list, r2_val_list, rmse_val_list = [], [], [], []\n",
    "    best_epoch_r2, best_epoch_rmse = -np.inf, np.inf\n",
    "    best_epoch_r2_index, best_epoch_rmse_index = None, None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.fit(X_train, y_train, epochs=1, batch_size=batch_size, verbose=0,shuffle=False)\n",
    "        \n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_val = model.predict(X_val)\n",
    "        \n",
    "        # Reverse normalization for both predictions and true values\n",
    "        y_pred_train = reverse_normalization(y_pred_train, min_train, max_train)\n",
    "        y_pred_val = reverse_normalization(y_pred_val, min_train, max_train)\n",
    "        y_train_orig = reverse_normalization(y_train, min_train, max_train)\n",
    "        y_val_orig = reverse_normalization(y_val, min_train, max_train)\n",
    "        \n",
    "        # Calculate R² and RMSE\n",
    "        r2_train = r2_score(y_train_orig, y_pred_train)\n",
    "        rmse_train = np.sqrt(mean_squared_error(y_train_orig, y_pred_train))\n",
    "        \n",
    "        r2_val = r2_score(y_val_orig, y_pred_val)\n",
    "        rmse_val = np.sqrt(mean_squared_error(y_val_orig, y_pred_val))\n",
    "        \n",
    "        r2_train_list.append(r2_train)\n",
    "        rmse_train_list.append(rmse_train)\n",
    "        r2_val_list.append(r2_val)\n",
    "        rmse_val_list.append(rmse_val)\n",
    "        \n",
    "        # Track best epoch for R² (max) and RMSE (min)\n",
    "        if r2_val > best_epoch_r2:\n",
    "            best_epoch_r2 = r2_val\n",
    "            best_epoch_r2_index = epoch\n",
    "        \n",
    "        if rmse_val < best_epoch_rmse:\n",
    "            best_epoch_rmse = rmse_val\n",
    "            best_epoch_rmse_index = epoch\n",
    "    \n",
    "    return r2_train_list, rmse_train_list, r2_val_list, rmse_val_list, best_epoch_r2_index, best_epoch_rmse_index\n",
    "\n",
    "# Function to plot metrics for both LSTM and GRU models with different reds for RMSE\n",
    "def plot_metrics(r2_train_lstm, rmse_train_lstm, r2_val_lstm, rmse_val_lstm, \n",
    "                 r2_train_gru, rmse_train_gru, r2_val_gru, rmse_val_gru, \n",
    "                 best_epoch_r2_lstm, best_epoch_rmse_lstm,\n",
    "                 best_epoch_r2_gru, best_epoch_rmse_gru, epochs):\n",
    "    \n",
    "    fig, axs = plt.subplots(2, 1, figsize=(10, 12))\n",
    "    \n",
    "    # Plot R² for train and validation sets (keeping previous colors)\n",
    "    axs[0].plot(range(epochs), r2_train_lstm, label='LSTM Train R²', color='blue', linestyle='-', marker='o')\n",
    "    axs[0].plot(range(epochs), r2_val_lstm, label='LSTM Test R²', color='blue', linestyle='--')\n",
    "    axs[0].plot(range(epochs), r2_train_gru, label='GRU Train R²', color='green', linestyle='-', marker='o')\n",
    "    axs[0].plot(range(epochs), r2_val_gru, label='GRU Test R²', color='green', linestyle='--')\n",
    "    axs[0].set_title('R² for LSTM and GRU Models (LB=8, Epochs=128, Units=64-32-24) - Site_130MV')\n",
    "    axs[0].set_xlabel('Epochs')\n",
    "    axs[0].set_ylabel('R²')\n",
    "    axs[0].legend(loc='lower right')\n",
    "    axs[0].grid(True)\n",
    "    \n",
    "    # Plot RMSE for train and validation sets with different red and orange shades\n",
    "    axs[1].plot(range(epochs), rmse_train_lstm, label='LSTM Train RMSE', color='darkorange', linestyle='-', marker='o')\n",
    "    axs[1].plot(range(epochs), rmse_val_lstm, label='LSTM Test RMSE', color='orangered', linestyle='--')\n",
    "    axs[1].plot(range(epochs), rmse_train_gru, label='GRU Train RMSE', color='darkred', linestyle='-', marker='o')\n",
    "    axs[1].plot(range(epochs), rmse_val_gru, label='GRU Test RMSE', color='firebrick', linestyle='--')\n",
    "    axs[1].set_title('RMSE for LSTM and GRU Models (LB=8, Epochs=128, Units=64-32-24) - Site_130MV')\n",
    "    axs[1].set_xlabel('Epochs')\n",
    "    axs[1].set_ylabel('RMSE')\n",
    "    axs[1].legend(loc='upper right')\n",
    "    axs[1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Load your training and validation data (assuming they are already normalized and available)\n",
    "# Train and validate LSTM model\n",
    "lookback_steps = 8\n",
    "epochs = 128\n",
    "batch_size = 64\n",
    "\n",
    "train_df = normalized_train_df_130 # This is the training DataFrame\n",
    "val_df = normalized_test_df_130 \n",
    "X_train, y_train = create_lookback_features(train_df, 'Power(MW)', lookback_steps)\n",
    "X_val, y_val = create_lookback_features(val_df, 'Power(MW)', lookback_steps)\n",
    "\n",
    "# Normalize features and target for LSTM/GRU\n",
    "min_train, max_train = np.min(y_train), np.max(y_train)\n",
    "X_train = (X_train - X_train.min(axis=0)) / (X_train.max(axis=0) - X_train.min(axis=0))\n",
    "X_val = (X_val - X_val.min(axis=0)) / (X_val.max(axis=0) - X_val.min(axis=0))\n",
    "\n",
    "# Train LSTM\n",
    "r2_train_lstm, rmse_train_lstm, r2_val_lstm, rmse_val_lstm, best_epoch_r2_lstm, best_epoch_rmse_lstm = build_train_lstm(\n",
    "    X_train, y_train, X_val, y_val, lookback_steps, epochs, batch_size, min_train, max_train\n",
    ")\n",
    "\n",
    "# Train GRU\n",
    "r2_train_gru, rmse_train_gru, r2_val_gru, rmse_val_gru, best_epoch_r2_gru, best_epoch_rmse_gru = build_train_gru(\n",
    "    X_train, y_train, X_val, y_val, lookback_steps, epochs, batch_size, min_train, max_train\n",
    ")\n",
    "\n",
    "# Plot metrics for both models\n",
    "plot_metrics(r2_train_lstm, rmse_train_lstm, r2_val_lstm, rmse_val_lstm,\n",
    "             r2_train_gru, rmse_train_gru, r2_val_gru, rmse_val_gru,\n",
    "             best_epoch_r2_lstm, best_epoch_rmse_lstm,\n",
    "             best_epoch_r2_gru, best_epoch_rmse_gru, epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0a5562",
   "metadata": {},
   "source": [
    "# Site_30MV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e5d9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Function to create look-back features for time series forecasting\n",
    "def create_lookback_features(df, target_column, lookback_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - lookback_steps):\n",
    "        X.append(df.iloc[i:i + lookback_steps].values)  # Slice features\n",
    "        y.append(df.iloc[i + lookback_steps][target_column])  # Target is the next value\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Function to reverse normalization\n",
    "def reverse_normalization(normalized_values, min_val, max_val):\n",
    "    return normalized_values * (max_val - min_val) + min_val\n",
    "\n",
    "# Function to calculate and print R squared and RMSE\n",
    "def calculate_metrics(y_true_train, y_pred_train, y_true_val, y_pred_val, title, best_epoch=None):\n",
    "    r2_train = r2_score(y_true_train, y_pred_train)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_true_train, y_pred_train))\n",
    "    \n",
    "    r2_val = r2_score(y_true_val, y_pred_val)\n",
    "    rmse_val = np.sqrt(mean_squared_error(y_true_val, y_pred_val))\n",
    "    \n",
    "    print(f\"{title} - Train R squared: {r2_train}, RMSE: {rmse_train}\")\n",
    "    print(f\"{title} - Validation R squared: {r2_val}, RMSE: {rmse_val}\")\n",
    "    \n",
    "    if best_epoch is not None:\n",
    "        print(f\"Best Epoch: {best_epoch}\")\n",
    "\n",
    "# Function to build and train LSTM model\n",
    "def build_train_lstm(X_train, y_train, X_val, y_val, lookback_steps, epochs, batch_size, min_train, max_train):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                   kernel_regularizer=regularizers.l2(0.001)))  # LSTM Layer with L2 regularization\n",
    "    model.add(BatchNormalization())  # BatchNormalization after LSTM\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(LSTM(units=32, return_sequences=False, kernel_regularizer=regularizers.l2(0.001)))  # Second LSTM Layer\n",
    "    model.add(BatchNormalization())  # BatchNormalization after second LSTM\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(Dense(24, activation='relu', kernel_regularizer=regularizers.l2(0.001)))  # Dense layer with ReLU\n",
    "    model.add(Dropout(0.2))  # Dropout layer after Dense layer\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))  # Output layer\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    r2_train_list, rmse_train_list, r2_val_list, rmse_val_list = [], [], [], []\n",
    "    best_epoch_r2, best_epoch_rmse = -np.inf, np.inf\n",
    "    best_epoch_r2_index, best_epoch_rmse_index = None, None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.fit(X_train, y_train, epochs=1, batch_size=batch_size, verbose=0,shuffle=False)\n",
    "        \n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_val = model.predict(X_val)\n",
    "        \n",
    "        # Reverse normalization for both predictions and true values\n",
    "        y_pred_train = reverse_normalization(y_pred_train, min_train, max_train)\n",
    "        y_pred_val = reverse_normalization(y_pred_val, min_train, max_train)\n",
    "        y_train_orig = reverse_normalization(y_train, min_train, max_train)\n",
    "        y_val_orig = reverse_normalization(y_val, min_train, max_train)\n",
    "        \n",
    "        # Calculate R² and RMSE\n",
    "        r2_train = r2_score(y_train_orig, y_pred_train)\n",
    "        rmse_train = np.sqrt(mean_squared_error(y_train_orig, y_pred_train))\n",
    "        \n",
    "        r2_val = r2_score(y_val_orig, y_pred_val)\n",
    "        rmse_val = np.sqrt(mean_squared_error(y_val_orig, y_pred_val))\n",
    "        \n",
    "        r2_train_list.append(r2_train)\n",
    "        rmse_train_list.append(rmse_train)\n",
    "        r2_val_list.append(r2_val)\n",
    "        rmse_val_list.append(rmse_val)\n",
    "        \n",
    "        # Track best epoch for R² (max) and RMSE (min)\n",
    "        if r2_val > best_epoch_r2:\n",
    "            best_epoch_r2 = r2_val\n",
    "            best_epoch_r2_index = epoch\n",
    "        \n",
    "        if rmse_val < best_epoch_rmse:\n",
    "            best_epoch_rmse = rmse_val\n",
    "            best_epoch_rmse_index = epoch\n",
    "    \n",
    "    return r2_train_list, rmse_train_list, r2_val_list, rmse_val_list, best_epoch_r2_index, best_epoch_rmse_index\n",
    "\n",
    "# Function to build and train GRU model\n",
    "def build_train_gru(X_train, y_train, X_val, y_val, lookback_steps, epochs, batch_size, min_train, max_train):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(units=64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                  kernel_regularizer=regularizers.l2(0.001)))  # GRU Layer with L2 regularization\n",
    "    model.add(BatchNormalization())  # BatchNormalization after GRU\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(GRU(units=32, return_sequences=False, kernel_regularizer=regularizers.l2(0.001)))  # Second GRU Layer\n",
    "    model.add(BatchNormalization())  # BatchNormalization after second GRU\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(Dense(24, activation='relu', kernel_regularizer=regularizers.l2(0.001)))  # Dense layer with ReLU\n",
    "    model.add(Dropout(0.2))  # Dropout layer after Dense layer\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))  # Output layer\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    r2_train_list, rmse_train_list, r2_val_list, rmse_val_list = [], [], [], []\n",
    "    best_epoch_r2, best_epoch_rmse = -np.inf, np.inf\n",
    "    best_epoch_r2_index, best_epoch_rmse_index = None, None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.fit(X_train, y_train, epochs=1, batch_size=batch_size, verbose=0,shuffle=False)\n",
    "        \n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_val = model.predict(X_val)\n",
    "        \n",
    "        # Reverse normalization for both predictions and true values\n",
    "        y_pred_train = reverse_normalization(y_pred_train, min_train, max_train)\n",
    "        y_pred_val = reverse_normalization(y_pred_val, min_train, max_train)\n",
    "        y_train_orig = reverse_normalization(y_train, min_train, max_train)\n",
    "        y_val_orig = reverse_normalization(y_val, min_train, max_train)\n",
    "        \n",
    "        # Calculate R² and RMSE\n",
    "        r2_train = r2_score(y_train_orig, y_pred_train)\n",
    "        rmse_train = np.sqrt(mean_squared_error(y_train_orig, y_pred_train))\n",
    "        \n",
    "        r2_val = r2_score(y_val_orig, y_pred_val)\n",
    "        rmse_val = np.sqrt(mean_squared_error(y_val_orig, y_pred_val))\n",
    "        \n",
    "        r2_train_list.append(r2_train)\n",
    "        rmse_train_list.append(rmse_train)\n",
    "        r2_val_list.append(r2_val)\n",
    "        rmse_val_list.append(rmse_val)\n",
    "        \n",
    "        # Track best epoch for R² (max) and RMSE (min)\n",
    "        if r2_val > best_epoch_r2:\n",
    "            best_epoch_r2 = r2_val\n",
    "            best_epoch_r2_index = epoch\n",
    "        \n",
    "        if rmse_val < best_epoch_rmse:\n",
    "            best_epoch_rmse = rmse_val\n",
    "            best_epoch_rmse_index = epoch\n",
    "    \n",
    "    return r2_train_list, rmse_train_list, r2_val_list, rmse_val_list, best_epoch_r2_index, best_epoch_rmse_index\n",
    "\n",
    "# Function to plot metrics for both LSTM and GRU models with different reds for RMSE\n",
    "def plot_metrics(r2_train_lstm, rmse_train_lstm, r2_val_lstm, rmse_val_lstm, \n",
    "                 r2_train_gru, rmse_train_gru, r2_val_gru, rmse_val_gru, \n",
    "                 best_epoch_r2_lstm, best_epoch_rmse_lstm,\n",
    "                 best_epoch_r2_gru, best_epoch_rmse_gru, epochs):\n",
    "    \n",
    "    fig, axs = plt.subplots(2, 1, figsize=(10, 12))\n",
    "    \n",
    "    # Plot R² for train and validation sets (keeping previous colors)\n",
    "    axs[0].plot(range(epochs), r2_train_lstm, label='LSTM Train R²', color='blue', linestyle='-', marker='o')\n",
    "    axs[0].plot(range(epochs), r2_val_lstm, label='LSTM Test R²', color='blue', linestyle='--')\n",
    "    axs[0].plot(range(epochs), r2_train_gru, label='GRU Train R²', color='green', linestyle='-', marker='o')\n",
    "    axs[0].plot(range(epochs), r2_val_gru, label='GRU Test R²', color='green', linestyle='--')\n",
    "    axs[0].set_title('R² for LSTM and GRU Models (LB=8, Epochs=128, Units=64-32-24) - Site_30MV')\n",
    "    axs[0].set_xlabel('Epochs')\n",
    "    axs[0].set_ylabel('R²')\n",
    "    axs[0].legend(loc='lower right')\n",
    "    axs[0].grid(True)\n",
    "    \n",
    "    # Plot RMSE for train and validation sets with different red and orange shades\n",
    "    axs[1].plot(range(epochs), rmse_train_lstm, label='LSTM Train RMSE', color='darkorange', linestyle='-', marker='o')\n",
    "    axs[1].plot(range(epochs), rmse_val_lstm, label='LSTM Test RMSE', color='orangered', linestyle='--')\n",
    "    axs[1].plot(range(epochs), rmse_train_gru, label='GRU Train RMSE', color='darkred', linestyle='-', marker='o')\n",
    "    axs[1].plot(range(epochs), rmse_val_gru, label='GRU Test RMSE', color='firebrick', linestyle='--')\n",
    "    axs[1].set_title('RMSE for LSTM and GRU Models (LB=8, Epochs=128, Units=64-32-24) - Site_30MV')\n",
    "    axs[1].set_xlabel('Epochs')\n",
    "    axs[1].set_ylabel('RMSE')\n",
    "    axs[1].legend(loc='upper right')\n",
    "    axs[1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Load your training and validation data (assuming they are already normalized and available)\n",
    "# Train and validate LSTM model\n",
    "lookback_steps = 8\n",
    "epochs = 128\n",
    "batch_size = 64\n",
    "\n",
    "train_df = normalized_train_df_30 # This is the training DataFrame\n",
    "val_df = normalized_test_df_30 \n",
    "X_train, y_train = create_lookback_features(train_df, 'Power(MW)', lookback_steps)\n",
    "X_val, y_val = create_lookback_features(val_df, 'Power(MW)', lookback_steps)\n",
    "\n",
    "# Normalize features and target for LSTM/GRU\n",
    "min_train, max_train = np.min(y_train), np.max(y_train)\n",
    "X_train = (X_train - X_train.min(axis=0)) / (X_train.max(axis=0) - X_train.min(axis=0))\n",
    "X_val = (X_val - X_val.min(axis=0)) / (X_val.max(axis=0) - X_val.min(axis=0))\n",
    "\n",
    "# Train LSTM\n",
    "r2_train_lstm, rmse_train_lstm, r2_val_lstm, rmse_val_lstm, best_epoch_r2_lstm, best_epoch_rmse_lstm = build_train_lstm(\n",
    "    X_train, y_train, X_val, y_val, lookback_steps, epochs, batch_size, min_train, max_train\n",
    ")\n",
    "\n",
    "# Train GRU\n",
    "r2_train_gru, rmse_train_gru, r2_val_gru, rmse_val_gru, best_epoch_r2_gru, best_epoch_rmse_gru = build_train_gru(\n",
    "    X_train, y_train, X_val, y_val, lookback_steps, epochs, batch_size, min_train, max_train\n",
    ")\n",
    "\n",
    "# Plot metrics for both models\n",
    "plot_metrics(r2_train_lstm, rmse_train_lstm, r2_val_lstm, rmse_val_lstm,\n",
    "             r2_train_gru, rmse_train_gru, r2_val_gru, rmse_val_gru,\n",
    "             best_epoch_r2_lstm, best_epoch_rmse_lstm,\n",
    "             best_epoch_r2_gru, best_epoch_rmse_gru, epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7a4916",
   "metadata": {},
   "source": [
    "# Site_35MV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e83551b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Function to create look-back features for time series forecasting\n",
    "def create_lookback_features(df, target_column, lookback_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - lookback_steps):\n",
    "        X.append(df.iloc[i:i + lookback_steps].values)  # Slice features\n",
    "        y.append(df.iloc[i + lookback_steps][target_column])  # Target is the next value\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Function to reverse normalization\n",
    "def reverse_normalization(normalized_values, min_val, max_val):\n",
    "    return normalized_values * (max_val - min_val) + min_val\n",
    "\n",
    "# Function to calculate and print R squared and RMSE\n",
    "def calculate_metrics(y_true_train, y_pred_train, y_true_val, y_pred_val, title, best_epoch=None):\n",
    "    r2_train = r2_score(y_true_train, y_pred_train)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_true_train, y_pred_train))\n",
    "    \n",
    "    r2_val = r2_score(y_true_val, y_pred_val)\n",
    "    rmse_val = np.sqrt(mean_squared_error(y_true_val, y_pred_val))\n",
    "    \n",
    "    print(f\"{title} - Train R squared: {r2_train}, RMSE: {rmse_train}\")\n",
    "    print(f\"{title} - Validation R squared: {r2_val}, RMSE: {rmse_val}\")\n",
    "    \n",
    "    if best_epoch is not None:\n",
    "        print(f\"Best Epoch: {best_epoch}\")\n",
    "\n",
    "# Function to build and train LSTM model\n",
    "def build_train_lstm(X_train, y_train, X_val, y_val, lookback_steps, epochs, batch_size, min_train, max_train):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                   kernel_regularizer=regularizers.l2(0.001)))  # LSTM Layer with L2 regularization\n",
    "    model.add(BatchNormalization())  # BatchNormalization after LSTM\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(LSTM(units=32, return_sequences=False, kernel_regularizer=regularizers.l2(0.001)))  # Second LSTM Layer\n",
    "    model.add(BatchNormalization())  # BatchNormalization after second LSTM\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(Dense(24, activation='relu', kernel_regularizer=regularizers.l2(0.001)))  # Dense layer with ReLU\n",
    "    model.add(Dropout(0.2))  # Dropout layer after Dense layer\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))  # Output layer\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    r2_train_list, rmse_train_list, r2_val_list, rmse_val_list = [], [], [], []\n",
    "    best_epoch_r2, best_epoch_rmse = -np.inf, np.inf\n",
    "    best_epoch_r2_index, best_epoch_rmse_index = None, None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.fit(X_train, y_train, epochs=1, batch_size=batch_size, verbose=0,shuffle=False)\n",
    "        \n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_val = model.predict(X_val)\n",
    "        \n",
    "        # Reverse normalization for both predictions and true values\n",
    "        y_pred_train = reverse_normalization(y_pred_train, min_train, max_train)\n",
    "        y_pred_val = reverse_normalization(y_pred_val, min_train, max_train)\n",
    "        y_train_orig = reverse_normalization(y_train, min_train, max_train)\n",
    "        y_val_orig = reverse_normalization(y_val, min_train, max_train)\n",
    "        \n",
    "        # Calculate R² and RMSE\n",
    "        r2_train = r2_score(y_train_orig, y_pred_train)\n",
    "        rmse_train = np.sqrt(mean_squared_error(y_train_orig, y_pred_train))\n",
    "        \n",
    "        r2_val = r2_score(y_val_orig, y_pred_val)\n",
    "        rmse_val = np.sqrt(mean_squared_error(y_val_orig, y_pred_val))\n",
    "        \n",
    "        r2_train_list.append(r2_train)\n",
    "        rmse_train_list.append(rmse_train)\n",
    "        r2_val_list.append(r2_val)\n",
    "        rmse_val_list.append(rmse_val)\n",
    "        \n",
    "        # Track best epoch for R² (max) and RMSE (min)\n",
    "        if r2_val > best_epoch_r2:\n",
    "            best_epoch_r2 = r2_val\n",
    "            best_epoch_r2_index = epoch\n",
    "        \n",
    "        if rmse_val < best_epoch_rmse:\n",
    "            best_epoch_rmse = rmse_val\n",
    "            best_epoch_rmse_index = epoch\n",
    "    \n",
    "    return r2_train_list, rmse_train_list, r2_val_list, rmse_val_list, best_epoch_r2_index, best_epoch_rmse_index\n",
    "\n",
    "# Function to build and train GRU model\n",
    "def build_train_gru(X_train, y_train, X_val, y_val, lookback_steps, epochs, batch_size, min_train, max_train):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(units=64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                  kernel_regularizer=regularizers.l2(0.001)))  # GRU Layer with L2 regularization\n",
    "    model.add(BatchNormalization())  # BatchNormalization after GRU\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(GRU(units=32, return_sequences=False, kernel_regularizer=regularizers.l2(0.001)))  # Second GRU Layer\n",
    "    model.add(BatchNormalization())  # BatchNormalization after second GRU\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(Dense(24, activation='relu', kernel_regularizer=regularizers.l2(0.001)))  # Dense layer with ReLU\n",
    "    model.add(Dropout(0.2))  # Dropout layer after Dense layer\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))  # Output layer\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    r2_train_list, rmse_train_list, r2_val_list, rmse_val_list = [], [], [], []\n",
    "    best_epoch_r2, best_epoch_rmse = -np.inf, np.inf\n",
    "    best_epoch_r2_index, best_epoch_rmse_index = None, None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.fit(X_train, y_train, epochs=1, batch_size=batch_size, verbose=0,shuffle=False)\n",
    "        \n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_val = model.predict(X_val)\n",
    "        \n",
    "        # Reverse normalization for both predictions and true values\n",
    "        y_pred_train = reverse_normalization(y_pred_train, min_train, max_train)\n",
    "        y_pred_val = reverse_normalization(y_pred_val, min_train, max_train)\n",
    "        y_train_orig = reverse_normalization(y_train, min_train, max_train)\n",
    "        y_val_orig = reverse_normalization(y_val, min_train, max_train)\n",
    "        \n",
    "        # Calculate R² and RMSE\n",
    "        r2_train = r2_score(y_train_orig, y_pred_train)\n",
    "        rmse_train = np.sqrt(mean_squared_error(y_train_orig, y_pred_train))\n",
    "        \n",
    "        r2_val = r2_score(y_val_orig, y_pred_val)\n",
    "        rmse_val = np.sqrt(mean_squared_error(y_val_orig, y_pred_val))\n",
    "        \n",
    "        r2_train_list.append(r2_train)\n",
    "        rmse_train_list.append(rmse_train)\n",
    "        r2_val_list.append(r2_val)\n",
    "        rmse_val_list.append(rmse_val)\n",
    "        \n",
    "        # Track best epoch for R² (max) and RMSE (min)\n",
    "        if r2_val > best_epoch_r2:\n",
    "            best_epoch_r2 = r2_val\n",
    "            best_epoch_r2_index = epoch\n",
    "        \n",
    "        if rmse_val < best_epoch_rmse:\n",
    "            best_epoch_rmse = rmse_val\n",
    "            best_epoch_rmse_index = epoch\n",
    "    \n",
    "    return r2_train_list, rmse_train_list, r2_val_list, rmse_val_list, best_epoch_r2_index, best_epoch_rmse_index\n",
    "\n",
    "# Function to plot metrics for both LSTM and GRU models with different reds for RMSE\n",
    "def plot_metrics(r2_train_lstm, rmse_train_lstm, r2_val_lstm, rmse_val_lstm, \n",
    "                 r2_train_gru, rmse_train_gru, r2_val_gru, rmse_val_gru, \n",
    "                 best_epoch_r2_lstm, best_epoch_rmse_lstm,\n",
    "                 best_epoch_r2_gru, best_epoch_rmse_gru, epochs):\n",
    "    \n",
    "    fig, axs = plt.subplots(2, 1, figsize=(10, 12))\n",
    "    \n",
    "    # Plot R² for train and validation sets (keeping previous colors)\n",
    "    axs[0].plot(range(epochs), r2_train_lstm, label='LSTM Train R²', color='blue', linestyle='-', marker='o')\n",
    "    axs[0].plot(range(epochs), r2_val_lstm, label='LSTM Test R²', color='blue', linestyle='--')\n",
    "    axs[0].plot(range(epochs), r2_train_gru, label='GRU Train R²', color='green', linestyle='-', marker='o')\n",
    "    axs[0].plot(range(epochs), r2_val_gru, label='GRU Test R²', color='green', linestyle='--')\n",
    "    axs[0].set_title('R² for LSTM and GRU Models (LB=8, Epochs=128, Units=64-32-24) - Site_35MV')\n",
    "    axs[0].set_xlabel('Epochs')\n",
    "    axs[0].set_ylabel('R²')\n",
    "    axs[0].legend(loc='lower right')\n",
    "    axs[0].grid(True)\n",
    "    \n",
    "    # Plot RMSE for train and validation sets with different red and orange shades\n",
    "    axs[1].plot(range(epochs), rmse_train_lstm, label='LSTM Train RMSE', color='darkorange', linestyle='-', marker='o')\n",
    "    axs[1].plot(range(epochs), rmse_val_lstm, label='LSTM Test RMSE', color='orangered', linestyle='--')\n",
    "    axs[1].plot(range(epochs), rmse_train_gru, label='GRU Train RMSE', color='darkred', linestyle='-', marker='o')\n",
    "    axs[1].plot(range(epochs), rmse_val_gru, label='GRU Test RMSE', color='firebrick', linestyle='--')\n",
    "    axs[1].set_title('RMSE for LSTM and GRU Models (LB=8, Epochs=128, Units=64-32-24) - Site_35MV')\n",
    "    axs[1].set_xlabel('Epochs')\n",
    "    axs[1].set_ylabel('RMSE')\n",
    "    axs[1].legend(loc='upper right')\n",
    "    axs[1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Load your training and validation data (assuming they are already normalized and available)\n",
    "# Train and validate LSTM model\n",
    "lookback_steps = 8\n",
    "epochs = 128\n",
    "batch_size = 64\n",
    "\n",
    "train_df = normalized_train_df_35 # This is the training DataFrame\n",
    "val_df = normalized_test_df_35 \n",
    "X_train, y_train = create_lookback_features(train_df, 'Power(MW)', lookback_steps)\n",
    "X_val, y_val = create_lookback_features(val_df, 'Power(MW)', lookback_steps)\n",
    "\n",
    "# Normalize features and target for LSTM/GRU\n",
    "min_train, max_train = np.min(y_train), np.max(y_train)\n",
    "X_train = (X_train - X_train.min(axis=0)) / (X_train.max(axis=0) - X_train.min(axis=0))\n",
    "X_val = (X_val - X_val.min(axis=0)) / (X_val.max(axis=0) - X_val.min(axis=0))\n",
    "\n",
    "# Train LSTM\n",
    "r2_train_lstm, rmse_train_lstm, r2_val_lstm, rmse_val_lstm, best_epoch_r2_lstm, best_epoch_rmse_lstm = build_train_lstm(\n",
    "    X_train, y_train, X_val, y_val, lookback_steps, epochs, batch_size, min_train, max_train\n",
    ")\n",
    "\n",
    "# Train GRU\n",
    "r2_train_gru, rmse_train_gru, r2_val_gru, rmse_val_gru, best_epoch_r2_gru, best_epoch_rmse_gru = build_train_gru(\n",
    "    X_train, y_train, X_val, y_val, lookback_steps, epochs, batch_size, min_train, max_train\n",
    ")\n",
    "\n",
    "# Plot metrics for both models\n",
    "plot_metrics(r2_train_lstm, rmse_train_lstm, r2_val_lstm, rmse_val_lstm,\n",
    "             r2_train_gru, rmse_train_gru, r2_val_gru, rmse_val_gru,\n",
    "             best_epoch_r2_lstm, best_epoch_rmse_lstm,\n",
    "             best_epoch_r2_gru, best_epoch_rmse_gru, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a97aa1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
