{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ba62ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating the scikit-learn package to the latest version\n",
    "!pip install scikit-learn --upgrade\n",
    "# Importing Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596c464a",
   "metadata": {},
   "source": [
    "# Import Data from all Sites with Different Nominal Power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f64ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_50 = pd.read_excel(r'C:\\Users\\Farin\\Documents\\Tilburg_University\\00_Thesis_start\\DB\\s1_50MW.xlsx').drop(index=0)\n",
    "df_130 = pd.read_excel(r'C:\\Users\\Farin\\Documents\\Tilburg_University\\00_Thesis_start\\DB\\s2_130MW.xlsx').drop(index=0)\n",
    "df_35 = pd.read_excel(r'C:\\Users\\Farin\\Documents\\Tilburg_University\\00_Thesis_start\\DB\\s6_35MW.xlsx').drop(index=0)\n",
    "df_30 = pd.read_excel(r'C:\\Users\\Farin\\Documents\\Tilburg_University\\00_Thesis_start\\DB\\s8_30MW.xlsx').drop(index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c42ec1",
   "metadata": {},
   "source": [
    "# Explanatory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a0e2ed",
   "metadata": {},
   "source": [
    "Rename Columns,\n",
    "Replace <NULL> with NaN,\n",
    "Change Data types,\n",
    "Remove records with missing values for all features,\n",
    "Remove records with values such as -99 and --.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb91e581",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in [df_50, df_130, df_35, df_30]:\n",
    "        dataset.columns=['time','TSI','DNI','GHI','Air_T','Air_P','Air_H','Power(MW)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad3032c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_50.replace('<NULL>', 'NaN', inplace=True)\n",
    "df_130.replace('<NULL>', 'NaN', inplace=True)\n",
    "df_35.replace('<NULL>', 'NaN', inplace=True)\n",
    "df_30.replace('<NULL>', 'NaN', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87c6b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in [df_50, df_130, df_35, df_30]:\n",
    "    for tile in ['TSI','DNI','GHI','Air_T','Air_P','Air_H','Power(MW)']:\n",
    "        dataset[tile]= dataset[tile].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ec25a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_50['time'] = pd.to_datetime(df_50['time'], errors='coerce')\n",
    "df_130['time'] = pd.to_datetime(df_130['time'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0082f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_35.dropna(how='any', inplace=True)# Drop rows where 'TSI' is -99 using the index\n",
    "df_50 = df_50.drop(df_50[df_50['TSI'] == -99].index)\n",
    "df_130 = df_130.drop(df_130[df_130['TSI'] == -99].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112c2ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where 'TSI' is -99 using the index\n",
    "df_50 = df_50.drop(df_50[df_50['TSI'] == -99].index)\n",
    "df_130 = df_130.drop(df_130[df_130['TSI'] == -99].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403c6b6f",
   "metadata": {},
   "source": [
    "# Scatter Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508a8377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# List of datasets\n",
    "datasets = [df_50, df_130, df_30, df_30]  # Assuming you want to display all four datasets\n",
    "dataset_names = ['df_50', 'df_130', 'df_30', 'df_30']\n",
    "\n",
    "# Set up the seaborn style for aesthetics\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Define the number of rows and columns for the subplot grid\n",
    "num_rows = 7  # Number of features (rows)\n",
    "num_columns = 4  # Number of datasets (columns)\n",
    "\n",
    "# Create a large figure with subplots\n",
    "fig, axes = plt.subplots(nrows=num_rows, ncols=num_columns, figsize=(20, 30))\n",
    "\n",
    "# Define colors for each dataset (red, blue, green, yellow)\n",
    "colors = ['red', 'blue', 'green', 'pink']  # Explicit colors for datasets\n",
    "\n",
    "# Loop through the datasets (columns)\n",
    "for col_idx, (dataset, dataset_name) in enumerate(zip(datasets, dataset_names)):\n",
    "    # Get the list of feature columns (assuming all columns except 'Power(MW)' are features)\n",
    "    features = dataset.columns[dataset.columns != 'Power(MW)']\n",
    "    \n",
    "    # Loop through each feature (row)\n",
    "    for row_idx, feature in enumerate(features):\n",
    "        ax = axes[row_idx, col_idx]  # Get the correct axis in the grid\n",
    "        \n",
    "        # Plot the scatter plot for the feature vs Power(MW) for the current dataset\n",
    "        sns.scatterplot(x=dataset[feature], y=dataset['Power(MW)'], \n",
    "                        ax=ax, color=colors[col_idx], s=80, label=dataset_name)\n",
    "        \n",
    "        # Set title and axis labels\n",
    "        ax.set_title(f'{feature} vs Power(MW)', fontsize=10)\n",
    "        ax.set_xlabel(feature, fontsize=8)\n",
    "        ax.set_ylabel('Power(MW)', fontsize=8)\n",
    "        \n",
    "        # Adjust font size for axis ticks\n",
    "        ax.tick_params(axis='both', which='major', labelsize=8)\n",
    "\n",
    "# Adjust layout to avoid overlap and make it readable\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot as a PNG file\n",
    "plt.savefig('scatter_plots_by_dataset2.png', dpi=300)  # Save with 300 dpi for high quality\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618465c6",
   "metadata": {},
   "source": [
    "# Box Plot for Site_50MW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f40175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the features to plot\n",
    "feature_names = ['TSI', 'DNI', 'GHI', 'Air_T', 'Air_P', 'Air_H', 'Power(MW)']\n",
    "\n",
    "# Enhanced Box plots\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Create the boxplot with additional styling\n",
    "boxprops = dict(linestyle='-', linewidth=2, color='navy')\n",
    "medianprops = dict(linestyle='-', linewidth=2, color='firebrick')\n",
    "whiskerprops = dict(linestyle='--', linewidth=2, color='black')\n",
    "capprops = dict(linestyle='-', linewidth=2, color='grey')\n",
    "\n",
    "# Create the boxplot\n",
    "boxplot = plt.boxplot([df_50[feature] for feature in feature_names], \n",
    "                      labels=feature_names, \n",
    "                      notch=True, \n",
    "                      patch_artist=True, \n",
    "                      boxprops=boxprops, \n",
    "                      medianprops=medianprops, \n",
    "                      whiskerprops=whiskerprops, \n",
    "                      capprops=capprops)\n",
    "\n",
    "# Adding grid, title and labels\n",
    "plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7, axis='y')\n",
    "plt.title('Feature Distribution - Boxplot', fontsize=18, fontweight='bold')\n",
    "plt.xlabel('Features', fontsize=16)\n",
    "plt.ylabel('Values', fontsize=16)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "# Remove spines\n",
    "for spine in plt.gca().spines.values():\n",
    "    spine.set_visible(False)\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('Feature_boxplot_50.pdf', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837d041c",
   "metadata": {},
   "source": [
    "# Box Plot for Site_130MW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aab6616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the features to plot\n",
    "feature_names = ['TSI', 'DNI', 'GHI', 'Air_T', 'Air_P', 'Air_H', 'Power(MW)']\n",
    "\n",
    "# Enhanced Box plots\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Create the boxplot with additional styling\n",
    "boxprops = dict(linestyle='-', linewidth=2, color='navy')\n",
    "medianprops = dict(linestyle='-', linewidth=2, color='firebrick')\n",
    "whiskerprops = dict(linestyle='--', linewidth=2, color='black')\n",
    "capprops = dict(linestyle='-', linewidth=2, color='grey')\n",
    "\n",
    "# Create the boxplot\n",
    "boxplot = plt.boxplot([df_130[feature] for feature in feature_names], \n",
    "                      labels=feature_names, \n",
    "                      notch=True, \n",
    "                      patch_artist=True, \n",
    "                      boxprops=boxprops, \n",
    "                      medianprops=medianprops, \n",
    "                      whiskerprops=whiskerprops, \n",
    "                      capprops=capprops)\n",
    "\n",
    "# Adding grid, title and labels\n",
    "plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7, axis='y')\n",
    "plt.title('Feature Distribution - Boxplot', fontsize=18, fontweight='bold')\n",
    "plt.xlabel('Features', fontsize=16)\n",
    "plt.ylabel('Values', fontsize=16)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "# Remove spines\n",
    "for spine in plt.gca().spines.values():\n",
    "    spine.set_visible(False)\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('Feature_boxplot_130.pdf', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ed4c06",
   "metadata": {},
   "source": [
    " # Box Plot for Site_30MW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347b75ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the features to plot\n",
    "feature_names = ['TSI', 'DNI', 'GHI', 'Air_T', 'Air_P', 'Air_H', 'Power(MW)']\n",
    "\n",
    "# Enhanced Box plots\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Create the boxplot with additional styling\n",
    "boxprops = dict(linestyle='-', linewidth=2, color='navy')\n",
    "medianprops = dict(linestyle='-', linewidth=2, color='firebrick')\n",
    "whiskerprops = dict(linestyle='--', linewidth=2, color='black')\n",
    "capprops = dict(linestyle='-', linewidth=2, color='grey')\n",
    "\n",
    "# Create the boxplot\n",
    "boxplot = plt.boxplot([df_30[feature] for feature in feature_names], \n",
    "                      labels=feature_names, \n",
    "                      notch=True, \n",
    "                      patch_artist=True, \n",
    "                      boxprops=boxprops, \n",
    "                      medianprops=medianprops, \n",
    "                      whiskerprops=whiskerprops, \n",
    "                      capprops=capprops)\n",
    "\n",
    "# Adding grid, title and labels\n",
    "plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7, axis='y')\n",
    "plt.title('Feature Distribution - Boxplot', fontsize=18, fontweight='bold')\n",
    "plt.xlabel('Features', fontsize=16)\n",
    "plt.ylabel('Values', fontsize=16)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "# Remove spines\n",
    "for spine in plt.gca().spines.values():\n",
    "    spine.set_visible(False)\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('Feature_boxplot_30.pdf', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c279934b",
   "metadata": {},
   "source": [
    " # Box Plot for Site_35MW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae30b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the features to plot\n",
    "feature_names = ['TSI', 'DNI', 'GHI', 'Air_T', 'Air_P', 'Air_H', 'Power(MW)']\n",
    "\n",
    "# Enhanced Box plots\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Create the boxplot with additional styling\n",
    "boxprops = dict(linestyle='-', linewidth=2, color='navy')\n",
    "medianprops = dict(linestyle='-', linewidth=2, color='firebrick')\n",
    "whiskerprops = dict(linestyle='--', linewidth=2, color='black')\n",
    "capprops = dict(linestyle='-', linewidth=2, color='grey')\n",
    "\n",
    "# Create the boxplot\n",
    "boxplot = plt.boxplot([df_35[feature] for feature in feature_names], \n",
    "                      labels=feature_names, \n",
    "                      notch=True, \n",
    "                      patch_artist=True, \n",
    "                      boxprops=boxprops, \n",
    "                      medianprops=medianprops, \n",
    "                      whiskerprops=whiskerprops, \n",
    "                      capprops=capprops)\n",
    "\n",
    "# Adding grid, title and labels\n",
    "plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7, axis='y')\n",
    "plt.title('Feature Distribution - Boxplot', fontsize=18, fontweight='bold')\n",
    "plt.xlabel('Features', fontsize=16)\n",
    "plt.ylabel('Values', fontsize=16)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "# Remove spines\n",
    "for spine in plt.gca().spines.values():\n",
    "    spine.set_visible(False)\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('Feature_boxplot_35.pdf', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae412bd3",
   "metadata": {},
   "source": [
    "# Correlation_heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aed7fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# List of DataFrames and their respective titles\n",
    "dataframes = [df_50, df_130, df_30, df_35]\n",
    "titles = ['df_50 Correlation Matrix', 'df_130 Correlation Matrix', 'df_30 Correlation Matrix', 'df_35 Correlation Matrix']\n",
    "\n",
    "# List of features to consider for correlation\n",
    "features = ['TSI', 'DNI', 'GHI', 'Air_T', 'Air_P', 'Air_H', 'Power(MW)']\n",
    "\n",
    "# Create subplots for each DataFrame\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axs = axs.flatten()\n",
    "\n",
    "for i, (df, title) in enumerate(zip(dataframes, titles)):\n",
    "    # Calculate the correlation matrix\n",
    "    correlation_matrix = df[features].corr()\n",
    "\n",
    "    # Plot the correlation matrix\n",
    "    cax = axs[i].imshow(correlation_matrix, cmap='coolwarm', vmax=1, vmin=-1)\n",
    "    axs[i].set_title(title)\n",
    "    axs[i].set_xticks(range(len(correlation_matrix.columns)))\n",
    "    axs[i].set_xticklabels(correlation_matrix.columns, rotation=90)\n",
    "    axs[i].set_yticks(range(len(correlation_matrix.columns)))\n",
    "    axs[i].set_yticklabels(correlation_matrix.columns)\n",
    "\n",
    "    # Add color bar\n",
    "    plt.colorbar(cax, ax=axs[i], fraction=0.046, pad=0.04)\n",
    "\n",
    "    # Add correlation values in the matrix\n",
    "    for j in range(len(correlation_matrix.columns)):\n",
    "        for k in range(len(correlation_matrix.columns)):\n",
    "            text = axs[i].text(k, j, np.around(correlation_matrix.iloc[j, k], decimals=2),\n",
    "                               ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.savefig('Correlation_heatmap_before.pdf', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9665d9",
   "metadata": {},
   "source": [
    "# Resamples Data Frames(with Navg=2) for all Sites with Different Nominal Power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c8cd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming your data frames are named df_50, df_130, df_35, df_30\n",
    "dataframes = [df_50, df_130, df_35, df_30]\n",
    "\n",
    "# Function to resample and average the data frames\n",
    "def resample_and_average(df):\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    df.set_index('time', inplace=True)\n",
    "    resampled_df = df.resample('30T').mean()\n",
    "    return resampled_df\n",
    "\n",
    "# Apply the function to each data frame\n",
    "resampled_dfs = [resample_and_average(df) for df in dataframes]\n",
    "\n",
    "# If you want to access each resampled data frame individually\n",
    "resampled_df_50, resampled_df_130, resampled_df_35, resampled_df_30 = resampled_dfs\n",
    "\n",
    "# Example: Print the resampled df_50\n",
    "print(resampled_df_50)\n",
    "\n",
    "resampled_df_50 = resampled_df_50.fillna(method='ffill')\n",
    "resampled_df_130 = resampled_df_50.fillna(method='ffill')\n",
    "resampled_df_35 = resampled_df_50.fillna(method='ffill')\n",
    "resampled_df_30 = resampled_df_50.fillna(method='ffill')\n",
    "\n",
    "\n",
    "# Assuming the dataframes are already defined\n",
    "resampled_df_50.to_csv('resampled_df_50_time.csv', index=True)\n",
    "resampled_df_130.to_csv('resampled_df_130_time.csv', index=True)\n",
    "resampled_df_35.to_csv('resampled_df_35_time.csv', index=True)\n",
    "resampled_df_30.to_csv('resampled_df_30_time.csv', index=True)\n",
    "\n",
    "print(\"Dataframes have been successfully exported to CSV files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefd0bb3",
   "metadata": {},
   "source": [
    "# Spilit Each Resample data for all Sites to Train, Validation and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0e8e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to split the data into train, validation, and test sets\n",
    "def split_data(df, train_size=0.7, val_size=0.1, test_size=0.2):\n",
    "    # Ensure the sizes sum to 1\n",
    "    assert train_size + val_size + test_size == 1, \"Train, validation, and test sizes must sum to 1\"\n",
    "    \n",
    "    # Calculate the number of samples for each set\n",
    "    n = len(df)\n",
    "    train_end = int(train_size * n)\n",
    "    val_end = train_end + int(val_size * n)\n",
    "    \n",
    "    # Split the data\n",
    "    train = df[:train_end]\n",
    "    val = df[train_end:val_end]\n",
    "    test = df[val_end:]\n",
    "    \n",
    "    return train, val, test\n",
    "\n",
    "# Assuming your resampled data frames are named resampled_df_50, resampled_df_130, resampled_df_35, resampled_df_30\n",
    "resampled_dataframes = [resampled_df_50, resampled_df_130, resampled_df_35, resampled_df_30]\n",
    "\n",
    "# Apply the function to each resampled data frame\n",
    "split_resampled_dfs = [split_data(df) for df in resampled_dataframes]\n",
    "\n",
    "# Corrected unpacking of the split data frames\n",
    "(train_resampled_df_50, val_resampled_df_50, test_resampled_df_50), \\\n",
    "(train_resampled_df_130, val_resampled_df_130, test_resampled_df_130), \\\n",
    "(train_resampled_df_35, val_resampled_df_35, test_resampled_df_35), \\\n",
    "(train_resampled_df_30, val_resampled_df_30, test_resampled_df_30) = split_resampled_dfs\n",
    "\n",
    "# Example: Print the shapes of the split resampled_df_50\n",
    "print(\"Train shape:\", train_resampled_df_50.shape)\n",
    "print(\"Validation shape:\", val_resampled_df_50.shape)\n",
    "print(\"Test shape:\", test_resampled_df_50.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40135799",
   "metadata": {},
   "source": [
    "# Scaling Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ba6f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing mean and normalizing deviation of the training data for df_50\n",
    "max_train_50 = train_resampled_df_50.max(axis=0)\n",
    "min_train_50 = train_resampled_df_50.min(axis=0)\n",
    "\n",
    "# Normalizing training, validation, and test data for df_50\n",
    "normalized_train_df_50 = (train_resampled_df_50 - min_train_50) / (max_train_50 - min_train_50)\n",
    "normalized_val_df_50 = (val_resampled_df_50 - min_train_50) / (max_train_50 - min_train_50)\n",
    "normalized_test_df_50 = (test_resampled_df_50 - min_train_50) / (max_train_50 - min_train_50)\n",
    "\n",
    "# Computing mean and normalizing deviation of the training data for df_130\n",
    "max_train_130 = train_resampled_df_130.max(axis=0)\n",
    "min_train_130 = train_resampled_df_130.min(axis=0)\n",
    "\n",
    "# Normalizing training, validation, and test data for df_130\n",
    "normalized_train_df_130 = (train_resampled_df_130 - min_train_130) / (max_train_130 - min_train_130)\n",
    "normalized_val_df_130 = (val_resampled_df_130 - min_train_130) / (max_train_130 - min_train_130)\n",
    "normalized_test_df_130 = (test_resampled_df_130 - min_train_130) / (max_train_130 - min_train_130)\n",
    "\n",
    "# Computing mean and normalizing deviation of the training data for df_30\n",
    "max_train_30 = train_resampled_df_30.max(axis=0)\n",
    "min_train_30 = train_resampled_df_30.min(axis=0)\n",
    "\n",
    "# Normalizing training, validation, and test data for df_30\n",
    "normalized_train_df_30 = (train_resampled_df_30 - min_train_30) / (max_train_30 - min_train_30)\n",
    "normalized_val_df_30 = (val_resampled_df_30 - min_train_30) / (max_train_30 - min_train_30)\n",
    "normalized_test_df_30 = (test_resampled_df_30 - min_train_30) / (max_train_30 - min_train_30)\n",
    "\n",
    "# Computing mean and normalizing deviation of the training data for df_35\n",
    "max_train_35 = train_resampled_df_35.max(axis=0)\n",
    "min_train_35 = train_resampled_df_35.min(axis=0)\n",
    "\n",
    "# Normalizing training, validation, and test data for df_35\n",
    "normalized_train_df_35 = (train_resampled_df_35 - min_train_35) / (max_train_35 - min_train_35)\n",
    "normalized_val_df_35 = (val_resampled_df_35 - min_train_35) / (max_train_35 - min_train_35)\n",
    "normalized_test_df_35 = (test_resampled_df_35 - min_train_35) / (max_train_35 - min_train_35)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2c6e14",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de6642f",
   "metadata": {},
   "source": [
    "# Grid Search for Site_50MW on Train and Validatian Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb59fabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Function to create look-back features for time series forecasting\n",
    "def create_lookback_features(df, target_column, lookback_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - lookback_steps):\n",
    "        X.append(df.iloc[i:i + lookback_steps].values)\n",
    "        y.append(df.iloc[i + lookback_steps][target_column])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Function to calculate and print R squared and RMSE\n",
    "def calculate_metrics(y_true_train, y_pred_train, y_true_val, y_pred_val, title):\n",
    "    r2_train = r2_score(y_true_train, y_pred_train)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_true_train, y_pred_train))\n",
    "    \n",
    "    r2_val = r2_score(y_true_val, y_pred_val)\n",
    "    rmse_val = np.sqrt(mean_squared_error(y_true_val, y_pred_val))\n",
    "    \n",
    "    print(f\"{title} - Train R squared: {r2_train}, RMSE: {rmse_train}\")\n",
    "    print(f\"{title} - Validation R squared: {r2_val}, RMSE: {rmse_val}\")\n",
    "\n",
    "# Function to plot the results of Grid Search\n",
    "def plot_search_results(grid, dataset_name):\n",
    "    \"\"\"\n",
    "    Params: \n",
    "        grid: A trained GridSearchCV object.\n",
    "        dataset_name: A string representing the name of the dataset (e.g., 'Dataset 50').\n",
    "    \"\"\"\n",
    "    # Results from grid search\n",
    "    results = grid.cv_results_\n",
    "    means_test = np.sqrt(-results['mean_test_score'])\n",
    "    stds_test = np.sqrt(results['std_test_score'])\n",
    "    means_train = np.sqrt(-results['mean_train_score'])\n",
    "    stds_train = np.sqrt(results['std_train_score'])\n",
    "\n",
    "    # Getting indexes of values per hyper-parameter\n",
    "    masks = []\n",
    "    masks_names = list(grid.best_params_.keys())\n",
    "    for p_k, p_v in grid.best_params_.items():\n",
    "        masks.append(list(results['param_' + p_k].data == p_v))\n",
    "\n",
    "    params = grid.param_grid\n",
    "\n",
    "    # Plotting results\n",
    "    fig, ax = plt.subplots(1, len(params), sharex='none', sharey='all', figsize=(20, 5))\n",
    "    fig.suptitle(f'Random Forest Hyperparameter Tuning Results ({dataset_name})')\n",
    "    fig.text(0.04, 0.5, 'MEAN RMSE', va='center', rotation='vertical')\n",
    "    for i, p in enumerate(masks_names):\n",
    "        m = np.stack(masks[:i] + masks[i + 1:])\n",
    "        best_params_mask = m.all(axis=0)\n",
    "        best_index = np.where(best_params_mask)[0]\n",
    "        x = np.array(params[p])\n",
    "        y_1 = np.array(means_test[best_index])\n",
    "        e_1 = np.array(stds_test[best_index])\n",
    "        y_2 = np.array(means_train[best_index])\n",
    "        e_2 = np.array(stds_train[best_index])\n",
    "        ax[i].errorbar(x, y_1, e_1, linestyle='--', marker='o', label='Validation')\n",
    "        ax[i].errorbar(x, y_2, e_2, linestyle='-', marker='^', label='Train')\n",
    "        ax[i].set_xlabel(p.upper())\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Function to train and evaluate the Random Forest model with Grid Search\n",
    "def train_evaluate_rf_grid_search(train_df, val_df, target_column='Power(MW)', lookback_steps=8, dataset_name='Dataset'):\n",
    "    # Create look-back features\n",
    "    X_train, y_train = create_lookback_features(train_df, target_column, lookback_steps)\n",
    "    X_val, y_val = create_lookback_features(val_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape X to 2D array for RandomForestRegressor\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    X_val = X_val.reshape(X_val.shape[0], -1)\n",
    "    \n",
    "    # Define the parameter grid for Grid Search\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [10, 20, 30],\n",
    "        'min_samples_leaf': [2, 4, 6],\n",
    "        'bootstrap': [True, False]\n",
    "    }\n",
    "    \n",
    "    # Initialize the Random Forest model\n",
    "    rf_model = RandomForestRegressor()\n",
    "    \n",
    "    # Perform Grid Search with cross-validation\n",
    "    grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid,\n",
    "                               cv=3, n_jobs=-1, scoring='neg_mean_squared_error', return_train_score=True)\n",
    "    \n",
    "    # Fit the Grid Search model\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best parameters from Grid Search\n",
    "    best_params = grid_search.best_params_\n",
    "    print(f\"Best parameters from Grid Search for {dataset_name}: {best_params}\")\n",
    "    \n",
    "    # Train the Random Forest model with the best parameters\n",
    "    best_rf_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Predict on train and validation sets\n",
    "    y_pred_train = best_rf_model.predict(X_train)\n",
    "    y_pred_val = best_rf_model.predict(X_val)\n",
    "    \n",
    "    # Calculate and print R squared and RMSE\n",
    "    calculate_metrics(y_train, y_pred_train, y_val, y_pred_val, title=f'Random Forest with Grid Search ({dataset_name})')\n",
    "    \n",
    "    # Plot the results of Grid Search\n",
    "    plot_search_results(grid_search, dataset_name)\n",
    "\n",
    "# List of split data frames (train_df, val_df) and their corresponding dataset names\n",
    "split_resampled_dfs = [\n",
    "    (normalized_train_df_50, normalized_val_df_50, 'Dataset 50')\n",
    "]\n",
    "\n",
    "# Loop through each pair of train and validation dataframes\n",
    "for i, (train_df, val_df, dataset_name) in enumerate(split_resampled_dfs):\n",
    "    print(f\"Evaluating Random Forest model with Grid Search for {dataset_name}\")\n",
    "    train_evaluate_rf_grid_search(train_df=train_df,\n",
    "                                  val_df=val_df,\n",
    "                                  target_column='Power(MW)',\n",
    "                                  lookback_steps=8,\n",
    "                                  dataset_name=dataset_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e38b4a",
   "metadata": {},
   "source": [
    "# Build Random Forest model with best combination of Hyperparameters, then forcast On Test Data for all sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762c5586",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Function to create look-back features for time series forecasting\n",
    "def create_lookback_features(df, target_column, lookback_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - lookback_steps):\n",
    "        X.append(df.iloc[i:i + lookback_steps].values)\n",
    "        y.append(df.iloc[i + lookback_steps][target_column])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Function to calculate and print R squared and RMSE for train and test\n",
    "def calculate_metrics(y_true_train, y_pred_train, y_true_test, y_pred_test, title):\n",
    "    r2_train = r2_score(y_true_train, y_pred_train)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_true_train, y_pred_train))\n",
    "    \n",
    "    r2_test = r2_score(y_true_test, y_pred_test)\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_true_test, y_pred_test))\n",
    "    \n",
    "    print(f\"{title} - Train R squared: {r2_train}, RMSE: {rmse_train}\")\n",
    "    print(f\"{title} - Test R squared: {r2_test}, RMSE: {rmse_test}\")\n",
    "\n",
    "# Function to train and evaluate the Random Forest model\n",
    "def train_evaluate_rf(train_df, test_df, target_column='Power(MW)', lookback_steps=8):\n",
    "    # Create look-back features\n",
    "    X_train, y_train = create_lookback_features(train_df, target_column, lookback_steps)\n",
    "    X_test, y_test = create_lookback_features(test_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape X to 2D array for RandomForestRegressor\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "    \n",
    "    # Train the Random Forest model with specified hyperparameters\n",
    "    rf_model = RandomForestRegressor(n_estimators=100, bootstrap=True, max_depth=20, min_samples_leaf=2)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on train and test sets\n",
    "    y_pred_train = rf_model.predict(X_train)\n",
    "    y_pred_test = rf_model.predict(X_test)\n",
    "    \n",
    "    # Calculate and print R squared and RMSE for train and test\n",
    "    calculate_metrics(y_train, y_pred_train, y_test, y_pred_test, title='Random Forest')\n",
    "\n",
    "# List of split data frames (train and test)\n",
    "split_resampled_dfs = [\n",
    "    (normalized_train_df_50, normalized_test_df_50),\n",
    "    (normalized_train_df_130, normalized_test_df_130),\n",
    "    (normalized_train_df_35, normalized_test_df_35),\n",
    "    (normalized_train_df_30, normalized_test_df_30)\n",
    "]\n",
    "\n",
    "# Apply the Random Forest model to each set of split data frames and calculate metrics\n",
    "for i, (train_df, test_df) in enumerate(split_resampled_dfs):\n",
    "    print(f\"Evaluating Random Forest model for resampled dataset {i+1}\")\n",
    "    train_evaluate_rf(train_df=train_df,\n",
    "                      test_df=test_df,\n",
    "                      target_column='Power(MW)',\n",
    "                      lookback_steps=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d553d868",
   "metadata": {},
   "source": [
    "# Feature Importance for All Sites with Different Nominal Power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925c9847",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Function to create look-back features for time series forecasting\n",
    "def create_lookback_features(df, target_column, lookback_steps):\n",
    "    feature_names = df.columns.tolist()\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - lookback_steps):\n",
    "        X.append(df.iloc[i:i + lookback_steps].values)\n",
    "        y.append(df.iloc[i + lookback_steps][target_column])\n",
    "    \n",
    "    # Create labels for each feature to indicate time step\n",
    "    labels = [f\"{feature} (t-{lookback_steps+1-j})\" for j in range(1, lookback_steps+1) for feature in feature_names]\n",
    "    return np.array(X), np.array(y), labels\n",
    "\n",
    "# Function to calculate and print R squared and RMSE for train and test\n",
    "def calculate_metrics(y_true_train, y_pred_train, y_true_test, y_pred_test, title):\n",
    "    r2_train = r2_score(y_true_train, y_pred_train)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_true_train, y_pred_train))\n",
    "    \n",
    "    r2_test = r2_score(y_true_test, y_pred_test)\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_true_test, y_pred_test))\n",
    "    \n",
    "    print(f\"{title} - Train R squared: {r2_train}, RMSE: {rmse_train}\")\n",
    "    print(f\"{title} - Test R squared: {r2_test}, RMSE: {rmse_test}\")\n",
    "\n",
    "# Function to train and evaluate the Random Forest model\n",
    "def train_evaluate_rf(train_df, test_df, target_column='Power(MW)', lookback_steps=8):\n",
    "    # Create look-back features\n",
    "    X_train, y_train, feature_labels = create_lookback_features(train_df, target_column, lookback_steps)\n",
    "    X_test, y_test, _ = create_lookback_features(test_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape X to 2D array for RandomForestRegressor\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "    \n",
    "    # Train the Random Forest model with specified hyperparameters\n",
    "    rf_model = RandomForestRegressor(n_estimators=100, bootstrap=True, max_depth=20, min_samples_leaf=2)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on train and test sets\n",
    "    y_pred_train = rf_model.predict(X_train)\n",
    "    y_pred_test = rf_model.predict(X_test)\n",
    "    \n",
    "    # Calculate and print R squared and RMSE for train and test\n",
    "    calculate_metrics(y_train, y_pred_train, y_test, y_pred_test, title='Random Forest')\n",
    "    \n",
    "    # Plot feature importance with actual feature names indicating time step\n",
    "    importances = rf_model.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    best_indices = indices[:10]  # Select the top 10 features\n",
    "    \n",
    "    # Create a color map for different time steps\n",
    "    cmap = plt.get_cmap('tab20')  # Use a colormap with distinct colors\n",
    "    num_features_per_step = len(train_df.columns)\n",
    "    colors = [cmap(i) for i in range(lookback_steps)]\n",
    "    bar_colors = np.array([colors[j // num_features_per_step] for j in best_indices])\n",
    "    \n",
    "    plt.figure(figsize=(20, 8))\n",
    "    plt.title(\"Feature Importances - Site_30MV\")\n",
    "    bars = plt.bar(range(len(best_indices)), importances[best_indices], color=bar_colors)\n",
    "    \n",
    "    # Add a legend to indicate time steps\n",
    "    handles = [plt.Rectangle((0, 0), 1, 1, color=cmap(i)) for i in range(lookback_steps)]\n",
    "    labels = [f't-{lookback_steps+1-j}' for j in range(lookback_steps)]\n",
    "    plt.legend(handles, labels)\n",
    "    \n",
    "    plt.xticks(range(len(best_indices)), [feature_labels[i] for i in best_indices], rotation=90)\n",
    "    plt.xlim([-1, len(best_indices)])\n",
    "\n",
    "    # Adjust layout and save the plot as PNG\n",
    "    plt.tight_layout()  # Fit the plot to avoid overlap\n",
    "    plt.savefig('feature_importance_plot.png')  # Save the plot as PNG\n",
    "    plt.show()  # Show the plot\n",
    "\n",
    "# List of split data frames (train and test)\n",
    "split_resampled_dfs = [\n",
    "    (normalized_train_df_30, normalized_test_df_50),\n",
    "    (normalized_train_df_30, normalized_test_df_130),\n",
    "    (normalized_train_df_30, normalized_test_df_30),\n",
    "    (normalized_train_df_30, normalized_test_df_35)\n",
    "]\n",
    "\n",
    "# Apply the Random Forest model to each set of split data frames and calculate metrics\n",
    "for i, (train_df, test_df) in enumerate(split_resampled_dfs):\n",
    "    print(f\"Evaluating Random Forest model for resampled dataset {i+1}\")\n",
    "    train_evaluate_rf(train_df, test_df, target_column='Power(MW)', lookback_steps=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00670fbf",
   "metadata": {},
   "source": [
    "# Residual Plot for Actual And Predicted For All Sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcab2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Function to create look-back features for time series forecasting\n",
    "def create_lookback_features(df, target_column, lookback_steps):\n",
    "    feature_names = df.columns.tolist()\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - lookback_steps):\n",
    "        X.append(df.iloc[i:i + lookback_steps].values)\n",
    "        y.append(df.iloc[i + lookback_steps][target_column])\n",
    "    \n",
    "    # Create labels for each feature to indicate time step\n",
    "    labels = [f\"{feature} (t-{lookback_steps+1-j})\" for j in range(1, lookback_steps+1) for feature in feature_names]\n",
    "    return np.array(X), np.array(y), labels\n",
    "\n",
    "# Function to calculate and print R squared and RMSE for train and test\n",
    "def calculate_metrics(y_true_train, y_pred_train, y_true_test, y_pred_test, title):\n",
    "    r2_train = r2_score(y_true_train, y_pred_train)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_true_train, y_pred_train))\n",
    "    \n",
    "    r2_test = r2_score(y_true_test, y_pred_test)\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_true_test, y_pred_test))\n",
    "    \n",
    "    print(f\"{title} - Train R squared: {r2_train}, RMSE: {rmse_train}\")\n",
    "    print(f\"{title} - Test R squared: {r2_test}, RMSE: {rmse_test}\")\n",
    "\n",
    "# Function to train and evaluate the Random Forest model\n",
    "def train_evaluate_rf(train_df, test_df, target_column='Power(MW)', lookback_steps=8):\n",
    "    # Create look-back features\n",
    "    X_train, y_train, feature_labels = create_lookback_features(train_df, target_column, lookback_steps)\n",
    "    X_test, y_test, _ = create_lookback_features(test_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape X to 2D array for RandomForestRegressor\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "    \n",
    "    # Train the Random Forest model with specified hyperparameters\n",
    "    rf_model = RandomForestRegressor(n_estimators=100, bootstrap=True, max_depth=20, min_samples_leaf=2)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on train and test sets\n",
    "    y_pred_train = rf_model.predict(X_train)\n",
    "    y_pred_test = rf_model.predict(X_test)\n",
    "    \n",
    "    # Calculate and print R squared and RMSE for train and test\n",
    "    calculate_metrics(y_train, y_pred_train, y_test, y_pred_test, title='Random Forest')\n",
    "    \n",
    "    # Plot feature importance with actual feature names indicating time step\n",
    "    importances = rf_model.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    # Create a color map for different time steps\n",
    "    cmap = plt.get_cmap('tab20')  # Use a colormap with distinct colors\n",
    "    num_features_per_step = len(train_df.columns)\n",
    "    colors = [cmap(i) for i in range(lookback_steps)]\n",
    "    bar_colors = np.array([colors[j // num_features_per_step] for j in indices])\n",
    "    \n",
    " \n",
    "    \n",
    "    # Plot Actual vs Predicted on Test Data\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(y_test, y_pred_test, alpha=0.5, color='b', label='Test Data')\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', label='Perfect Prediction')\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.title('Actual vs Predicted on Test Data - Site_50MV')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Apply the Random Forest model to each set of split data frames and calculate metrics\n",
    "split_resampled_data = [\n",
    "    (normalized_train_df_50, normalized_test_df_50),\n",
    "    (normalized_train_df_130, normalized_test_df_130),\n",
    "    (normalized_train_df_35, normalized_test_df_35),\n",
    "    (normalized_train_df_30, normalized_test_df_30)\n",
    "]\n",
    "\n",
    "\n",
    "for i, (train_df, test_df) in enumerate(split_resampled_data):\n",
    "    print(f\"Evaluating Random Forest model for resampled dataset {i+1}\")\n",
    "    train_evaluate_rf(train_df, test_df, target_column='Power(MW)', lookback_steps=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab25c68a",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39142e6",
   "metadata": {},
   "source": [
    "# Evaluating LSTM on Validation set without Batch Normalization for site_50MW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1937ee40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Function to create look-back features for time series forecasting\n",
    "def create_lookback_features(df, target_column, lookback_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - lookback_steps):\n",
    "        X.append(df.iloc[i:i + lookback_steps].values)\n",
    "        y.append(df.iloc[i + lookback_steps][target_column])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Function to reverse normalization\n",
    "def reverse_normalization(normalized_values, min_val, max_val):\n",
    "    return normalized_values * (max_val - min_val) + min_val\n",
    "\n",
    "# Function to calculate and print R squared and RMSE\n",
    "def calculate_metrics(y_true_train, y_pred_train, y_true_val, y_pred_val, title, best_epoch=None):\n",
    "    r2_train = r2_score(y_true_train, y_pred_train)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_true_train, y_pred_train))\n",
    "    \n",
    "    r2_val = r2_score(y_true_val, y_pred_val)\n",
    "    rmse_val = np.sqrt(mean_squared_error(y_true_val, y_pred_val))\n",
    "    \n",
    "    print(f\"{title} - Train R squared: {r2_train}, RMSE: {rmse_train}\")\n",
    "    print(f\"{title} - Validation R squared: {r2_val}, RMSE: {rmse_val}\")\n",
    "    \n",
    "    if best_epoch is not None:\n",
    "        print(f\"Best Epoch: {best_epoch}\")\n",
    "\n",
    "# Function to train and evaluate the LSTM model\n",
    "def train_evaluate_lstm(train_df, val_df, target_column='Power(MW)', lookback_steps=8, epochs=100, batch_size=64, site_name='Site', min_train=None, max_train=None):\n",
    "    # Create look-back features\n",
    "    X_train, y_train = create_lookback_features(train_df, target_column, lookback_steps)\n",
    "    X_val, y_val = create_lookback_features(val_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape data for LSTM: [samples, time steps, features]\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2])\n",
    "    X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], X_val.shape[2])\n",
    "    \n",
    "    # Build the LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                   kernel_regularizer=regularizers.l2(0.001)))  # LSTM Layer with L2 regularization\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(LSTM(units=32, return_sequences=False, kernel_regularizer=regularizers.l2(0.001)))  # Second LSTM Layer\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    # Adding Dense layers with ReLU activation\n",
    "    model.add(Dense(24, activation='relu', kernel_regularizer=regularizers.l2(0.001)))  # Hidden Dense layer with ReLU\n",
    "    model.add(Dropout(0.2))  # Dropout layer after Dense layer\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))  # Output layer with Linear activation (default for regression)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    # Initialize lists to store metrics for each epoch\n",
    "    r2_train_list = []\n",
    "    rmse_train_list = []\n",
    "    r2_val_list = []  # List to store R² for validation set\n",
    "    rmse_val_list = []  # List to store RMSE for validation set\n",
    "    \n",
    "    # Initialize variables to track the best epoch\n",
    "    best_epoch_r2 = -np.inf\n",
    "    best_epoch_rmse = np.inf\n",
    "    best_epoch_r2_index = None\n",
    "    best_epoch_rmse_index = None\n",
    "    \n",
    "    # Train the model and track history\n",
    "    for epoch in range(epochs):\n",
    "        # Train for one epoch\n",
    "        history = model.fit(X_train, y_train, epochs=1, batch_size=batch_size, verbose=0,shuffle=False)\n",
    "        \n",
    "        # Get predictions for train and validation data\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_val = model.predict(X_val)\n",
    "        \n",
    "        # Reverse normalization for both predictions and true values\n",
    "        y_pred_train = reverse_normalization(y_pred_train, min_train, max_train)\n",
    "        y_pred_val = reverse_normalization(y_pred_val, min_train, max_train)\n",
    "        y_train_orig = reverse_normalization(y_train, min_train, max_train)\n",
    "        y_val_orig = reverse_normalization(y_val, min_train, max_train)\n",
    "        \n",
    "        # Calculate R-squared and RMSE for the current epoch for both train and validation data\n",
    "        r2_train = r2_score(y_train_orig, y_pred_train)\n",
    "        rmse_train = np.sqrt(mean_squared_error(y_train_orig, y_pred_train))\n",
    "        \n",
    "        r2_val = r2_score(y_val_orig, y_pred_val)\n",
    "        rmse_val = np.sqrt(mean_squared_error(y_val_orig, y_pred_val))\n",
    "        \n",
    "        # Append metrics to lists\n",
    "        r2_train_list.append(r2_train)\n",
    "        rmse_train_list.append(rmse_train)\n",
    "        r2_val_list.append(r2_val)  # Append to validation list\n",
    "        rmse_val_list.append(rmse_val)  # Append to validation list\n",
    "        \n",
    "        # Track best epoch for R² (max)\n",
    "        if r2_val > best_epoch_r2:\n",
    "            best_epoch_r2 = r2_val\n",
    "            best_epoch_r2_index = epoch\n",
    "        \n",
    "        # Track best epoch for RMSE (min)\n",
    "        if rmse_val < best_epoch_rmse:\n",
    "            best_epoch_rmse = rmse_val\n",
    "            best_epoch_rmse_index = epoch\n",
    "    \n",
    "    # Predict on the training and validation sets after training\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    \n",
    "    # Reverse normalization for both predictions and true values\n",
    "    y_pred_train = reverse_normalization(y_pred_train, min_train, max_train)\n",
    "    y_pred_val = reverse_normalization(y_pred_val, min_train, max_train)\n",
    "    y_train_orig = reverse_normalization(y_train, min_train, max_train)\n",
    "    y_val_orig = reverse_normalization(y_val, min_train, max_train)\n",
    "    \n",
    "    # Calculate and print final R squared and RMSE for train and validation sets\n",
    "    calculate_metrics(y_train_orig, y_pred_train, y_val_orig, y_pred_val, title=f'LSTM Model - {site_name}', best_epoch=best_epoch_r2_index)\n",
    "    \n",
    "    # Plot the loss, R squared, and RMSE metrics for train and validation sets\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(10, 12))\n",
    "    \n",
    "    # Plot R2 for train and validation sets\n",
    "    axs[0].plot(range(epochs), r2_train_list, label='Train R²', color='blue', linestyle='-', marker='o')\n",
    "    axs[0].plot(range(epochs), r2_val_list, label='Validation R²', color='green', linestyle='--', marker='s')\n",
    "    axs[0].set_title(f'R² Score per Epoch - {site_name}')\n",
    "    axs[0].set_xlabel('Epochs')\n",
    "    axs[0].set_ylabel('R²')\n",
    "    axs[0].legend()\n",
    "    \n",
    "    # Mark best epoch for R² and display R² value\n",
    "    axs[0].scatter(best_epoch_r2_index, best_epoch_r2, color='red', zorder=5, label=f'Best Epoch: {best_epoch_r2_index}')\n",
    "    axs[0].text(best_epoch_r2_index, best_epoch_r2, f' R²: {best_epoch_r2:.3f}', color='red', fontsize=10, verticalalignment='bottom')\n",
    "    \n",
    "    # Plot RMSE for train and validation sets\n",
    "    axs[1].plot(range(epochs), rmse_train_list, label='Train RMSE', color='red', linestyle='-', marker='x')\n",
    "    axs[1].plot(range(epochs), rmse_val_list, label='Validation RMSE', color='purple', linestyle='--', marker='v')\n",
    "    axs[1].set_title(f'RMSE per Epoch - {site_name}')\n",
    "    axs[1].set_xlabel('Epochs')\n",
    "    axs[1].set_ylabel('RMSE')\n",
    "    axs[1].legend()\n",
    "    \n",
    "    # Mark best epoch for RMSE and display RMSE value\n",
    "    axs[1].scatter(best_epoch_rmse_index, best_epoch_rmse, color='blue', zorder=5, label=f'Best Epoch: {best_epoch_rmse_index}')\n",
    "    axs[1].text(best_epoch_rmse_index, best_epoch_rmse, f' RMSE: {best_epoch_rmse:.3f}', color='blue', fontsize=10, verticalalignment='top')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming your split data frames are named as follows:\n",
    "# (train_resampled_df_50, val_resampled_df_50)\n",
    "# (train_resampled_df_130, val_resampled_df_130)\n",
    "# (train_resampled_df_35, val_resampled_df_35)\n",
    "# (train_resampled_df_30, val_resampled_df_30)\n",
    "\n",
    "# List of split data frames and site names\n",
    "split_resampled_dfs = [\n",
    "    (normalized_train_df_50, normalized_val_df_50, 'Site_50MV')\n",
    "]\n",
    "\n",
    "# Define the min and max values used for normalization\n",
    "min_train_50 = normalized_train_df_50.min().min()  # Get the minimum value from the training data\n",
    "max_train_50 = normalized_train_df_50.max().max()  # Get the maximum value from the training data\n",
    "\n",
    "# Apply the LSTM model to each set of split data frames and calculate metrics\n",
    "for i, (train_df, val_df, site_name) in enumerate(split_resampled_dfs):\n",
    "    print(f\"Evaluating LSTM model for resampled dataset {site_name} ({i+1})\")\n",
    "    train_evaluate_lstm(\n",
    "        train_df=train_df,\n",
    "        val_df=val_df,\n",
    "        target_column='Power(MW)',\n",
    "        lookback_steps=8,\n",
    "        epochs=128,\n",
    "        batch_size=64,\n",
    "        site_name=site_name,\n",
    "        min_train=min_train_50,  # Provide the min value for reversing normalization\n",
    "        max_train=max_train_50   # Provide the max value for reversing normalization\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe8e72e",
   "metadata": {},
   "source": [
    "# Evaluating LSTM on Validation set with Batch Normalization for site_50MW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490be3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "# Function to create look-back features for time series forecasting\n",
    "def create_lookback_features(df, target_column, lookback_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - lookback_steps):\n",
    "        X.append(df.iloc[i:i + lookback_steps].values)\n",
    "        y.append(df.iloc[i + lookback_steps][target_column])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Function to reverse normalization\n",
    "def reverse_normalization(normalized_values, min_val, max_val):\n",
    "    return normalized_values * (max_val - min_val) + min_val\n",
    "\n",
    "# Function to calculate and print R squared and RMSE\n",
    "def calculate_metrics(y_true_train, y_pred_train, y_true_val, y_pred_val, title, best_epoch=None):\n",
    "    r2_train = r2_score(y_true_train, y_pred_train)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_true_train, y_pred_train))\n",
    "    \n",
    "    r2_val = r2_score(y_true_val, y_pred_val)\n",
    "    rmse_val = np.sqrt(mean_squared_error(y_true_val, y_pred_val))\n",
    "    \n",
    "    print(f\"{title} - Train R squared: {r2_train}, RMSE: {rmse_train}\")\n",
    "    print(f\"{title} - Validation R squared: {r2_val}, RMSE: {rmse_val}\")\n",
    "    \n",
    "    if best_epoch is not None:\n",
    "        print(f\"Best Epoch: {best_epoch}\")\n",
    "\n",
    "# Function to train and evaluate the LSTM model\n",
    "def train_evaluate_lstm(train_df, val_df, target_column='Power(MW)', lookback_steps=8, epochs=100, batch_size=64, site_name='Site', min_train=None, max_train=None):\n",
    "    # Create look-back features\n",
    "    X_train, y_train = create_lookback_features(train_df, target_column, lookback_steps)\n",
    "    X_val, y_val = create_lookback_features(val_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape data for LSTM: [samples, time steps, features]\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2])\n",
    "    X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], X_val.shape[2])\n",
    "    \n",
    "    # Build the LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                   kernel_regularizer=regularizers.l2(0.001)))  # LSTM Layer with L2 regularization\n",
    "    model.add(BatchNormalization())  # BatchNormalization after second LSTM\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(LSTM(units=32, return_sequences=False, kernel_regularizer=regularizers.l2(0.001)))  # Second LSTM Layer\n",
    "    model.add(BatchNormalization())  # BatchNormalization after second LSTM\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    # Adding Dense layers with ReLU activation\n",
    "    model.add(Dense(24, activation='relu', kernel_regularizer=regularizers.l2(0.001)))  # Hidden Dense layer with ReLU\n",
    "    model.add(Dropout(0.2))  # Dropout layer after Dense layer\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))  # Output layer with Linear activation (default for regression)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    # Initialize lists to store metrics for each epoch\n",
    "    r2_train_list = []\n",
    "    rmse_train_list = []\n",
    "    r2_val_list = []  # List to store R² for validation set\n",
    "    rmse_val_list = []  # List to store RMSE for validation set\n",
    "    \n",
    "    # Initialize variables to track the best epoch\n",
    "    best_epoch_r2 = -np.inf\n",
    "    best_epoch_rmse = np.inf\n",
    "    best_epoch_r2_index = None\n",
    "    best_epoch_rmse_index = None\n",
    "    \n",
    "    # Train the model and track history\n",
    "    for epoch in range(epochs):\n",
    "        # Train for one epoch\n",
    "        history = model.fit(X_train, y_train, epochs=1, batch_size=batch_size, verbose=2,shuffle=False)\n",
    "        \n",
    "        # Get predictions for train and validation data\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_val = model.predict(X_val)\n",
    "        \n",
    "        # Reverse normalization for both predictions and true values\n",
    "        y_pred_train = reverse_normalization(y_pred_train, min_train, max_train)\n",
    "        y_pred_val = reverse_normalization(y_pred_val, min_train, max_train)\n",
    "        y_train_orig = reverse_normalization(y_train, min_train, max_train)\n",
    "        y_val_orig = reverse_normalization(y_val, min_train, max_train)\n",
    "        \n",
    "        # Calculate R-squared and RMSE for the current epoch for both train and validation data\n",
    "        r2_train = r2_score(y_train_orig, y_pred_train)\n",
    "        rmse_train = np.sqrt(mean_squared_error(y_train_orig, y_pred_train))\n",
    "        \n",
    "        r2_val = r2_score(y_val_orig, y_pred_val)\n",
    "        rmse_val = np.sqrt(mean_squared_error(y_val_orig, y_pred_val))\n",
    "        \n",
    "        # Append metrics to lists\n",
    "        r2_train_list.append(r2_train)\n",
    "        rmse_train_list.append(rmse_train)\n",
    "        r2_val_list.append(r2_val)  # Append to validation list\n",
    "        rmse_val_list.append(rmse_val)  # Append to validation list\n",
    "        \n",
    "        # Track best epoch for R² (max)\n",
    "        if r2_val > best_epoch_r2:\n",
    "            best_epoch_r2 = r2_val\n",
    "            best_epoch_r2_index = epoch\n",
    "        \n",
    "        # Track best epoch for RMSE (min)\n",
    "        if rmse_val < best_epoch_rmse:\n",
    "            best_epoch_rmse = rmse_val\n",
    "            best_epoch_rmse_index = epoch\n",
    "    \n",
    "    # Predict on the training and validation sets after training\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    \n",
    "    # Reverse normalization for both predictions and true values\n",
    "    y_pred_train = reverse_normalization(y_pred_train, min_train, max_train)\n",
    "    y_pred_val = reverse_normalization(y_pred_val, min_train, max_train)\n",
    "    y_train_orig = reverse_normalization(y_train, min_train, max_train)\n",
    "    y_val_orig = reverse_normalization(y_val, min_train, max_train)\n",
    "    \n",
    "    # Calculate and print final R squared and RMSE for train and validation sets\n",
    "    calculate_metrics(y_train_orig, y_pred_train, y_val_orig, y_pred_val, title=f'LSTM Model - {site_name}', best_epoch=best_epoch_r2_index)\n",
    "    \n",
    "    # Plot the loss, R squared, and RMSE metrics for train and validation sets\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(10, 12))\n",
    "    \n",
    "    # Plot R2 for train and validation sets\n",
    "    axs[0].plot(range(epochs), r2_train_list, label='Train R²', color='blue', linestyle='-', marker='o')\n",
    "    axs[0].plot(range(epochs), r2_val_list, label='Validation R²', color='green', linestyle='--', marker='s')\n",
    "    axs[0].set_title(f'R² Score per Epoch - {site_name}')\n",
    "    axs[0].set_xlabel('Epochs')\n",
    "    axs[0].set_ylabel('R²')\n",
    "    axs[0].legend()\n",
    "    \n",
    "    # Mark best epoch for R² and display R² value\n",
    "    axs[0].scatter(best_epoch_r2_index, best_epoch_r2, color='red', zorder=5, label=f'Best Epoch: {best_epoch_r2_index}')\n",
    "    axs[0].text(best_epoch_r2_index, best_epoch_r2, f' R²: {best_epoch_r2:.3f}', color='red', fontsize=10, verticalalignment='bottom')\n",
    "    \n",
    "    # Plot RMSE for train and validation sets\n",
    "    axs[1].plot(range(epochs), rmse_train_list, label='Train RMSE', color='red', linestyle='-', marker='x')\n",
    "    axs[1].plot(range(epochs), rmse_val_list, label='Validation RMSE', color='purple', linestyle='--', marker='v')\n",
    "    axs[1].set_title(f'RMSE per Epoch - {site_name}')\n",
    "    axs[1].set_xlabel('Epochs')\n",
    "    axs[1].set_ylabel('RMSE')\n",
    "    axs[1].legend()\n",
    "    \n",
    "    # Mark best epoch for RMSE and display RMSE value\n",
    "    axs[1].scatter(best_epoch_rmse_index, best_epoch_rmse, color='blue', zorder=5, label=f'Best Epoch: {best_epoch_rmse_index}')\n",
    "    axs[1].text(best_epoch_rmse_index, best_epoch_rmse, f' RMSE: {best_epoch_rmse:.3f}', color='blue', fontsize=10, verticalalignment='top')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming your split data frames are named as follows:\n",
    "# (train_resampled_df_50, val_resampled_df_50)\n",
    "# (train_resampled_df_130, val_resampled_df_130)\n",
    "# (train_resampled_df_35, val_resampled_df_35)\n",
    "# (train_resampled_df_30, val_resampled_df_30)\n",
    "\n",
    "# List of split data frames and site names\n",
    "split_resampled_dfs = [\n",
    "    (normalized_train_df_50, normalized_val_df_50, 'Site_50MV')\n",
    "]\n",
    "\n",
    "# Define the min and max values used for normalization\n",
    "min_train_50 = normalized_train_df_50.min().min()  # Get the minimum value from the training data\n",
    "max_train_50 = normalized_train_df_50.max().max()  # Get the maximum value from the training data\n",
    "\n",
    "# Apply the LSTM model to each set of split data frames and calculate metrics\n",
    "for i, (train_df, val_df, site_name) in enumerate(split_resampled_dfs):\n",
    "    print(f\"Evaluating LSTM model for resampled dataset {site_name} ({i+1})\")\n",
    "    train_evaluate_lstm(\n",
    "        train_df=train_df,\n",
    "        val_df=val_df,\n",
    "        target_column='Power(MW)',\n",
    "        lookback_steps=8,\n",
    "        epochs=128,\n",
    "        batch_size=64,\n",
    "        site_name=site_name,\n",
    "        min_train=min_train_50,  # Provide the min value for reversing normalization\n",
    "        max_train=max_train_50   # Provide the max value for reversing normalization\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f7f4da",
   "metadata": {},
   "source": [
    " # Evaluating LSTM on Validation for site_50MW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c89a50",
   "metadata": {},
   "source": [
    "Consider different combination of number of units and epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7686eeac",
   "metadata": {},
   "source": [
    "Number of Units: 64-32-24, Epochs=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc45dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "# Function to create look-back features for time series forecasting\n",
    "def create_lookback_features(df, target_column, lookback_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - lookback_steps):\n",
    "        X.append(df.iloc[i:i + lookback_steps].values)\n",
    "        y.append(df.iloc[i + lookback_steps][target_column])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Function to reverse normalization\n",
    "def reverse_normalization(normalized_values, min_val, max_val):\n",
    "    return normalized_values * (max_val - min_val) + min_val\n",
    "\n",
    "# Function to calculate and print R squared and RMSE\n",
    "def calculate_metrics(y_true_train, y_pred_train, y_true_val, y_pred_val, title, best_epoch=None):\n",
    "    r2_train = r2_score(y_true_train, y_pred_train)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_true_train, y_pred_train))\n",
    "    \n",
    "    r2_val = r2_score(y_true_val, y_pred_val)\n",
    "    rmse_val = np.sqrt(mean_squared_error(y_true_val, y_pred_val))\n",
    "    \n",
    "    print(f\"{title} - Train R squared: {r2_train}, RMSE: {rmse_train}\")\n",
    "    print(f\"{title} - Validation R squared: {r2_val}, RMSE: {rmse_val}\")\n",
    "    \n",
    "    if best_epoch is not None:\n",
    "        print(f\"Best Epoch: {best_epoch}\")\n",
    "\n",
    "# Function to train and evaluate the LSTM model\n",
    "def train_evaluate_lstm(train_df, val_df, target_column='Power(MW)', lookback_steps=8, epochs=100, batch_size=64, site_name='Site', min_train=None, max_train=None):\n",
    "    # Create look-back features\n",
    "    X_train, y_train = create_lookback_features(train_df, target_column, lookback_steps)\n",
    "    X_val, y_val = create_lookback_features(val_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape data for LSTM: [samples, time steps, features]\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2])\n",
    "    X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], X_val.shape[2])\n",
    "    \n",
    "    # Build the LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                   kernel_regularizer=regularizers.l2(0.001)))  # LSTM Layer with L2 regularization\n",
    "    model.add(BatchNormalization())  # BatchNormalization after second LSTM\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(LSTM(units=32, return_sequences=False, kernel_regularizer=regularizers.l2(0.001)))  # Second LSTM Layer\n",
    "    model.add(BatchNormalization())  # BatchNormalization after second LSTM\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    # Adding Dense layers with ReLU activation\n",
    "    model.add(Dense(24, activation='relu', kernel_regularizer=regularizers.l2(0.001)))  # Hidden Dense layer with ReLU\n",
    "    model.add(Dropout(0.2))  # Dropout layer after Dense layer\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))  # Output layer with Linear activation (default for regression)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    # Initialize lists to store metrics for each epoch\n",
    "    r2_train_list = []\n",
    "    rmse_train_list = []\n",
    "    r2_val_list = []  # List to store R² for validation set\n",
    "    rmse_val_list = []  # List to store RMSE for validation set\n",
    "    \n",
    "    # Initialize variables to track the best epoch\n",
    "    best_epoch_r2 = -np.inf\n",
    "    best_epoch_rmse = np.inf\n",
    "    best_epoch_r2_index = None\n",
    "    best_epoch_rmse_index = None\n",
    "    \n",
    "    # Train the model and track history\n",
    "    for epoch in range(epochs):\n",
    "        # Train for one epoch\n",
    "        history = model.fit(X_train, y_train, epochs=1, batch_size=batch_size, verbose=2,shuffle=False)\n",
    "        \n",
    "        # Get predictions for train and validation data\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_val = model.predict(X_val)\n",
    "        \n",
    "        # Reverse normalization for both predictions and true values\n",
    "        y_pred_train = reverse_normalization(y_pred_train, min_train, max_train)\n",
    "        y_pred_val = reverse_normalization(y_pred_val, min_train, max_train)\n",
    "        y_train_orig = reverse_normalization(y_train, min_train, max_train)\n",
    "        y_val_orig = reverse_normalization(y_val, min_train, max_train)\n",
    "        \n",
    "        # Calculate R-squared and RMSE for the current epoch for both train and validation data\n",
    "        r2_train = r2_score(y_train_orig, y_pred_train)\n",
    "        rmse_train = np.sqrt(mean_squared_error(y_train_orig, y_pred_train))\n",
    "        \n",
    "        r2_val = r2_score(y_val_orig, y_pred_val)\n",
    "        rmse_val = np.sqrt(mean_squared_error(y_val_orig, y_pred_val))\n",
    "        \n",
    "        # Append metrics to lists\n",
    "        r2_train_list.append(r2_train)\n",
    "        rmse_train_list.append(rmse_train)\n",
    "        r2_val_list.append(r2_val)  # Append to validation list\n",
    "        rmse_val_list.append(rmse_val)  # Append to validation list\n",
    "        \n",
    "        # Track best epoch for R² (max)\n",
    "        if r2_val > best_epoch_r2:\n",
    "            best_epoch_r2 = r2_val\n",
    "            best_epoch_r2_index = epoch\n",
    "        \n",
    "        # Track best epoch for RMSE (min)\n",
    "        if rmse_val < best_epoch_rmse:\n",
    "            best_epoch_rmse = rmse_val\n",
    "            best_epoch_rmse_index = epoch\n",
    "    \n",
    "    # Predict on the training and validation sets after training\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    \n",
    "    # Reverse normalization for both predictions and true values\n",
    "    y_pred_train = reverse_normalization(y_pred_train, min_train, max_train)\n",
    "    y_pred_val = reverse_normalization(y_pred_val, min_train, max_train)\n",
    "    y_train_orig = reverse_normalization(y_train, min_train, max_train)\n",
    "    y_val_orig = reverse_normalization(y_val, min_train, max_train)\n",
    "    \n",
    "    # Calculate and print final R squared and RMSE for train and validation sets\n",
    "    calculate_metrics(y_train_orig, y_pred_train, y_val_orig, y_pred_val, title=f'LSTM Model - {site_name}', best_epoch=best_epoch_r2_index)\n",
    "    \n",
    "    # Plot the loss, R squared, and RMSE metrics for train and validation sets\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(10, 12))\n",
    "    \n",
    "    # Plot R2 for train and validation sets\n",
    "    axs[0].plot(range(epochs), r2_train_list, label='Train R²', color='blue', linestyle='-', marker='o')\n",
    "    axs[0].plot(range(epochs), r2_val_list, label='Validation R²', color='green', linestyle='--', marker='s')\n",
    "    axs[0].set_title(f'R² Score per Epoch (LSTM: LB=8, Epoch=64, Units=64-32-24) - {site_name}')\n",
    "    axs[0].set_xlabel('Epochs')\n",
    "    axs[0].set_ylabel('R²')\n",
    "    axs[0].legend()\n",
    "    \n",
    "    # Mark best epoch for R² and display R² value\n",
    "    axs[0].scatter(best_epoch_r2_index, best_epoch_r2, color='red', zorder=5, label=f'Best Epoch: {best_epoch_r2_index}')\n",
    "    axs[0].text(best_epoch_r2_index, best_epoch_r2, f' R²: {best_epoch_r2:.3f}', color='red', fontsize=10, verticalalignment='bottom')\n",
    "    \n",
    "    # Plot RMSE for train and validation sets\n",
    "    axs[1].plot(range(epochs), rmse_train_list, label='Train RMSE', color='red', linestyle='-', marker='x')\n",
    "    axs[1].plot(range(epochs), rmse_val_list, label='Validation RMSE', color='purple', linestyle='--', marker='v')\n",
    "    axs[1].set_title(f'RMSE per Epoch (LSTM: LB=8, Epoch=64, Units=64-32-24) - {site_name}')\n",
    "    axs[1].set_xlabel('Epochs')\n",
    "    axs[1].set_ylabel('RMSE')\n",
    "    axs[1].legend()\n",
    "    \n",
    "    # Mark best epoch for RMSE and display RMSE value\n",
    "    axs[1].scatter(best_epoch_rmse_index, best_epoch_rmse, color='blue', zorder=5, label=f'Best Epoch: {best_epoch_rmse_index}')\n",
    "    axs[1].text(best_epoch_rmse_index, best_epoch_rmse, f' RMSE: {best_epoch_rmse:.3f}', color='blue', fontsize=10, verticalalignment='top')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming your split data frames are named as follows:\n",
    "# (train_resampled_df_50, val_resampled_df_50)\n",
    "# (train_resampled_df_130, val_resampled_df_130)\n",
    "# (train_resampled_df_35, val_resampled_df_35)\n",
    "# (train_resampled_df_30, val_resampled_df_30)\n",
    "\n",
    "# List of split data frames and site names\n",
    "split_resampled_dfs = [\n",
    "    (normalized_train_df_50, normalized_val_df_50, 'Site_50MV')\n",
    "]\n",
    "\n",
    "# Define the min and max values used for normalization\n",
    "min_train_50 = normalized_train_df_50.min().min()  # Get the minimum value from the training data\n",
    "max_train_50 = normalized_train_df_50.max().max()  # Get the maximum value from the training data\n",
    "\n",
    "# Apply the LSTM model to each set of split data frames and calculate metrics\n",
    "for i, (train_df, val_df, site_name) in enumerate(split_resampled_dfs):\n",
    "    print(f\"Evaluating LSTM model for resampled dataset {site_name} ({i+1})\")\n",
    "    train_evaluate_lstm(\n",
    "        train_df=train_df,\n",
    "        val_df=val_df,\n",
    "        target_column='Power(MW)',\n",
    "        lookback_steps=8,\n",
    "        epochs=64,\n",
    "        batch_size=64,\n",
    "        site_name=site_name,\n",
    "        min_train=min_train_50,  # Provide the min value for reversing normalization\n",
    "        max_train=max_train_50   # Provide the max value for reversing normalization\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772619a4",
   "metadata": {},
   "source": [
    "Number of Units: 32-16-20, Epochs=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822b25a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "# Function to create look-back features for time series forecasting\n",
    "def create_lookback_features(df, target_column, lookback_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - lookback_steps):\n",
    "        X.append(df.iloc[i:i + lookback_steps].values)\n",
    "        y.append(df.iloc[i + lookback_steps][target_column])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Function to reverse normalization\n",
    "def reverse_normalization(normalized_values, min_val, max_val):\n",
    "    return normalized_values * (max_val - min_val) + min_val\n",
    "\n",
    "# Function to calculate and print R squared and RMSE\n",
    "def calculate_metrics(y_true_train, y_pred_train, y_true_val, y_pred_val, title, best_epoch=None):\n",
    "    r2_train = r2_score(y_true_train, y_pred_train)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_true_train, y_pred_train))\n",
    "    \n",
    "    r2_val = r2_score(y_true_val, y_pred_val)\n",
    "    rmse_val = np.sqrt(mean_squared_error(y_true_val, y_pred_val))\n",
    "    \n",
    "    print(f\"{title} - Train R squared: {r2_train}, RMSE: {rmse_train}\")\n",
    "    print(f\"{title} - Validation R squared: {r2_val}, RMSE: {rmse_val}\")\n",
    "    \n",
    "    if best_epoch is not None:\n",
    "        print(f\"Best Epoch: {best_epoch}\")\n",
    "\n",
    "# Function to train and evaluate the LSTM model\n",
    "def train_evaluate_lstm(train_df, val_df, target_column='Power(MW)', lookback_steps=8, epochs=100, batch_size=64, site_name='Site', min_train=None, max_train=None):\n",
    "    # Create look-back features\n",
    "    X_train, y_train = create_lookback_features(train_df, target_column, lookback_steps)\n",
    "    X_val, y_val = create_lookback_features(val_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape data for LSTM: [samples, time steps, features]\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2])\n",
    "    X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], X_val.shape[2])\n",
    "    \n",
    "    # Build the LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=32, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                   kernel_regularizer=regularizers.l2(0.001)))  # LSTM Layer with L2 regularization\n",
    "    model.add(BatchNormalization())  # BatchNormalization after second LSTM\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(LSTM(units=16, return_sequences=False, kernel_regularizer=regularizers.l2(0.001)))  # Second LSTM Layer\n",
    "    model.add(BatchNormalization())  # BatchNormalization after second LSTM\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    # Adding Dense layers with ReLU activation\n",
    "    model.add(Dense(20, activation='relu', kernel_regularizer=regularizers.l2(0.001)))  # Hidden Dense layer with ReLU\n",
    "    model.add(Dropout(0.2))  # Dropout layer after Dense layer\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))  # Output layer with Linear activation (default for regression)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    # Initialize lists to store metrics for each epoch\n",
    "    r2_train_list = []\n",
    "    rmse_train_list = []\n",
    "    r2_val_list = []  # List to store R² for validation set\n",
    "    rmse_val_list = []  # List to store RMSE for validation set\n",
    "    \n",
    "    # Initialize variables to track the best epoch\n",
    "    best_epoch_r2 = -np.inf\n",
    "    best_epoch_rmse = np.inf\n",
    "    best_epoch_r2_index = None\n",
    "    best_epoch_rmse_index = None\n",
    "    \n",
    "    # Train the model and track history\n",
    "    for epoch in range(epochs):\n",
    "        # Train for one epoch\n",
    "        history = model.fit(X_train, y_train, epochs=1, batch_size=batch_size, verbose=2,shuffle=False)\n",
    "        \n",
    "        # Get predictions for train and validation data\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_val = model.predict(X_val)\n",
    "        \n",
    "        # Reverse normalization for both predictions and true values\n",
    "        y_pred_train = reverse_normalization(y_pred_train, min_train, max_train)\n",
    "        y_pred_val = reverse_normalization(y_pred_val, min_train, max_train)\n",
    "        y_train_orig = reverse_normalization(y_train, min_train, max_train)\n",
    "        y_val_orig = reverse_normalization(y_val, min_train, max_train)\n",
    "        \n",
    "        # Calculate R-squared and RMSE for the current epoch for both train and validation data\n",
    "        r2_train = r2_score(y_train_orig, y_pred_train)\n",
    "        rmse_train = np.sqrt(mean_squared_error(y_train_orig, y_pred_train))\n",
    "        \n",
    "        r2_val = r2_score(y_val_orig, y_pred_val)\n",
    "        rmse_val = np.sqrt(mean_squared_error(y_val_orig, y_pred_val))\n",
    "        \n",
    "        # Append metrics to lists\n",
    "        r2_train_list.append(r2_train)\n",
    "        rmse_train_list.append(rmse_train)\n",
    "        r2_val_list.append(r2_val)  # Append to validation list\n",
    "        rmse_val_list.append(rmse_val)  # Append to validation list\n",
    "        \n",
    "        # Track best epoch for R² (max)\n",
    "        if r2_val > best_epoch_r2:\n",
    "            best_epoch_r2 = r2_val\n",
    "            best_epoch_r2_index = epoch\n",
    "        \n",
    "        # Track best epoch for RMSE (min)\n",
    "        if rmse_val < best_epoch_rmse:\n",
    "            best_epoch_rmse = rmse_val\n",
    "            best_epoch_rmse_index = epoch\n",
    "    \n",
    "    # Predict on the training and validation sets after training\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    \n",
    "    # Reverse normalization for both predictions and true values\n",
    "    y_pred_train = reverse_normalization(y_pred_train, min_train, max_train)\n",
    "    y_pred_val = reverse_normalization(y_pred_val, min_train, max_train)\n",
    "    y_train_orig = reverse_normalization(y_train, min_train, max_train)\n",
    "    y_val_orig = reverse_normalization(y_val, min_train, max_train)\n",
    "    \n",
    "    # Calculate and print final R squared and RMSE for train and validation sets\n",
    "    calculate_metrics(y_train_orig, y_pred_train, y_val_orig, y_pred_val, title=f'LSTM Model - {site_name}', best_epoch=best_epoch_r2_index)\n",
    "    \n",
    "    # Plot the loss, R squared, and RMSE metrics for train and validation sets\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(10, 12))\n",
    "    \n",
    "    # Plot R2 for train and validation sets\n",
    "    axs[0].plot(range(epochs), r2_train_list, label='Train R²', color='blue', linestyle='-', marker='o')\n",
    "    axs[0].plot(range(epochs), r2_val_list, label='Validation R²', color='green', linestyle='--', marker='s')\n",
    "    axs[0].set_title(f'R² Score per Epoch (LSTM: LB=8, Epoch=128, Units=32-16-20) - {site_name}')\n",
    "    axs[0].set_xlabel('Epochs')\n",
    "    axs[0].set_ylabel('R²')\n",
    "    axs[0].legend()\n",
    "    \n",
    "    # Mark best epoch for R² and display R² value\n",
    "    axs[0].scatter(best_epoch_r2_index, best_epoch_r2, color='red', zorder=5, label=f'Best Epoch: {best_epoch_r2_index}')\n",
    "    axs[0].text(best_epoch_r2_index, best_epoch_r2, f' R²: {best_epoch_r2:.3f}', color='red', fontsize=10, verticalalignment='bottom')\n",
    "    \n",
    "    # Plot RMSE for train and validation sets\n",
    "    axs[1].plot(range(epochs), rmse_train_list, label='Train RMSE', color='red', linestyle='-', marker='x')\n",
    "    axs[1].plot(range(epochs), rmse_val_list, label='Validation RMSE', color='purple', linestyle='--', marker='v')\n",
    "    axs[1].set_title(f'RMSE per Epoch (LSTM: LB=8, Epoch=128, Units=32-16-20) - {site_name}')\n",
    "    axs[1].set_xlabel('Epochs')\n",
    "    axs[1].set_ylabel('RMSE')\n",
    "    axs[1].legend()\n",
    "    \n",
    "    # Mark best epoch for RMSE and display RMSE value\n",
    "    axs[1].scatter(best_epoch_rmse_index, best_epoch_rmse, color='blue', zorder=5, label=f'Best Epoch: {best_epoch_rmse_index}')\n",
    "    axs[1].text(best_epoch_rmse_index, best_epoch_rmse, f' RMSE: {best_epoch_rmse:.3f}', color='blue', fontsize=10, verticalalignment='top')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming your split data frames are named as follows:\n",
    "# (train_resampled_df_50, val_resampled_df_50)\n",
    "# (train_resampled_df_130, val_resampled_df_130)\n",
    "# (train_resampled_df_35, val_resampled_df_35)\n",
    "# (train_resampled_df_30, val_resampled_df_30)\n",
    "\n",
    "# List of split data frames and site names\n",
    "split_resampled_dfs = [\n",
    "    (normalized_train_df_50, normalized_val_df_50, 'Site_50MV')\n",
    "]\n",
    "\n",
    "# Define the min and max values used for normalization\n",
    "min_train_50 = normalized_train_df_50.min().min()  # Get the minimum value from the training data\n",
    "max_train_50 = normalized_train_df_50.max().max()  # Get the maximum value from the training data\n",
    "\n",
    "# Apply the LSTM model to each set of split data frames and calculate metrics\n",
    "for i, (train_df, val_df, site_name) in enumerate(split_resampled_dfs):\n",
    "    print(f\"Evaluating LSTM model for resampled dataset {site_name} ({i+1})\")\n",
    "    train_evaluate_lstm(\n",
    "        train_df=train_df,\n",
    "        val_df=val_df,\n",
    "        target_column='Power(MW)',\n",
    "        lookback_steps=8,\n",
    "        epochs=128,\n",
    "        batch_size=64,\n",
    "        site_name=site_name,\n",
    "        min_train=min_train_50,  # Provide the min value for reversing normalization\n",
    "        max_train=max_train_50   # Provide the max value for reversing normalization\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25936982",
   "metadata": {},
   "source": [
    "Number of Units: 64-32-24, Epochs=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dfa4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "# Function to create look-back features for time series forecasting\n",
    "def create_lookback_features(df, target_column, lookback_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - lookback_steps):\n",
    "        X.append(df.iloc[i:i + lookback_steps].values)\n",
    "        y.append(df.iloc[i + lookback_steps][target_column])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Function to reverse normalization\n",
    "def reverse_normalization(normalized_values, min_val, max_val):\n",
    "    return normalized_values * (max_val - min_val) + min_val\n",
    "\n",
    "# Function to calculate and print R squared and RMSE\n",
    "def calculate_metrics(y_true_train, y_pred_train, y_true_val, y_pred_val, title, best_epoch=None):\n",
    "    r2_train = r2_score(y_true_train, y_pred_train)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_true_train, y_pred_train))\n",
    "    \n",
    "    r2_val = r2_score(y_true_val, y_pred_val)\n",
    "    rmse_val = np.sqrt(mean_squared_error(y_true_val, y_pred_val))\n",
    "    \n",
    "    print(f\"{title} - Train R squared: {r2_train}, RMSE: {rmse_train}\")\n",
    "    print(f\"{title} - Validation R squared: {r2_val}, RMSE: {rmse_val}\")\n",
    "    \n",
    "    if best_epoch is not None:\n",
    "        print(f\"Best Epoch: {best_epoch}\")\n",
    "\n",
    "# Function to train and evaluate the LSTM model\n",
    "def train_evaluate_lstm(train_df, val_df, target_column='Power(MW)', lookback_steps=8, epochs=100, batch_size=64, site_name='Site', min_train=None, max_train=None):\n",
    "    # Create look-back features\n",
    "    X_train, y_train = create_lookback_features(train_df, target_column, lookback_steps)\n",
    "    X_val, y_val = create_lookback_features(val_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape data for LSTM: [samples, time steps, features]\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2])\n",
    "    X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], X_val.shape[2])\n",
    "    \n",
    "    # Build the LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                   kernel_regularizer=regularizers.l2(0.001)))  # LSTM Layer with L2 regularization\n",
    "    model.add(BatchNormalization())  # BatchNormalization after second LSTM\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(LSTM(units=32, return_sequences=False, kernel_regularizer=regularizers.l2(0.001)))  # Second LSTM Layer\n",
    "    model.add(BatchNormalization())  # BatchNormalization after second LSTM\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    # Adding Dense layers with ReLU activation\n",
    "    model.add(Dense(24, activation='relu', kernel_regularizer=regularizers.l2(0.001)))  # Hidden Dense layer with ReLU\n",
    "    model.add(Dropout(0.2))  # Dropout layer after Dense layer\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))  # Output layer with Linear activation (default for regression)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    # Initialize lists to store metrics for each epoch\n",
    "    r2_train_list = []\n",
    "    rmse_train_list = []\n",
    "    r2_val_list = []  # List to store R² for validation set\n",
    "    rmse_val_list = []  # List to store RMSE for validation set\n",
    "    \n",
    "    # Initialize variables to track the best epoch\n",
    "    best_epoch_r2 = -np.inf\n",
    "    best_epoch_rmse = np.inf\n",
    "    best_epoch_r2_index = None\n",
    "    best_epoch_rmse_index = None\n",
    "    \n",
    "    # Train the model and track history\n",
    "    for epoch in range(epochs):\n",
    "        # Train for one epoch\n",
    "        history = model.fit(X_train, y_train, epochs=1, batch_size=batch_size, verbose=2,shuffle=False)\n",
    "        \n",
    "        # Get predictions for train and validation data\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_val = model.predict(X_val)\n",
    "        \n",
    "        # Reverse normalization for both predictions and true values\n",
    "        y_pred_train = reverse_normalization(y_pred_train, min_train, max_train)\n",
    "        y_pred_val = reverse_normalization(y_pred_val, min_train, max_train)\n",
    "        y_train_orig = reverse_normalization(y_train, min_train, max_train)\n",
    "        y_val_orig = reverse_normalization(y_val, min_train, max_train)\n",
    "        \n",
    "        # Calculate R-squared and RMSE for the current epoch for both train and validation data\n",
    "        r2_train = r2_score(y_train_orig, y_pred_train)\n",
    "        rmse_train = np.sqrt(mean_squared_error(y_train_orig, y_pred_train))\n",
    "        \n",
    "        r2_val = r2_score(y_val_orig, y_pred_val)\n",
    "        rmse_val = np.sqrt(mean_squared_error(y_val_orig, y_pred_val))\n",
    "        \n",
    "        # Append metrics to lists\n",
    "        r2_train_list.append(r2_train)\n",
    "        rmse_train_list.append(rmse_train)\n",
    "        r2_val_list.append(r2_val)  # Append to validation list\n",
    "        rmse_val_list.append(rmse_val)  # Append to validation list\n",
    "        \n",
    "        # Track best epoch for R² (max)\n",
    "        if r2_val > best_epoch_r2:\n",
    "            best_epoch_r2 = r2_val\n",
    "            best_epoch_r2_index = epoch\n",
    "        \n",
    "        # Track best epoch for RMSE (min)\n",
    "        if rmse_val < best_epoch_rmse:\n",
    "            best_epoch_rmse = rmse_val\n",
    "            best_epoch_rmse_index = epoch\n",
    "    \n",
    "    # Predict on the training and validation sets after training\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    \n",
    "    # Reverse normalization for both predictions and true values\n",
    "    y_pred_train = reverse_normalization(y_pred_train, min_train, max_train)\n",
    "    y_pred_val = reverse_normalization(y_pred_val, min_train, max_train)\n",
    "    y_train_orig = reverse_normalization(y_train, min_train, max_train)\n",
    "    y_val_orig = reverse_normalization(y_val, min_train, max_train)\n",
    "    \n",
    "    # Calculate and print final R squared and RMSE for train and validation sets\n",
    "    calculate_metrics(y_train_orig, y_pred_train, y_val_orig, y_pred_val, title=f'LSTM Model - {site_name}', best_epoch=best_epoch_r2_index)\n",
    "    \n",
    "    # Plot the loss, R squared, and RMSE metrics for train and validation sets\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(10, 12))\n",
    "    \n",
    "    # Plot R2 for train and validation sets\n",
    "    axs[0].plot(range(epochs), r2_train_list, label='Train R²', color='blue', linestyle='-', marker='o')\n",
    "    axs[0].plot(range(epochs), r2_val_list, label='Validation R²', color='green', linestyle='--', marker='s')\n",
    "    axs[0].set_title(f'R² Score per Epoch (LSTM: LB=8, Epoch=64, Units=64-32-24) - {site_name}')\n",
    "    axs[0].set_xlabel('Epochs')\n",
    "    axs[0].set_ylabel('R²')\n",
    "    axs[0].legend()\n",
    "    \n",
    "    # Mark best epoch for R² and display R² value\n",
    "    axs[0].scatter(best_epoch_r2_index, best_epoch_r2, color='red', zorder=5, label=f'Best Epoch: {best_epoch_r2_index}')\n",
    "    axs[0].text(best_epoch_r2_index, best_epoch_r2, f' R²: {best_epoch_r2:.3f}', color='red', fontsize=10, verticalalignment='bottom')\n",
    "    \n",
    "    # Plot RMSE for train and validation sets\n",
    "    axs[1].plot(range(epochs), rmse_train_list, label='Train RMSE', color='red', linestyle='-', marker='x')\n",
    "    axs[1].plot(range(epochs), rmse_val_list, label='Validation RMSE', color='purple', linestyle='--', marker='v')\n",
    "    axs[1].set_title(f'RMSE per Epoch (LSTM: LB=8, Epoch=64, Units=64-32-24) - {site_name}')\n",
    "    axs[1].set_xlabel('Epochs')\n",
    "    axs[1].set_ylabel('RMSE')\n",
    "    axs[1].legend()\n",
    "    \n",
    "    # Mark best epoch for RMSE and display RMSE value\n",
    "    axs[1].scatter(best_epoch_rmse_index, best_epoch_rmse, color='blue', zorder=5, label=f'Best Epoch: {best_epoch_rmse_index}')\n",
    "    axs[1].text(best_epoch_rmse_index, best_epoch_rmse, f' RMSE: {best_epoch_rmse:.3f}', color='blue', fontsize=10, verticalalignment='top')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming your split data frames are named as follows:\n",
    "# (train_resampled_df_50, val_resampled_df_50)\n",
    "# (train_resampled_df_130, val_resampled_df_130)\n",
    "# (train_resampled_df_35, val_resampled_df_35)\n",
    "# (train_resampled_df_30, val_resampled_df_30)\n",
    "\n",
    "# List of split data frames and site names\n",
    "split_resampled_dfs = [\n",
    "    (normalized_train_df_50, normalized_val_df_50, 'Site_50MV')\n",
    "]\n",
    "\n",
    "# Define the min and max values used for normalization\n",
    "min_train_50 = normalized_train_df_50.min().min()  # Get the minimum value from the training data\n",
    "max_train_50 = normalized_train_df_50.max().max()  # Get the maximum value from the training data\n",
    "\n",
    "# Apply the LSTM model to each set of split data frames and calculate metrics\n",
    "for i, (train_df, val_df, site_name) in enumerate(split_resampled_dfs):\n",
    "    print(f\"Evaluating LSTM model for resampled dataset {site_name} ({i+1})\")\n",
    "    train_evaluate_lstm(\n",
    "        train_df=train_df,\n",
    "        val_df=val_df,\n",
    "        target_column='Power(MW)',\n",
    "        lookback_steps=8,\n",
    "        epochs=64,\n",
    "        batch_size=64,\n",
    "        site_name=site_name,\n",
    "        min_train=min_train_50,  # Provide the min value for reversing normalization\n",
    "        max_train=max_train_50   # Provide the max value for reversing normalization\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6fb974",
   "metadata": {},
   "source": [
    "Number of Units: 32-16-20, Epochs=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed90351",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "# Function to create look-back features for time series forecasting\n",
    "def create_lookback_features(df, target_column, lookback_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - lookback_steps):\n",
    "        X.append(df.iloc[i:i + lookback_steps].values)\n",
    "        y.append(df.iloc[i + lookback_steps][target_column])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Function to reverse normalization\n",
    "def reverse_normalization(normalized_values, min_val, max_val):\n",
    "    return normalized_values * (max_val - min_val) + min_val\n",
    "\n",
    "# Function to calculate and print R squared and RMSE\n",
    "def calculate_metrics(y_true_train, y_pred_train, y_true_val, y_pred_val, title, best_epoch=None):\n",
    "    r2_train = r2_score(y_true_train, y_pred_train)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_true_train, y_pred_train))\n",
    "    \n",
    "    r2_val = r2_score(y_true_val, y_pred_val)\n",
    "    rmse_val = np.sqrt(mean_squared_error(y_true_val, y_pred_val))\n",
    "    \n",
    "    print(f\"{title} - Train R squared: {r2_train}, RMSE: {rmse_train}\")\n",
    "    print(f\"{title} - Validation R squared: {r2_val}, RMSE: {rmse_val}\")\n",
    "    \n",
    "    if best_epoch is not None:\n",
    "        print(f\"Best Epoch: {best_epoch}\")\n",
    "\n",
    "# Function to train and evaluate the LSTM model\n",
    "def train_evaluate_lstm(train_df, val_df, target_column='Power(MW)', lookback_steps=8, epochs=100, batch_size=64, site_name='Site', min_train=None, max_train=None):\n",
    "    # Create look-back features\n",
    "    X_train, y_train = create_lookback_features(train_df, target_column, lookback_steps)\n",
    "    X_val, y_val = create_lookback_features(val_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape data for LSTM: [samples, time steps, features]\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2])\n",
    "    X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], X_val.shape[2])\n",
    "    \n",
    "    # Build the LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=32, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                   kernel_regularizer=regularizers.l2(0.001)))  # LSTM Layer with L2 regularization\n",
    "    model.add(BatchNormalization())  # BatchNormalization after second LSTM\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(LSTM(units=16, return_sequences=False, kernel_regularizer=regularizers.l2(0.001)))  # Second LSTM Layer\n",
    "    model.add(BatchNormalization())  # BatchNormalization after second LSTM\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    # Adding Dense layers with ReLU activation\n",
    "    model.add(Dense(20, activation='relu', kernel_regularizer=regularizers.l2(0.001)))  # Hidden Dense layer with ReLU\n",
    "    model.add(Dropout(0.2))  # Dropout layer after Dense layer\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))  # Output layer with Linear activation (default for regression)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    # Initialize lists to store metrics for each epoch\n",
    "    r2_train_list = []\n",
    "    rmse_train_list = []\n",
    "    r2_val_list = []  # List to store R² for validation set\n",
    "    rmse_val_list = []  # List to store RMSE for validation set\n",
    "    \n",
    "    # Initialize variables to track the best epoch\n",
    "    best_epoch_r2 = -np.inf\n",
    "    best_epoch_rmse = np.inf\n",
    "    best_epoch_r2_index = None\n",
    "    best_epoch_rmse_index = None\n",
    "    \n",
    "    # Train the model and track history\n",
    "    for epoch in range(epochs):\n",
    "        # Train for one epoch\n",
    "        history = model.fit(X_train, y_train, epochs=1, batch_size=batch_size, verbose=2,shuffle=False)\n",
    "        \n",
    "        # Get predictions for train and validation data\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_val = model.predict(X_val)\n",
    "        \n",
    "        # Reverse normalization for both predictions and true values\n",
    "        y_pred_train = reverse_normalization(y_pred_train, min_train, max_train)\n",
    "        y_pred_val = reverse_normalization(y_pred_val, min_train, max_train)\n",
    "        y_train_orig = reverse_normalization(y_train, min_train, max_train)\n",
    "        y_val_orig = reverse_normalization(y_val, min_train, max_train)\n",
    "        \n",
    "        # Calculate R-squared and RMSE for the current epoch for both train and validation data\n",
    "        r2_train = r2_score(y_train_orig, y_pred_train)\n",
    "        rmse_train = np.sqrt(mean_squared_error(y_train_orig, y_pred_train))\n",
    "        \n",
    "        r2_val = r2_score(y_val_orig, y_pred_val)\n",
    "        rmse_val = np.sqrt(mean_squared_error(y_val_orig, y_pred_val))\n",
    "        \n",
    "        # Append metrics to lists\n",
    "        r2_train_list.append(r2_train)\n",
    "        rmse_train_list.append(rmse_train)\n",
    "        r2_val_list.append(r2_val)  # Append to validation list\n",
    "        rmse_val_list.append(rmse_val)  # Append to validation list\n",
    "        \n",
    "        # Track best epoch for R² (max)\n",
    "        if r2_val > best_epoch_r2:\n",
    "            best_epoch_r2 = r2_val\n",
    "            best_epoch_r2_index = epoch\n",
    "        \n",
    "        # Track best epoch for RMSE (min)\n",
    "        if rmse_val < best_epoch_rmse:\n",
    "            best_epoch_rmse = rmse_val\n",
    "            best_epoch_rmse_index = epoch\n",
    "    \n",
    "    # Predict on the training and validation sets after training\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    \n",
    "    # Reverse normalization for both predictions and true values\n",
    "    y_pred_train = reverse_normalization(y_pred_train, min_train, max_train)\n",
    "    y_pred_val = reverse_normalization(y_pred_val, min_train, max_train)\n",
    "    y_train_orig = reverse_normalization(y_train, min_train, max_train)\n",
    "    y_val_orig = reverse_normalization(y_val, min_train, max_train)\n",
    "    \n",
    "    # Calculate and print final R squared and RMSE for train and validation sets\n",
    "    calculate_metrics(y_train_orig, y_pred_train, y_val_orig, y_pred_val, title=f'LSTM Model - {site_name}', best_epoch=best_epoch_r2_index)\n",
    "    \n",
    "    # Plot the loss, R squared, and RMSE metrics for train and validation sets\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(10, 12))\n",
    "    \n",
    "    # Plot R2 for train and validation sets\n",
    "    axs[0].plot(range(epochs), r2_train_list, label='Train R²', color='blue', linestyle='-', marker='o')\n",
    "    axs[0].plot(range(epochs), r2_val_list, label='Validation R²', color='green', linestyle='--', marker='s')\n",
    "    axs[0].set_title(f'R² Score per Epoch (LSTM: LB=8, Epoch=128, Units=32-16-20) - {site_name}')\n",
    "    axs[0].set_xlabel('Epochs')\n",
    "    axs[0].set_ylabel('R²')\n",
    "    axs[0].legend()\n",
    "    \n",
    "    # Mark best epoch for R² and display R² value\n",
    "    axs[0].scatter(best_epoch_r2_index, best_epoch_r2, color='red', zorder=5, label=f'Best Epoch: {best_epoch_r2_index}')\n",
    "    axs[0].text(best_epoch_r2_index, best_epoch_r2, f' R²: {best_epoch_r2:.3f}', color='red', fontsize=10, verticalalignment='bottom')\n",
    "    \n",
    "    # Plot RMSE for train and validation sets\n",
    "    axs[1].plot(range(epochs), rmse_train_list, label='Train RMSE', color='red', linestyle='-', marker='x')\n",
    "    axs[1].plot(range(epochs), rmse_val_list, label='Validation RMSE', color='purple', linestyle='--', marker='v')\n",
    "    axs[1].set_title(f'RMSE per Epoch (LSTM: LB=8, Epoch=128, Units=32-16-20) - {site_name}')\n",
    "    axs[1].set_xlabel('Epochs')\n",
    "    axs[1].set_ylabel('RMSE')\n",
    "    axs[1].legend()\n",
    "    \n",
    "    # Mark best epoch for RMSE and display RMSE value\n",
    "    axs[1].scatter(best_epoch_rmse_index, best_epoch_rmse, color='blue', zorder=5, label=f'Best Epoch: {best_epoch_rmse_index}')\n",
    "    axs[1].text(best_epoch_rmse_index, best_epoch_rmse, f' RMSE: {best_epoch_rmse:.3f}', color='blue', fontsize=10, verticalalignment='top')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming your split data frames are named as follows:\n",
    "# (train_resampled_df_50, val_resampled_df_50)\n",
    "# (train_resampled_df_130, val_resampled_df_130)\n",
    "# (train_resampled_df_35, val_resampled_df_35)\n",
    "# (train_resampled_df_30, val_resampled_df_30)\n",
    "\n",
    "# List of split data frames and site names\n",
    "split_resampled_dfs = [\n",
    "    (normalized_train_df_50, normalized_val_df_50, 'Site_50MV')\n",
    "]\n",
    "\n",
    "# Define the min and max values used for normalization\n",
    "min_train_50 = normalized_train_df_50.min().min()  # Get the minimum value from the training data\n",
    "max_train_50 = normalized_train_df_50.max().max()  # Get the maximum value from the training data\n",
    "\n",
    "# Apply the LSTM model to each set of split data frames and calculate metrics\n",
    "for i, (train_df, val_df, site_name) in enumerate(split_resampled_dfs):\n",
    "    print(f\"Evaluating LSTM model for resampled dataset {site_name} ({i+1})\")\n",
    "    train_evaluate_lstm(\n",
    "        train_df=train_df,\n",
    "        val_df=val_df,\n",
    "        target_column='Power(MW)',\n",
    "        lookback_steps=8,\n",
    "        epochs=64,\n",
    "        batch_size=64,\n",
    "        site_name=site_name,\n",
    "        min_train=min_train_50,  # Provide the min value for reversing normalization\n",
    "        max_train=max_train_50   # Provide the max value for reversing normalization\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d322d5",
   "metadata": {},
   "source": [
    "# Build LSTM model with best combination of Hyperparameters, then forcast On Test Data for all sites and plot actual vs predicted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86e71de",
   "metadata": {},
   "source": [
    "# Site_50MW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7e65cbe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'normalized_train_df_50' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 106\u001b[0m\n\u001b[0;32m    100\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# Assuming your split data frames are named as follows:\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# (train_resampled_df_50, test_resampled_df_50)\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# Define the min and max values used for normalization\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m min_train_50 \u001b[38;5;241m=\u001b[39m normalized_train_df_50\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;241m.\u001b[39mmin()  \u001b[38;5;66;03m# Get the minimum value from the training data\u001b[39;00m\n\u001b[0;32m    107\u001b[0m max_train_50 \u001b[38;5;241m=\u001b[39m normalized_train_df_50\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mmax()  \u001b[38;5;66;03m# Get the maximum value from the training data\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# List of split data frames and site names\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'normalized_train_df_50' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "# Function to create look-back features for time series forecasting\n",
    "def create_lookback_features(df, target_column, lookback_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - lookback_steps):\n",
    "        X.append(df.iloc[i:i + lookback_steps].values)\n",
    "        y.append(df.iloc[i + lookback_steps][target_column])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Function to calculate and print R squared and RMSE\n",
    "def calculate_metrics(y_true, y_pred, title):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    \n",
    "    print(f\"{title} - R squared: {r2}, RMSE: {rmse}\")\n",
    "\n",
    "# Function to reverse normalization\n",
    "def reverse_normalization(normalized_values, min_val, max_val):\n",
    "    return normalized_values * (max_val - min_val) + min_val\n",
    "\n",
    "# Function to train the LSTM model and predict on test data\n",
    "def train_evaluate_lstm(train_df, test_df, target_column='Power(MW)', lookback_steps=8, epochs=100, batch_size=64, site_name='Site', min_val=None, max_val=None):\n",
    "    # Create look-back features for training data\n",
    "    X_train, y_train = create_lookback_features(train_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape data for GRU: [samples, time steps, features]\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2])\n",
    "    \n",
    "    # Build the LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                   kernel_regularizer=regularizers.l2(0.001)))  # LSTM Layer with L2 regularization\n",
    "    model.add(BatchNormalization())  # BatchNormalization after first LSTM\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(LSTM(units=32, return_sequences=False, kernel_regularizer=regularizers.l2(0.001)))  # Second LSTM Layer\n",
    "    model.add(BatchNormalization())  # BatchNormalization after second LSTM\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    # Adding Dense layers with ReLU activation\n",
    "    model.add(Dense(24, activation='relu', kernel_regularizer=regularizers.l2(0.001)))  # Hidden Dense layer with ReLU\n",
    "    model.add(BatchNormalization())  # BatchNormalization after Dense layer\n",
    "    model.add(Dropout(0.2))  # Dropout layer after Dense layer\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))  # Output layer with Linear activation (default for regression)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1, shuffle=False)\n",
    "    \n",
    "    # Create look-back features for test data\n",
    "    X_test, y_test = create_lookback_features(test_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape test data for LSTM\n",
    "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2])\n",
    "    \n",
    "    # Predict on the test data\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Reverse normalization on both actual and predicted values\n",
    "    y_test_reversed = reverse_normalization(y_test, min_val, max_val)\n",
    "    y_pred_test_reversed = reverse_normalization(y_pred_test, min_val, max_val)\n",
    "    \n",
    "    # Calculate and print R squared and RMSE for the test set\n",
    "    calculate_metrics(y_test_reversed, y_pred_test_reversed, title=f'LSTM Model - {site_name} (Test Data)')\n",
    "\n",
    "    # Define the range of days you want to plot (90th to 100th days)\n",
    "    start_day = 90\n",
    "    end_day = 95\n",
    "    records_per_day = 48  # 24 hours * 2 records per hour (30-minute intervals)\n",
    "\n",
    "    # Calculate the indices corresponding to the 90th and 100th days\n",
    "    start_index = (start_day - 1) * records_per_day  # Start index for 90th day\n",
    "    end_index = end_day * records_per_day  # End index for 100th day\n",
    "\n",
    "    # Slice the test data and predictions for the range from the 90th to 100th day\n",
    "    y_test_range = y_test_reversed[start_index:end_index]\n",
    "    y_pred_test_range = y_pred_test_reversed[start_index:end_index]\n",
    "\n",
    "    # Plot the comparison between actual and predicted values for the 90th to 100th days\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(start_index, end_index), y_test_range, label='Actual', color='blue', linestyle='-', marker='o')\n",
    "    plt.plot(range(start_index, end_index), y_pred_test_range, label='Predicted', color='red', linestyle='--', marker='x')\n",
    "    plt.title(f'Actual vs Predicted Power (LSTM: LB=8, Epoch=128, Unit=64-32-24) - {site_name}')\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('Power (MW)')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming your split data frames are named as follows:\n",
    "# (train_resampled_df_50, test_resampled_df_50)\n",
    "\n",
    "# Define the min and max values used for normalization\n",
    "min_train_50 = normalized_train_df_50.min().min()  # Get the minimum value from the training data\n",
    "max_train_50 = normalized_train_df_50.max().max()  # Get the maximum value from the training data\n",
    "\n",
    "# List of split data frames and site names\n",
    "split_resampled_dfs = [\n",
    "    (normalized_train_df_50, normalized_test_df_50, 'Site_50MV')\n",
    "]\n",
    "\n",
    "# Apply the GRU model to each set of split data frames and calculate metrics\n",
    "for i, (train_df, test_df, site_name) in enumerate(split_resampled_dfs):\n",
    "    print(f\"Evaluating LSTM model for resampled dataset {site_name} ({i+1})\")\n",
    "    train_evaluate_lstm(train_df=train_df,\n",
    "                        test_df=test_df,\n",
    "                        target_column='Power(MW)',\n",
    "                        lookback_steps=8, epochs=128, batch_size=64,\n",
    "                        site_name=site_name,\n",
    "                        min_val=min_train_50,\n",
    "                        max_val=max_train_50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9093c785",
   "metadata": {},
   "source": [
    "# Site_130MW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6748f9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "# Function to create look-back features for time series forecasting\n",
    "def create_lookback_features(df, target_column, lookback_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - lookback_steps):\n",
    "        X.append(df.iloc[i:i + lookback_steps].values)\n",
    "        y.append(df.iloc[i + lookback_steps][target_column])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Function to calculate and print R squared and RMSE\n",
    "def calculate_metrics(y_true, y_pred, title):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    \n",
    "    print(f\"{title} - R squared: {r2}, RMSE: {rmse}\")\n",
    "\n",
    "# Function to reverse normalization\n",
    "def reverse_normalization(normalized_values, min_val, max_val):\n",
    "    return normalized_values * (max_val - min_val) + min_val\n",
    "\n",
    "# Function to train the LSTM model and predict on test data\n",
    "def train_evaluate_lstm(train_df, test_df, target_column='Power(MW)', lookback_steps=8, epochs=100, batch_size=64, site_name='Site', min_val=None, max_val=None):\n",
    "    # Create look-back features for training data\n",
    "    X_train, y_train = create_lookback_features(train_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape data for GRU: [samples, time steps, features]\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2])\n",
    "    \n",
    "    # Build the LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                   kernel_regularizer=regularizers.l2(0.001)))  # LSTM Layer with L2 regularization\n",
    "    model.add(BatchNormalization())  # BatchNormalization after first LSTM\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(LSTM(units=32, return_sequences=False, kernel_regularizer=regularizers.l2(0.001)))  # Second LSTM Layer\n",
    "    model.add(BatchNormalization())  # BatchNormalization after second LSTM\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    # Adding Dense layers with ReLU activation\n",
    "    model.add(Dense(24, activation='relu', kernel_regularizer=regularizers.l2(0.001)))  # Hidden Dense layer with ReLU\n",
    "    model.add(BatchNormalization())  # BatchNormalization after Dense layer\n",
    "    model.add(Dropout(0.2))  # Dropout layer after Dense layer\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))  # Output layer with Linear activation (default for regression)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1, shuffle=False)\n",
    "    \n",
    "    # Create look-back features for test data\n",
    "    X_test, y_test = create_lookback_features(test_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape test data for LSTM\n",
    "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2])\n",
    "    \n",
    "    # Predict on the test data\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Reverse normalization on both actual and predicted values\n",
    "    y_test_reversed = reverse_normalization(y_test, min_val, max_val)\n",
    "    y_pred_test_reversed = reverse_normalization(y_pred_test, min_val, max_val)\n",
    "    \n",
    "    # Calculate and print R squared and RMSE for the test set\n",
    "    calculate_metrics(y_test_reversed, y_pred_test_reversed, title=f'LSTM Model - {site_name} (Test Data)')\n",
    "\n",
    "    # Define the range of days you want to plot (90th to 100th days)\n",
    "    start_day = 90\n",
    "    end_day = 95\n",
    "    records_per_day = 48  # 24 hours * 2 records per hour (30-minute intervals)\n",
    "\n",
    "    # Calculate the indices corresponding to the 90th and 100th days\n",
    "    start_index = (start_day - 1) * records_per_day  # Start index for 90th day\n",
    "    end_index = end_day * records_per_day  # End index for 100th day\n",
    "\n",
    "    # Slice the test data and predictions for the range from the 90th to 100th day\n",
    "    y_test_range = y_test_reversed[start_index:end_index]\n",
    "    y_pred_test_range = y_pred_test_reversed[start_index:end_index]\n",
    "\n",
    "    # Plot the comparison between actual and predicted values for the 90th to 100th days\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(start_index, end_index), y_test_range, label='Actual', color='blue', linestyle='-', marker='o')\n",
    "    plt.plot(range(start_index, end_index), y_pred_test_range, label='Predicted', color='red', linestyle='--', marker='x')\n",
    "    plt.title(f'Actual vs Predicted Power (LSTM: LB=8, Epoch=128, Unit=64-32-24) - {site_name}')\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('Power (MW)')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming your split data frames are named as follows:\n",
    "# (train_resampled_df_50, test_resampled_df_50)\n",
    "\n",
    "# Define the min and max values used for normalization\n",
    "min_train_130 = normalized_train_df_130.min().min()  # Get the minimum value from the training data\n",
    "max_train_130 = normalized_train_df_130.max().max()  # Get the maximum value from the training data\n",
    "\n",
    "# List of split data frames and site names\n",
    "split_resampled_dfs = [\n",
    "    (normalized_train_df_130, normalized_test_df_130, 'Site_130MV')\n",
    "]\n",
    "\n",
    "# Apply the GRU model to each set of split data frames and calculate metrics\n",
    "for i, (train_df, test_df, site_name) in enumerate(split_resampled_dfs):\n",
    "    print(f\"Evaluating LSTM model for resampled dataset {site_name} ({i+1})\")\n",
    "    train_evaluate_lstm(train_df=train_df,\n",
    "                        test_df=test_df,\n",
    "                        target_column='Power(MW)',\n",
    "                        lookback_steps=8, epochs=128, batch_size=64,\n",
    "                        site_name=site_name,\n",
    "                        min_val=min_train_130,\n",
    "                        max_val=max_train_130)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc1ebff",
   "metadata": {},
   "source": [
    "# Site_30MW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91da3e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "# Function to create look-back features for time series forecasting\n",
    "def create_lookback_features(df, target_column, lookback_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - lookback_steps):\n",
    "        X.append(df.iloc[i:i + lookback_steps].values)\n",
    "        y.append(df.iloc[i + lookback_steps][target_column])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Function to calculate and print R squared and RMSE\n",
    "def calculate_metrics(y_true, y_pred, title):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    \n",
    "    print(f\"{title} - R squared: {r2}, RMSE: {rmse}\")\n",
    "\n",
    "# Function to reverse normalization\n",
    "def reverse_normalization(normalized_values, min_val, max_val):\n",
    "    return normalized_values * (max_val - min_val) + min_val\n",
    "\n",
    "# Function to train the LSTM model and predict on test data\n",
    "def train_evaluate_lstm(train_df, test_df, target_column='Power(MW)', lookback_steps=8, epochs=100, batch_size=64, site_name='Site', min_val=None, max_val=None):\n",
    "    # Create look-back features for training data\n",
    "    X_train, y_train = create_lookback_features(train_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape data for GRU: [samples, time steps, features]\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2])\n",
    "    \n",
    "    # Build the LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                   kernel_regularizer=regularizers.l2(0.001)))  # LSTM Layer with L2 regularization\n",
    "    model.add(BatchNormalization())  # BatchNormalization after first LSTM\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(LSTM(units=32, return_sequences=False, kernel_regularizer=regularizers.l2(0.001)))  # Second LSTM Layer\n",
    "    model.add(BatchNormalization())  # BatchNormalization after second LSTM\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    # Adding Dense layers with ReLU activation\n",
    "    model.add(Dense(24, activation='relu', kernel_regularizer=regularizers.l2(0.001)))  # Hidden Dense layer with ReLU\n",
    "    model.add(BatchNormalization())  # BatchNormalization after Dense layer\n",
    "    model.add(Dropout(0.2))  # Dropout layer after Dense layer\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))  # Output layer with Linear activation (default for regression)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1, shuffle=False)\n",
    "    \n",
    "    # Create look-back features for test data\n",
    "    X_test, y_test = create_lookback_features(test_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape test data for LSTM\n",
    "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2])\n",
    "    \n",
    "    # Predict on the test data\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Reverse normalization on both actual and predicted values\n",
    "    y_test_reversed = reverse_normalization(y_test, min_val, max_val)\n",
    "    y_pred_test_reversed = reverse_normalization(y_pred_test, min_val, max_val)\n",
    "    \n",
    "    # Calculate and print R squared and RMSE for the test set\n",
    "    calculate_metrics(y_test_reversed, y_pred_test_reversed, title=f'LSTM Model - {site_name} (Test Data)')\n",
    "\n",
    "    # Define the range of days you want to plot (90th to 100th days)\n",
    "    start_day = 90\n",
    "    end_day = 95\n",
    "    records_per_day = 48  # 24 hours * 2 records per hour (30-minute intervals)\n",
    "\n",
    "    # Calculate the indices corresponding to the 90th and 100th days\n",
    "    start_index = (start_day - 1) * records_per_day  # Start index for 90th day\n",
    "    end_index = end_day * records_per_day  # End index for 100th day\n",
    "\n",
    "    # Slice the test data and predictions for the range from the 90th to 100th day\n",
    "    y_test_range = y_test_reversed[start_index:end_index]\n",
    "    y_pred_test_range = y_pred_test_reversed[start_index:end_index]\n",
    "\n",
    "    # Plot the comparison between actual and predicted values for the 90th to 100th days\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(start_index, end_index), y_test_range, label='Actual', color='blue', linestyle='-', marker='o')\n",
    "    plt.plot(range(start_index, end_index), y_pred_test_range, label='Predicted', color='red', linestyle='--', marker='x')\n",
    "    plt.title(f'Actual vs Predicted Power (LSTM: LB=8, Epoch=128, Unit=64-32-24) - {site_name}')\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('Power (MW)')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming your split data frames are named as follows:\n",
    "# (train_resampled_df_50, test_resampled_df_50)\n",
    "\n",
    "# Define the min and max values used for normalization\n",
    "min_train_30 = normalized_train_df_30.min().min()  # Get the minimum value from the training data\n",
    "max_train_30 = normalized_train_df_30.max().max()  # Get the maximum value from the training data\n",
    "\n",
    "# List of split data frames and site names\n",
    "split_resampled_dfs = [\n",
    "    (normalized_train_df_30, normalized_test_df_30, 'Site_30MV')\n",
    "]\n",
    "\n",
    "# Apply the GRU model to each set of split data frames and calculate metrics\n",
    "for i, (train_df, test_df, site_name) in enumerate(split_resampled_dfs):\n",
    "    print(f\"Evaluating LSTM model for resampled dataset {site_name} ({i+1})\")\n",
    "    train_evaluate_lstm(train_df=train_df,\n",
    "                        test_df=test_df,\n",
    "                        target_column='Power(MW)',\n",
    "                        lookback_steps=8, epochs=128, batch_size=64,\n",
    "                        site_name=site_name,\n",
    "                        min_val=min_train_30,\n",
    "                        max_val=max_train_30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f372b8ea",
   "metadata": {},
   "source": [
    "# Site_35MW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecda41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "# Function to create look-back features for time series forecasting\n",
    "def create_lookback_features(df, target_column, lookback_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - lookback_steps):\n",
    "        X.append(df.iloc[i:i + lookback_steps].values)\n",
    "        y.append(df.iloc[i + lookback_steps][target_column])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Function to calculate and print R squared and RMSE\n",
    "def calculate_metrics(y_true, y_pred, title):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    \n",
    "    print(f\"{title} - R squared: {r2}, RMSE: {rmse}\")\n",
    "\n",
    "# Function to reverse normalization\n",
    "def reverse_normalization(normalized_values, min_val, max_val):\n",
    "    return normalized_values * (max_val - min_val) + min_val\n",
    "\n",
    "# Function to train the LSTM model and predict on test data\n",
    "def train_evaluate_lstm(train_df, test_df, target_column='Power(MW)', lookback_steps=8, epochs=100, batch_size=64, site_name='Site', min_val=None, max_val=None):\n",
    "    # Create look-back features for training data\n",
    "    X_train, y_train = create_lookback_features(train_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape data for GRU: [samples, time steps, features]\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2])\n",
    "    \n",
    "    # Build the LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                   kernel_regularizer=regularizers.l2(0.001)))  # LSTM Layer with L2 regularization\n",
    "    model.add(BatchNormalization())  # BatchNormalization after first LSTM\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(LSTM(units=32, return_sequences=False, kernel_regularizer=regularizers.l2(0.001)))  # Second LSTM Layer\n",
    "    model.add(BatchNormalization())  # BatchNormalization after second LSTM\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    # Adding Dense layers with ReLU activation\n",
    "    model.add(Dense(24, activation='relu', kernel_regularizer=regularizers.l2(0.001)))  # Hidden Dense layer with ReLU\n",
    "    model.add(BatchNormalization())  # BatchNormalization after Dense layer\n",
    "    model.add(Dropout(0.2))  # Dropout layer after Dense layer\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))  # Output layer with Linear activation (default for regression)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1, shuffle=False)\n",
    "    \n",
    "    # Create look-back features for test data\n",
    "    X_test, y_test = create_lookback_features(test_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape test data for LSTM\n",
    "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2])\n",
    "    \n",
    "    # Predict on the test data\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Reverse normalization on both actual and predicted values\n",
    "    y_test_reversed = reverse_normalization(y_test, min_val, max_val)\n",
    "    y_pred_test_reversed = reverse_normalization(y_pred_test, min_val, max_val)\n",
    "    \n",
    "    # Calculate and print R squared and RMSE for the test set\n",
    "    calculate_metrics(y_test_reversed, y_pred_test_reversed, title=f'LSTM Model - {site_name} (Test Data)')\n",
    "\n",
    "    # Define the range of days you want to plot (90th to 100th days)\n",
    "    start_day = 90\n",
    "    end_day = 95\n",
    "    records_per_day = 48  # 24 hours * 2 records per hour (30-minute intervals)\n",
    "\n",
    "    # Calculate the indices corresponding to the 90th and 100th days\n",
    "    start_index = (start_day - 1) * records_per_day  # Start index for 90th day\n",
    "    end_index = end_day * records_per_day  # End index for 100th day\n",
    "\n",
    "    # Slice the test data and predictions for the range from the 90th to 100th day\n",
    "    y_test_range = y_test_reversed[start_index:end_index]\n",
    "    y_pred_test_range = y_pred_test_reversed[start_index:end_index]\n",
    "\n",
    "    # Plot the comparison between actual and predicted values for the 90th to 100th days\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(start_index, end_index), y_test_range, label='Actual', color='blue', linestyle='-', marker='o')\n",
    "    plt.plot(range(start_index, end_index), y_pred_test_range, label='Predicted', color='red', linestyle='--', marker='x')\n",
    "    plt.title(f'Actual vs Predicted Power (LSTM: LB=8, Epoch=128, Unit=64-32-24) - {site_name}')\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('Power (MW)')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming your split data frames are named as follows:\n",
    "# (train_resampled_df_50, test_resampled_df_50)\n",
    "\n",
    "# Define the min and max values used for normalization\n",
    "min_train_35 = normalized_train_df_35.min().min()  # Get the minimum value from the training data\n",
    "max_train_35 = normalized_train_df_35.max().max()  # Get the maximum value from the training data\n",
    "\n",
    "# List of split data frames and site names\n",
    "split_resampled_dfs = [\n",
    "    (normalized_train_df_35, normalized_test_df_35, 'Site_35MV')\n",
    "]\n",
    "\n",
    "# Apply the GRU model to each set of split data frames and calculate metrics\n",
    "for i, (train_df, test_df, site_name) in enumerate(split_resampled_dfs):\n",
    "    print(f\"Evaluating LSTM model for resampled dataset {site_name} ({i+1})\")\n",
    "    train_evaluate_lstm(train_df=train_df,\n",
    "                        test_df=test_df,\n",
    "                        target_column='Power(MW)',\n",
    "                        lookback_steps=8, epochs=128, batch_size=64,\n",
    "                        site_name=site_name,\n",
    "                        min_val=min_train_35,\n",
    "                        max_val=max_train_35)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3b3abf",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19e72b1",
   "metadata": {},
   "source": [
    "# Evaluating GRU on Validation set without Batch Normalization for site_50MW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce5ce2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout,GRU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "np.random.seed(42)\n",
    "# Function to create look-back features for time series forecasting\n",
    "def create_lookback_features(df, target_column, lookback_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - lookback_steps):\n",
    "        X.append(df.iloc[i:i + lookback_steps].values)\n",
    "        y.append(df.iloc[i + lookback_steps][target_column])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Function to reverse normalization\n",
    "def reverse_normalization(normalized_values, min_val, max_val):\n",
    "    return normalized_values * (max_val - min_val) + min_val\n",
    "\n",
    "# Function to calculate and print R squared and RMSE\n",
    "def calculate_metrics(y_true_train, y_pred_train, y_true_val, y_pred_val, title, best_epoch=None):\n",
    "    r2_train = r2_score(y_true_train, y_pred_train)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_true_train, y_pred_train))\n",
    "    \n",
    "    r2_val = r2_score(y_true_val, y_pred_val)\n",
    "    rmse_val = np.sqrt(mean_squared_error(y_true_val, y_pred_val))\n",
    "    \n",
    "    print(f\"{title} - Train R squared: {r2_train}, RMSE: {rmse_train}\")\n",
    "    print(f\"{title} - Validation R squared: {r2_val}, RMSE: {rmse_val}\")\n",
    "    \n",
    "    if best_epoch is not None:\n",
    "        print(f\"Best Epoch: {best_epoch}\")\n",
    "\n",
    "# Function to train and evaluate the LSTM model\n",
    "def train_evaluate_gru(train_df, val_df, target_column='Power(MW)', lookback_steps=8, epochs=100, batch_size=64,\n",
    "                       site_name='Site', min_train=None, max_train=None):\n",
    "    # Create look-back features\n",
    "    X_train, y_train = create_lookback_features(train_df, target_column, lookback_steps)\n",
    "    X_val, y_val = create_lookback_features(val_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape data for GRU: [samples, time steps, features]\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2])\n",
    "    X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], X_val.shape[2])\n",
    "    \n",
    "    # Build the GRU model\n",
    "    model = Sequential()\n",
    "    model.add(GRU(units=64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                   kernel_regularizer=regularizers.l2(0.001)))  # GRU Layer with L2 regularization\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(GRU(units=32, return_sequences=False, kernel_regularizer=regularizers.l2(0.001)))  # Second GRU Layer\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    # Adding Dense layers with ReLU activation\n",
    "    model.add(Dense(24, activation='relu', kernel_regularizer=regularizers.l2(0.001)))  # Hidden Dense layer with ReLU\n",
    "    model.add(Dropout(0.2))  # Dropout layer after Dense layer\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))  # Output layer with Linear activation (default for regression)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    # Initialize lists to store metrics for each epoch\n",
    "    r2_train_list = []\n",
    "    rmse_train_list = []\n",
    "    r2_val_list = []  # List to store R² for validation set\n",
    "    rmse_val_list = []  # List to store RMSE for validation set\n",
    "    \n",
    "    # Initialize variables to track the best epoch\n",
    "    best_epoch_r2 = -np.inf\n",
    "    best_epoch_rmse = np.inf\n",
    "    best_epoch_r2_index = None\n",
    "    best_epoch_rmse_index = None\n",
    "    \n",
    "    # Train the model and track history\n",
    "    for epoch in range(epochs):\n",
    "        # Train for one epoch\n",
    "        history = model.fit(X_train, y_train, epochs=1, batch_size=batch_size, verbose=0,shuffle=False)\n",
    "        \n",
    "        # Get predictions for train and validation data\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_val = model.predict(X_val)\n",
    "        \n",
    "        # Reverse normalization for both predictions and true values\n",
    "        y_pred_train = reverse_normalization(y_pred_train, min_train, max_train)\n",
    "        y_pred_val = reverse_normalization(y_pred_val, min_train, max_train)\n",
    "        y_train_orig = reverse_normalization(y_train, min_train, max_train)\n",
    "        y_val_orig = reverse_normalization(y_val, min_train, max_train)\n",
    "        \n",
    "        # Calculate R-squared and RMSE for the current epoch for both train and validation data\n",
    "        r2_train = r2_score(y_train_orig, y_pred_train)\n",
    "        rmse_train = np.sqrt(mean_squared_error(y_train_orig, y_pred_train))\n",
    "        \n",
    "        r2_val = r2_score(y_val_orig, y_pred_val)\n",
    "        rmse_val = np.sqrt(mean_squared_error(y_val_orig, y_pred_val))\n",
    "        \n",
    "        # Append metrics to lists\n",
    "        r2_train_list.append(r2_train)\n",
    "        rmse_train_list.append(rmse_train)\n",
    "        r2_val_list.append(r2_val)  # Append to validation list\n",
    "        rmse_val_list.append(rmse_val)  # Append to validation list\n",
    "        \n",
    "        # Track best epoch for R² (max)\n",
    "        if r2_val > best_epoch_r2:\n",
    "            best_epoch_r2 = r2_val\n",
    "            best_epoch_r2_index = epoch\n",
    "        \n",
    "        # Track best epoch for RMSE (min)\n",
    "        if rmse_val < best_epoch_rmse:\n",
    "            best_epoch_rmse = rmse_val\n",
    "            best_epoch_rmse_index = epoch\n",
    "    \n",
    "    # Predict on the training and validation sets after training\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    \n",
    "    # Reverse normalization for both predictions and true values\n",
    "    y_pred_train = reverse_normalization(y_pred_train, min_train, max_train)\n",
    "    y_pred_val = reverse_normalization(y_pred_val, min_train, max_train)\n",
    "    y_train_orig = reverse_normalization(y_train, min_train, max_train)\n",
    "    y_val_orig = reverse_normalization(y_val, min_train, max_train)\n",
    "    \n",
    "    # Calculate and print final R squared and RMSE for train and validation sets\n",
    "    calculate_metrics(y_train_orig, y_pred_train, y_val_orig, y_pred_val, title=f'GRU Model - {site_name}', best_epoch=best_epoch_r2_index)\n",
    "    \n",
    "    # Plot the loss, R squared, and RMSE metrics for train and validation sets\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(10, 12))\n",
    "    \n",
    "    # Plot R2 for train and validation sets\n",
    "    axs[0].plot(range(epochs), r2_train_list, label='Train R²', color='blue', linestyle='-', marker='o')\n",
    "    axs[0].plot(range(epochs), r2_val_list, label='Validation R²', color='green', linestyle='--', marker='s')\n",
    "    axs[0].set_title(f'R² Score per Epoch - {site_name}')\n",
    "    axs[0].set_xlabel('Epochs')\n",
    "    axs[0].set_ylabel('R²')\n",
    "    axs[0].legend()\n",
    "    \n",
    "    # Mark best epoch for R² and display R² value\n",
    "    axs[0].scatter(best_epoch_r2_index, best_epoch_r2, color='red', zorder=5, label=f'Best Epoch: {best_epoch_r2_index}')\n",
    "    axs[0].text(best_epoch_r2_index, best_epoch_r2, f' R²: {best_epoch_r2:.3f}', color='red', fontsize=10, verticalalignment='bottom')\n",
    "    \n",
    "    # Plot RMSE for train and validation sets\n",
    "    axs[1].plot(range(epochs), rmse_train_list, label='Train RMSE', color='red', linestyle='-', marker='x')\n",
    "    axs[1].plot(range(epochs), rmse_val_list, label='Validation RMSE', color='purple', linestyle='--', marker='v')\n",
    "    axs[1].set_title(f'RMSE per Epoch - {site_name}')\n",
    "    axs[1].set_xlabel('Epochs')\n",
    "    axs[1].set_ylabel('RMSE')\n",
    "    axs[1].legend()\n",
    "    \n",
    "    # Mark best epoch for RMSE and display RMSE value\n",
    "    axs[1].scatter(best_epoch_rmse_index, best_epoch_rmse, color='blue', zorder=5, label=f'Best Epoch: {best_epoch_rmse_index}')\n",
    "    axs[1].text(best_epoch_rmse_index, best_epoch_rmse, f' RMSE: {best_epoch_rmse:.3f}', color='blue', fontsize=10, verticalalignment='top')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming your split data frames are named as follows:\n",
    "# (train_resampled_df_50, val_resampled_df_50)\n",
    "# (train_resampled_df_130, val_resampled_df_130)\n",
    "# (train_resampled_df_35, val_resampled_df_35)\n",
    "# (train_resampled_df_30, val_resampled_df_30)\n",
    "\n",
    "# List of split data frames and site names\n",
    "split_resampled_dfs = [\n",
    "    (normalized_train_df_50, normalized_val_df_50, 'Site_50MV')\n",
    "]\n",
    "\n",
    "# Define the min and max values used for normalization\n",
    "min_train_50 = normalized_train_df_50.min().min()  # Get the minimum value from the training data\n",
    "max_train_50 = normalized_train_df_50.max().max()  # Get the maximum value from the training data\n",
    "\n",
    "# Apply the LSTM model to each set of split data frames and calculate metrics\n",
    "for i, (train_df, val_df, site_name) in enumerate(split_resampled_dfs):\n",
    "    print(f\"Evaluating GRU model for resampled dataset {site_name} ({i+1})\")\n",
    "    train_evaluate_gru(\n",
    "        train_df=train_df,\n",
    "        val_df=val_df,\n",
    "        target_column='Power(MW)',\n",
    "        lookback_steps=8,\n",
    "        epochs=128,\n",
    "        batch_size=64,\n",
    "        site_name=site_name,\n",
    "        min_train=min_train_50,  # Provide the min value for reversing normalization\n",
    "        max_train=max_train_50   # Provide the max value for reversing normalization\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68211c8",
   "metadata": {},
   "source": [
    "# Evaluating GRU on Validation set with Batch Normalization for site_50MW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fa6a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout,GRU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "np.random.seed(42)\n",
    "# Function to create look-back features for time series forecasting\n",
    "def create_lookback_features(df, target_column, lookback_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - lookback_steps):\n",
    "        X.append(df.iloc[i:i + lookback_steps].values)\n",
    "        y.append(df.iloc[i + lookback_steps][target_column])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Function to reverse normalization\n",
    "def reverse_normalization(normalized_values, min_val, max_val):\n",
    "    return normalized_values * (max_val - min_val) + min_val\n",
    "\n",
    "# Function to calculate and print R squared and RMSE\n",
    "def calculate_metrics(y_true_train, y_pred_train, y_true_val, y_pred_val, title, best_epoch=None):\n",
    "    r2_train = r2_score(y_true_train, y_pred_train)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_true_train, y_pred_train))\n",
    "    \n",
    "    r2_val = r2_score(y_true_val, y_pred_val)\n",
    "    rmse_val = np.sqrt(mean_squared_error(y_true_val, y_pred_val))\n",
    "    \n",
    "    print(f\"{title} - Train R squared: {r2_train}, RMSE: {rmse_train}\")\n",
    "    print(f\"{title} - Validation R squared: {r2_val}, RMSE: {rmse_val}\")\n",
    "    \n",
    "    if best_epoch is not None:\n",
    "        print(f\"Best Epoch: {best_epoch}\")\n",
    "\n",
    "# Function to train and evaluate the LSTM model\n",
    "def train_evaluate_gru(train_df, val_df, target_column='Power(MW)', lookback_steps=8, epochs=100, batch_size=64, site_name='Site', min_train=None, max_train=None):\n",
    "    # Create look-back features\n",
    "    X_train, y_train = create_lookback_features(train_df, target_column, lookback_steps)\n",
    "    X_val, y_val = create_lookback_features(val_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape data for GRU: [samples, time steps, features]\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2])\n",
    "    X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], X_val.shape[2])\n",
    "    \n",
    "    # Build the GRU model\n",
    "    model = Sequential()\n",
    "    model.add(GRU(units=64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                   kernel_regularizer=regularizers.l2(0.001)))  # GRU Layer with L2 regularization\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(GRU(units=32, return_sequences=False, kernel_regularizer=regularizers.l2(0.001)))  # Second GRU Layer\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    # Adding Dense layers with ReLU activation\n",
    "    model.add(Dense(24, activation='relu', kernel_regularizer=regularizers.l2(0.001)))  # Hidden Dense layer with ReLU\n",
    "    model.add(Dropout(0.2))  # Dropout layer after Dense layer\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))  # Output layer with Linear activation (default for regression)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    # Initialize lists to store metrics for each epoch\n",
    "    r2_train_list = []\n",
    "    rmse_train_list = []\n",
    "    r2_val_list = []  # List to store R² for validation set\n",
    "    rmse_val_list = []  # List to store RMSE for validation set\n",
    "    \n",
    "    # Initialize variables to track the best epoch\n",
    "    best_epoch_r2 = -np.inf\n",
    "    best_epoch_rmse = np.inf\n",
    "    best_epoch_r2_index = None\n",
    "    best_epoch_rmse_index = None\n",
    "    \n",
    "    # Train the model and track history\n",
    "    for epoch in range(epochs):\n",
    "        # Train for one epoch\n",
    "        history = model.fit(X_train, y_train, epochs=1, batch_size=batch_size, verbose=0,shuffle=False)\n",
    "        \n",
    "        # Get predictions for train and validation data\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_val = model.predict(X_val)\n",
    "        \n",
    "        # Reverse normalization for both predictions and true values\n",
    "        y_pred_train = reverse_normalization(y_pred_train, min_train, max_train)\n",
    "        y_pred_val = reverse_normalization(y_pred_val, min_train, max_train)\n",
    "        y_train_orig = reverse_normalization(y_train, min_train, max_train)\n",
    "        y_val_orig = reverse_normalization(y_val, min_train, max_train)\n",
    "        \n",
    "        # Calculate R-squared and RMSE for the current epoch for both train and validation data\n",
    "        r2_train = r2_score(y_train_orig, y_pred_train)\n",
    "        rmse_train = np.sqrt(mean_squared_error(y_train_orig, y_pred_train))\n",
    "        \n",
    "        r2_val = r2_score(y_val_orig, y_pred_val)\n",
    "        rmse_val = np.sqrt(mean_squared_error(y_val_orig, y_pred_val))\n",
    "        \n",
    "        # Append metrics to lists\n",
    "        r2_train_list.append(r2_train)\n",
    "        rmse_train_list.append(rmse_train)\n",
    "        r2_val_list.append(r2_val)  # Append to validation list\n",
    "        rmse_val_list.append(rmse_val)  # Append to validation list\n",
    "        \n",
    "        # Track best epoch for R² (max)\n",
    "        if r2_val > best_epoch_r2:\n",
    "            best_epoch_r2 = r2_val\n",
    "            best_epoch_r2_index = epoch\n",
    "        \n",
    "        # Track best epoch for RMSE (min)\n",
    "        if rmse_val < best_epoch_rmse:\n",
    "            best_epoch_rmse = rmse_val\n",
    "            best_epoch_rmse_index = epoch\n",
    "    \n",
    "    # Predict on the training and validation sets after training\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    \n",
    "    # Reverse normalization for both predictions and true values\n",
    "    y_pred_train = reverse_normalization(y_pred_train, min_train, max_train)\n",
    "    y_pred_val = reverse_normalization(y_pred_val, min_train, max_train)\n",
    "    y_train_orig = reverse_normalization(y_train, min_train, max_train)\n",
    "    y_val_orig = reverse_normalization(y_val, min_train, max_train)\n",
    "    \n",
    "    # Calculate and print final R squared and RMSE for train and validation sets\n",
    "    calculate_metrics(y_train_orig, y_pred_train, y_val_orig, y_pred_val, title=f'GRU Model - {site_name}', best_epoch=best_epoch_r2_index)\n",
    "    \n",
    "    # Plot the loss, R squared, and RMSE metrics for train and validation sets\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(10, 12))\n",
    "    \n",
    "    # Plot R2 for train and validation sets\n",
    "    axs[0].plot(range(epochs), r2_train_list, label='Train R²', color='blue', linestyle='-', marker='o')\n",
    "    axs[0].plot(range(epochs), r2_val_list, label='Validation R²', color='green', linestyle='--', marker='s')\n",
    "    axs[0].set_title(f'R² Score per Epoch - {site_name}')\n",
    "    axs[0].set_xlabel('Epochs')\n",
    "    axs[0].set_ylabel('R²')\n",
    "    axs[0].legend()\n",
    "    \n",
    "    # Mark best epoch for R² and display R² value\n",
    "    axs[0].scatter(best_epoch_r2_index, best_epoch_r2, color='red', zorder=5, label=f'Best Epoch: {best_epoch_r2_index}')\n",
    "    axs[0].text(best_epoch_r2_index, best_epoch_r2, f' R²: {best_epoch_r2:.3f}', color='red', fontsize=10, verticalalignment='bottom')\n",
    "    \n",
    "    # Plot RMSE for train and validation sets\n",
    "    axs[1].plot(range(epochs), rmse_train_list, label='Train RMSE', color='red', linestyle='-', marker='x')\n",
    "    axs[1].plot(range(epochs), rmse_val_list, label='Validation RMSE', color='purple', linestyle='--', marker='v')\n",
    "    axs[1].set_title(f'RMSE per Epoch - {site_name}')\n",
    "    axs[1].set_xlabel('Epochs')\n",
    "    axs[1].set_ylabel('RMSE')\n",
    "    axs[1].legend()\n",
    "    \n",
    "    # Mark best epoch for RMSE and display RMSE value\n",
    "    axs[1].scatter(best_epoch_rmse_index, best_epoch_rmse, color='blue', zorder=5, label=f'Best Epoch: {best_epoch_rmse_index}')\n",
    "    axs[1].text(best_epoch_rmse_index, best_epoch_rmse, f' RMSE: {best_epoch_rmse:.3f}', color='blue', fontsize=10, verticalalignment='top')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming your split data frames are named as follows:\n",
    "# (train_resampled_df_50, val_resampled_df_50)\n",
    "# (train_resampled_df_130, val_resampled_df_130)\n",
    "# (train_resampled_df_35, val_resampled_df_35)\n",
    "# (train_resampled_df_30, val_resampled_df_30)\n",
    "\n",
    "# List of split data frames and site names\n",
    "split_resampled_dfs = [\n",
    "    (normalized_train_df_50, normalized_val_df_50, 'Site_50MV')\n",
    "]\n",
    "\n",
    "# Define the min and max values used for normalization\n",
    "min_train_50 = normalized_train_df_50.min().min()  # Get the minimum value from the training data\n",
    "max_train_50 = normalized_train_df_50.max().max()  # Get the maximum value from the training data\n",
    "\n",
    "# Apply the LSTM model to each set of split data frames and calculate metrics\n",
    "for i, (train_df, val_df, site_name) in enumerate(split_resampled_dfs):\n",
    "    print(f\"Evaluating GRU model for resampled dataset {site_name} ({i+1})\")\n",
    "    train_evaluate_gru(\n",
    "        train_df=train_df,\n",
    "        val_df=val_df,\n",
    "        target_column='Power(MW)',\n",
    "        lookback_steps=8,\n",
    "        epochs=128,\n",
    "        batch_size=64,\n",
    "        site_name=site_name,\n",
    "        min_train=min_train_50,  # Provide the min value for reversing normalization\n",
    "        max_train=max_train_50   # Provide the max value for reversing normalization\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbb66cc",
   "metadata": {},
   "source": [
    "# Evaluating GRU on Validation for site_50MW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a88649",
   "metadata": {},
   "source": [
    "Consider different combination of number of units and epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2d5266",
   "metadata": {},
   "source": [
    "Number of Units: 64-32-24, Epochs=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31142474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout,GRU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "np.random.seed(42)\n",
    "# Function to create look-back features for time series forecasting\n",
    "def create_lookback_features(df, target_column, lookback_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - lookback_steps):\n",
    "        X.append(df.iloc[i:i + lookback_steps].values)\n",
    "        y.append(df.iloc[i + lookback_steps][target_column])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Function to reverse normalization\n",
    "def reverse_normalization(normalized_values, min_val, max_val):\n",
    "    return normalized_values * (max_val - min_val) + min_val\n",
    "\n",
    "# Function to calculate and print R squared and RMSE\n",
    "def calculate_metrics(y_true_train, y_pred_train, y_true_val, y_pred_val, title, best_epoch=None):\n",
    "    r2_train = r2_score(y_true_train, y_pred_train)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_true_train, y_pred_train))\n",
    "    \n",
    "    r2_val = r2_score(y_true_val, y_pred_val)\n",
    "    rmse_val = np.sqrt(mean_squared_error(y_true_val, y_pred_val))\n",
    "    \n",
    "    print(f\"{title} - Train R squared: {r2_train}, RMSE: {rmse_train}\")\n",
    "    print(f\"{title} - Validation R squared: {r2_val}, RMSE: {rmse_val}\")\n",
    "    \n",
    "    if best_epoch is not None:\n",
    "        print(f\"Best Epoch: {best_epoch}\")\n",
    "\n",
    "# Function to train and evaluate the LSTM model\n",
    "def train_evaluate_gru(train_df, val_df, target_column='Power(MW)', lookback_steps=8, epochs=100, batch_size=64, site_name='Site', min_train=None, max_train=None):\n",
    "    # Create look-back features\n",
    "    X_train, y_train = create_lookback_features(train_df, target_column, lookback_steps)\n",
    "    X_val, y_val = create_lookback_features(val_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape data for GRU: [samples, time steps, features]\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2])\n",
    "    X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], X_val.shape[2])\n",
    "    \n",
    "    # Build the GRU model\n",
    "    model = Sequential()\n",
    "    model.add(GRU(units=64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                   kernel_regularizer=regularizers.l2(0.001)))  # GRU Layer with L2 regularization\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(GRU(units=32, return_sequences=False, kernel_regularizer=regularizers.l2(0.001)))  # Second GRU Layer\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    # Adding Dense layers with ReLU activation\n",
    "    model.add(Dense(24, activation='relu', kernel_regularizer=regularizers.l2(0.001)))  # Hidden Dense layer with ReLU\n",
    "    model.add(Dropout(0.2))  # Dropout layer after Dense layer\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))  # Output layer with Linear activation (default for regression)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    # Initialize lists to store metrics for each epoch\n",
    "    r2_train_list = []\n",
    "    rmse_train_list = []\n",
    "    r2_val_list = []  # List to store R² for validation set\n",
    "    rmse_val_list = []  # List to store RMSE for validation set\n",
    "    \n",
    "    # Initialize variables to track the best epoch\n",
    "    best_epoch_r2 = -np.inf\n",
    "    best_epoch_rmse = np.inf\n",
    "    best_epoch_r2_index = None\n",
    "    best_epoch_rmse_index = None\n",
    "    \n",
    "    # Train the model and track history\n",
    "    for epoch in range(epochs):\n",
    "        # Train for one epoch\n",
    "        history = model.fit(X_train, y_train, epochs=1, batch_size=batch_size, verbose=0,shuffle=False)\n",
    "        \n",
    "        # Get predictions for train and validation data\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_val = model.predict(X_val)\n",
    "        \n",
    "        # Reverse normalization for both predictions and true values\n",
    "        y_pred_train = reverse_normalization(y_pred_train, min_train, max_train)\n",
    "        y_pred_val = reverse_normalization(y_pred_val, min_train, max_train)\n",
    "        y_train_orig = reverse_normalization(y_train, min_train, max_train)\n",
    "        y_val_orig = reverse_normalization(y_val, min_train, max_train)\n",
    "        \n",
    "        # Calculate R-squared and RMSE for the current epoch for both train and validation data\n",
    "        r2_train = r2_score(y_train_orig, y_pred_train)\n",
    "        rmse_train = np.sqrt(mean_squared_error(y_train_orig, y_pred_train))\n",
    "        \n",
    "        r2_val = r2_score(y_val_orig, y_pred_val)\n",
    "        rmse_val = np.sqrt(mean_squared_error(y_val_orig, y_pred_val))\n",
    "        \n",
    "        # Append metrics to lists\n",
    "        r2_train_list.append(r2_train)\n",
    "        rmse_train_list.append(rmse_train)\n",
    "        r2_val_list.append(r2_val)  # Append to validation list\n",
    "        rmse_val_list.append(rmse_val)  # Append to validation list\n",
    "        \n",
    "        # Track best epoch for R² (max)\n",
    "        if r2_val > best_epoch_r2:\n",
    "            best_epoch_r2 = r2_val\n",
    "            best_epoch_r2_index = epoch\n",
    "        \n",
    "        # Track best epoch for RMSE (min)\n",
    "        if rmse_val < best_epoch_rmse:\n",
    "            best_epoch_rmse = rmse_val\n",
    "            best_epoch_rmse_index = epoch\n",
    "    \n",
    "    # Predict on the training and validation sets after training\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    \n",
    "    # Reverse normalization for both predictions and true values\n",
    "    y_pred_train = reverse_normalization(y_pred_train, min_train, max_train)\n",
    "    y_pred_val = reverse_normalization(y_pred_val, min_train, max_train)\n",
    "    y_train_orig = reverse_normalization(y_train, min_train, max_train)\n",
    "    y_val_orig = reverse_normalization(y_val, min_train, max_train)\n",
    "    \n",
    "    # Calculate and print final R squared and RMSE for train and validation sets\n",
    "    calculate_metrics(y_train_orig, y_pred_train, y_val_orig, y_pred_val, title=f'GRU Model - {site_name}', best_epoch=best_epoch_r2_index)\n",
    "    \n",
    "    # Plot the loss, R squared, and RMSE metrics for train and validation sets\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(10, 12))\n",
    "    \n",
    "    # Plot R2 for train and validation sets\n",
    "    axs[0].plot(range(epochs), r2_train_list, label='Train R²', color='blue', linestyle='-', marker='o')\n",
    "    axs[0].plot(range(epochs), r2_val_list, label='Validation R²', color='green', linestyle='--', marker='s')\n",
    "    axs[0].set_title(f'R² Score per Epoch - {site_name}')\n",
    "    axs[0].set_xlabel('Epochs')\n",
    "    axs[0].set_ylabel('R²')\n",
    "    axs[0].legend()\n",
    "    \n",
    "    # Mark best epoch for R² and display R² value\n",
    "    axs[0].scatter(best_epoch_r2_index, best_epoch_r2, color='red', zorder=5, label=f'Best Epoch: {best_epoch_r2_index}')\n",
    "    axs[0].text(best_epoch_r2_index, best_epoch_r2, f' R²: {best_epoch_r2:.3f}', color='red', fontsize=10, verticalalignment='bottom')\n",
    "    \n",
    "    # Plot RMSE for train and validation sets\n",
    "    axs[1].plot(range(epochs), rmse_train_list, label='Train RMSE', color='red', linestyle='-', marker='x')\n",
    "    axs[1].plot(range(epochs), rmse_val_list, label='Validation RMSE', color='purple', linestyle='--', marker='v')\n",
    "    axs[1].set_title(f'RMSE per Epoch - {site_name}')\n",
    "    axs[1].set_xlabel('Epochs')\n",
    "    axs[1].set_ylabel('RMSE')\n",
    "    axs[1].legend()\n",
    "    \n",
    "    # Mark best epoch for RMSE and display RMSE value\n",
    "    axs[1].scatter(best_epoch_rmse_index, best_epoch_rmse, color='blue', zorder=5, label=f'Best Epoch: {best_epoch_rmse_index}')\n",
    "    axs[1].text(best_epoch_rmse_index, best_epoch_rmse, f' RMSE: {best_epoch_rmse:.3f}', color='blue', fontsize=10, verticalalignment='top')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming your split data frames are named as follows:\n",
    "# (train_resampled_df_50, val_resampled_df_50)\n",
    "# (train_resampled_df_130, val_resampled_df_130)\n",
    "# (train_resampled_df_35, val_resampled_df_35)\n",
    "# (train_resampled_df_30, val_resampled_df_30)\n",
    "\n",
    "# List of split data frames and site names\n",
    "split_resampled_dfs = [\n",
    "    (normalized_train_df_50, normalized_val_df_50, 'Site_50MV')\n",
    "]\n",
    "\n",
    "# Define the min and max values used for normalization\n",
    "min_train_50 = normalized_train_df_50.min().min()  # Get the minimum value from the training data\n",
    "max_train_50 = normalized_train_df_50.max().max()  # Get the maximum value from the training data\n",
    "\n",
    "# Apply the LSTM model to each set of split data frames and calculate metrics\n",
    "for i, (train_df, val_df, site_name) in enumerate(split_resampled_dfs):\n",
    "    print(f\"Evaluating GRU model for resampled dataset {site_name} ({i+1})\")\n",
    "    train_evaluate_gru(\n",
    "        train_df=train_df,\n",
    "        val_df=val_df,\n",
    "        target_column='Power(MW)',\n",
    "        lookback_steps=8,\n",
    "        epochs=128,\n",
    "        batch_size=64,\n",
    "        site_name=site_name,\n",
    "        min_train=min_train_50,  # Provide the min value for reversing normalization\n",
    "        max_train=max_train_50   # Provide the max value for reversing normalization\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e301c516",
   "metadata": {},
   "source": [
    "Number of Units: 64-32-24, Epochs=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5466757c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout,GRU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "np.random.seed(42)\n",
    "# Function to create look-back features for time series forecasting\n",
    "def create_lookback_features(df, target_column, lookback_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - lookback_steps):\n",
    "        X.append(df.iloc[i:i + lookback_steps].values)\n",
    "        y.append(df.iloc[i + lookback_steps][target_column])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Function to reverse normalization\n",
    "def reverse_normalization(normalized_values, min_val, max_val):\n",
    "    return normalized_values * (max_val - min_val) + min_val\n",
    "\n",
    "# Function to calculate and print R squared and RMSE\n",
    "def calculate_metrics(y_true_train, y_pred_train, y_true_val, y_pred_val, title, best_epoch=None):\n",
    "    r2_train = r2_score(y_true_train, y_pred_train)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_true_train, y_pred_train))\n",
    "    \n",
    "    r2_val = r2_score(y_true_val, y_pred_val)\n",
    "    rmse_val = np.sqrt(mean_squared_error(y_true_val, y_pred_val))\n",
    "    \n",
    "    print(f\"{title} - Train R squared: {r2_train}, RMSE: {rmse_train}\")\n",
    "    print(f\"{title} - Validation R squared: {r2_val}, RMSE: {rmse_val}\")\n",
    "    \n",
    "    if best_epoch is not None:\n",
    "        print(f\"Best Epoch: {best_epoch}\")\n",
    "\n",
    "# Function to train and evaluate the LSTM model\n",
    "def train_evaluate_gru(train_df, val_df, target_column='Power(MW)', lookback_steps=8, epochs=100, batch_size=64, site_name='Site', min_train=None, max_train=None):\n",
    "    # Create look-back features\n",
    "    X_train, y_train = create_lookback_features(train_df, target_column, lookback_steps)\n",
    "    X_val, y_val = create_lookback_features(val_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape data for GRU: [samples, time steps, features]\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2])\n",
    "    X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], X_val.shape[2])\n",
    "    \n",
    "    # Build the GRU model\n",
    "    model = Sequential()\n",
    "    model.add(GRU(units=64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                   kernel_regularizer=regularizers.l2(0.001)))  # GRU Layer with L2 regularization\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(GRU(units=32, return_sequences=False, kernel_regularizer=regularizers.l2(0.001)))  # Second GRU Layer\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    # Adding Dense layers with ReLU activation\n",
    "    model.add(Dense(24, activation='relu', kernel_regularizer=regularizers.l2(0.001)))  # Hidden Dense layer with ReLU\n",
    "    model.add(Dropout(0.2))  # Dropout layer after Dense layer\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))  # Output layer with Linear activation (default for regression)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    # Initialize lists to store metrics for each epoch\n",
    "    r2_train_list = []\n",
    "    rmse_train_list = []\n",
    "    r2_val_list = []  # List to store R² for validation set\n",
    "    rmse_val_list = []  # List to store RMSE for validation set\n",
    "    \n",
    "    # Initialize variables to track the best epoch\n",
    "    best_epoch_r2 = -np.inf\n",
    "    best_epoch_rmse = np.inf\n",
    "    best_epoch_r2_index = None\n",
    "    best_epoch_rmse_index = None\n",
    "    \n",
    "    # Train the model and track history\n",
    "    for epoch in range(epochs):\n",
    "        # Train for one epoch\n",
    "        history = model.fit(X_train, y_train, epochs=1, batch_size=batch_size, verbose=0,shuffle=False)\n",
    "        \n",
    "        # Get predictions for train and validation data\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_val = model.predict(X_val)\n",
    "        \n",
    "        # Reverse normalization for both predictions and true values\n",
    "        y_pred_train = reverse_normalization(y_pred_train, min_train, max_train)\n",
    "        y_pred_val = reverse_normalization(y_pred_val, min_train, max_train)\n",
    "        y_train_orig = reverse_normalization(y_train, min_train, max_train)\n",
    "        y_val_orig = reverse_normalization(y_val, min_train, max_train)\n",
    "        \n",
    "        # Calculate R-squared and RMSE for the current epoch for both train and validation data\n",
    "        r2_train = r2_score(y_train_orig, y_pred_train)\n",
    "        rmse_train = np.sqrt(mean_squared_error(y_train_orig, y_pred_train))\n",
    "        \n",
    "        r2_val = r2_score(y_val_orig, y_pred_val)\n",
    "        rmse_val = np.sqrt(mean_squared_error(y_val_orig, y_pred_val))\n",
    "        \n",
    "        # Append metrics to lists\n",
    "        r2_train_list.append(r2_train)\n",
    "        rmse_train_list.append(rmse_train)\n",
    "        r2_val_list.append(r2_val)  # Append to validation list\n",
    "        rmse_val_list.append(rmse_val)  # Append to validation list\n",
    "        \n",
    "        # Track best epoch for R² (max)\n",
    "        if r2_val > best_epoch_r2:\n",
    "            best_epoch_r2 = r2_val\n",
    "            best_epoch_r2_index = epoch\n",
    "        \n",
    "        # Track best epoch for RMSE (min)\n",
    "        if rmse_val < best_epoch_rmse:\n",
    "            best_epoch_rmse = rmse_val\n",
    "            best_epoch_rmse_index = epoch\n",
    "    \n",
    "    # Predict on the training and validation sets after training\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    \n",
    "    # Reverse normalization for both predictions and true values\n",
    "    y_pred_train = reverse_normalization(y_pred_train, min_train, max_train)\n",
    "    y_pred_val = reverse_normalization(y_pred_val, min_train, max_train)\n",
    "    y_train_orig = reverse_normalization(y_train, min_train, max_train)\n",
    "    y_val_orig = reverse_normalization(y_val, min_train, max_train)\n",
    "    \n",
    "    # Calculate and print final R squared and RMSE for train and validation sets\n",
    "    calculate_metrics(y_train_orig, y_pred_train, y_val_orig, y_pred_val, title=f'GRU Model - {site_name}', best_epoch=best_epoch_r2_index)\n",
    "    \n",
    "    # Plot the loss, R squared, and RMSE metrics for train and validation sets\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(10, 12))\n",
    "    \n",
    "    # Plot R2 for train and validation sets\n",
    "    axs[0].plot(range(epochs), r2_train_list, label='Train R²', color='blue', linestyle='-', marker='o')\n",
    "    axs[0].plot(range(epochs), r2_val_list, label='Validation R²', color='green', linestyle='--', marker='s')\n",
    "    axs[0].set_title(f'R² Score per Epoch - {site_name}')\n",
    "    axs[0].set_xlabel('Epochs')\n",
    "    axs[0].set_ylabel('R²')\n",
    "    axs[0].legend()\n",
    "    \n",
    "    # Mark best epoch for R² and display R² value\n",
    "    axs[0].scatter(best_epoch_r2_index, best_epoch_r2, color='red', zorder=5, label=f'Best Epoch: {best_epoch_r2_index}')\n",
    "    axs[0].text(best_epoch_r2_index, best_epoch_r2, f' R²: {best_epoch_r2:.3f}', color='red', fontsize=10, verticalalignment='bottom')\n",
    "    \n",
    "    # Plot RMSE for train and validation sets\n",
    "    axs[1].plot(range(epochs), rmse_train_list, label='Train RMSE', color='red', linestyle='-', marker='x')\n",
    "    axs[1].plot(range(epochs), rmse_val_list, label='Validation RMSE', color='purple', linestyle='--', marker='v')\n",
    "    axs[1].set_title(f'RMSE per Epoch - {site_name}')\n",
    "    axs[1].set_xlabel('Epochs')\n",
    "    axs[1].set_ylabel('RMSE')\n",
    "    axs[1].legend()\n",
    "    \n",
    "    # Mark best epoch for RMSE and display RMSE value\n",
    "    axs[1].scatter(best_epoch_rmse_index, best_epoch_rmse, color='blue', zorder=5, label=f'Best Epoch: {best_epoch_rmse_index}')\n",
    "    axs[1].text(best_epoch_rmse_index, best_epoch_rmse, f' RMSE: {best_epoch_rmse:.3f}', color='blue', fontsize=10, verticalalignment='top')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming your split data frames are named as follows:\n",
    "# (train_resampled_df_50, val_resampled_df_50)\n",
    "# (train_resampled_df_130, val_resampled_df_130)\n",
    "# (train_resampled_df_35, val_resampled_df_35)\n",
    "# (train_resampled_df_30, val_resampled_df_30)\n",
    "\n",
    "# List of split data frames and site names\n",
    "split_resampled_dfs = [\n",
    "    (normalized_train_df_50, normalized_val_df_50, 'Site_50MV')\n",
    "]\n",
    "\n",
    "# Define the min and max values used for normalization\n",
    "min_train_50 = normalized_train_df_50.min().min()  # Get the minimum value from the training data\n",
    "max_train_50 = normalized_train_df_50.max().max()  # Get the maximum value from the training data\n",
    "\n",
    "# Apply the LSTM model to each set of split data frames and calculate metrics\n",
    "for i, (train_df, val_df, site_name) in enumerate(split_resampled_dfs):\n",
    "    print(f\"Evaluating GRU model for resampled dataset {site_name} ({i+1})\")\n",
    "    train_evaluate_gru(\n",
    "        train_df=train_df,\n",
    "        val_df=val_df,\n",
    "        target_column='Power(MW)',\n",
    "        lookback_steps=8,\n",
    "        epochs=64,\n",
    "        batch_size=64,\n",
    "        site_name=site_name,\n",
    "        min_train=min_train_50,  # Provide the min value for reversing normalization\n",
    "        max_train=max_train_50   # Provide the max value for reversing normalization\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ffe1ac",
   "metadata": {},
   "source": [
    "Number of Units: 32-16-20, Epochs=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d683b14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout,GRU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "np.random.seed(42)\n",
    "# Function to create look-back features for time series forecasting\n",
    "def create_lookback_features(df, target_column, lookback_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - lookback_steps):\n",
    "        X.append(df.iloc[i:i + lookback_steps].values)\n",
    "        y.append(df.iloc[i + lookback_steps][target_column])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Function to reverse normalization\n",
    "def reverse_normalization(normalized_values, min_val, max_val):\n",
    "    return normalized_values * (max_val - min_val) + min_val\n",
    "\n",
    "# Function to calculate and print R squared and RMSE\n",
    "def calculate_metrics(y_true_train, y_pred_train, y_true_val, y_pred_val, title, best_epoch=None):\n",
    "    r2_train = r2_score(y_true_train, y_pred_train)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_true_train, y_pred_train))\n",
    "    \n",
    "    r2_val = r2_score(y_true_val, y_pred_val)\n",
    "    rmse_val = np.sqrt(mean_squared_error(y_true_val, y_pred_val))\n",
    "    \n",
    "    print(f\"{title} - Train R squared: {r2_train}, RMSE: {rmse_train}\")\n",
    "    print(f\"{title} - Validation R squared: {r2_val}, RMSE: {rmse_val}\")\n",
    "    \n",
    "    if best_epoch is not None:\n",
    "        print(f\"Best Epoch: {best_epoch}\")\n",
    "\n",
    "# Function to train and evaluate the LSTM model\n",
    "def train_evaluate_gru(train_df, val_df, target_column='Power(MW)', lookback_steps=8, epochs=100, batch_size=64, site_name='Site', min_train=None, max_train=None):\n",
    "    # Create look-back features\n",
    "    X_train, y_train = create_lookback_features(train_df, target_column, lookback_steps)\n",
    "    X_val, y_val = create_lookback_features(val_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape data for GRU: [samples, time steps, features]\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2])\n",
    "    X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], X_val.shape[2])\n",
    "    \n",
    "    # Build the GRU model\n",
    "    model = Sequential()\n",
    "    model.add(GRU(units=32, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                   kernel_regularizer=regularizers.l2(0.001)))  # GRU Layer with L2 regularization\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(GRU(units=16, return_sequences=False, kernel_regularizer=regularizers.l2(0.001)))  # Second GRU Layer\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    # Adding Dense layers with ReLU activation\n",
    "    model.add(Dense(20, activation='relu', kernel_regularizer=regularizers.l2(0.001)))  # Hidden Dense layer with ReLU\n",
    "    model.add(Dropout(0.2))  # Dropout layer after Dense layer\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))  # Output layer with Linear activation (default for regression)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    # Initialize lists to store metrics for each epoch\n",
    "    r2_train_list = []\n",
    "    rmse_train_list = []\n",
    "    r2_val_list = []  # List to store R² for validation set\n",
    "    rmse_val_list = []  # List to store RMSE for validation set\n",
    "    \n",
    "    # Initialize variables to track the best epoch\n",
    "    best_epoch_r2 = -np.inf\n",
    "    best_epoch_rmse = np.inf\n",
    "    best_epoch_r2_index = None\n",
    "    best_epoch_rmse_index = None\n",
    "    \n",
    "    # Train the model and track history\n",
    "    for epoch in range(epochs):\n",
    "        # Train for one epoch\n",
    "        history = model.fit(X_train, y_train, epochs=1, batch_size=batch_size, verbose=0,shuffle=False)\n",
    "        \n",
    "        # Get predictions for train and validation data\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_val = model.predict(X_val)\n",
    "        \n",
    "        # Reverse normalization for both predictions and true values\n",
    "        y_pred_train = reverse_normalization(y_pred_train, min_train, max_train)\n",
    "        y_pred_val = reverse_normalization(y_pred_val, min_train, max_train)\n",
    "        y_train_orig = reverse_normalization(y_train, min_train, max_train)\n",
    "        y_val_orig = reverse_normalization(y_val, min_train, max_train)\n",
    "        \n",
    "        # Calculate R-squared and RMSE for the current epoch for both train and validation data\n",
    "        r2_train = r2_score(y_train_orig, y_pred_train)\n",
    "        rmse_train = np.sqrt(mean_squared_error(y_train_orig, y_pred_train))\n",
    "        \n",
    "        r2_val = r2_score(y_val_orig, y_pred_val)\n",
    "        rmse_val = np.sqrt(mean_squared_error(y_val_orig, y_pred_val))\n",
    "        \n",
    "        # Append metrics to lists\n",
    "        r2_train_list.append(r2_train)\n",
    "        rmse_train_list.append(rmse_train)\n",
    "        r2_val_list.append(r2_val)  # Append to validation list\n",
    "        rmse_val_list.append(rmse_val)  # Append to validation list\n",
    "        \n",
    "        # Track best epoch for R² (max)\n",
    "        if r2_val > best_epoch_r2:\n",
    "            best_epoch_r2 = r2_val\n",
    "            best_epoch_r2_index = epoch\n",
    "        \n",
    "        # Track best epoch for RMSE (min)\n",
    "        if rmse_val < best_epoch_rmse:\n",
    "            best_epoch_rmse = rmse_val\n",
    "            best_epoch_rmse_index = epoch\n",
    "    \n",
    "    # Predict on the training and validation sets after training\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    \n",
    "    # Reverse normalization for both predictions and true values\n",
    "    y_pred_train = reverse_normalization(y_pred_train, min_train, max_train)\n",
    "    y_pred_val = reverse_normalization(y_pred_val, min_train, max_train)\n",
    "    y_train_orig = reverse_normalization(y_train, min_train, max_train)\n",
    "    y_val_orig = reverse_normalization(y_val, min_train, max_train)\n",
    "    \n",
    "    # Calculate and print final R squared and RMSE for train and validation sets\n",
    "    calculate_metrics(y_train_orig, y_pred_train, y_val_orig, y_pred_val, title=f'GRU Model - {site_name}', best_epoch=best_epoch_r2_index)\n",
    "    \n",
    "    # Plot the loss, R squared, and RMSE metrics for train and validation sets\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(10, 12))\n",
    "    \n",
    "    # Plot R2 for train and validation sets\n",
    "    axs[0].plot(range(epochs), r2_train_list, label='Train R²', color='blue', linestyle='-', marker='o')\n",
    "    axs[0].plot(range(epochs), r2_val_list, label='Validation R²', color='green', linestyle='--', marker='s')\n",
    "    axs[0].set_title(f'R² Score per Epoch - {site_name}')\n",
    "    axs[0].set_xlabel('Epochs')\n",
    "    axs[0].set_ylabel('R²')\n",
    "    axs[0].legend()\n",
    "    \n",
    "    # Mark best epoch for R² and display R² value\n",
    "    axs[0].scatter(best_epoch_r2_index, best_epoch_r2, color='red', zorder=5, label=f'Best Epoch: {best_epoch_r2_index}')\n",
    "    axs[0].text(best_epoch_r2_index, best_epoch_r2, f' R²: {best_epoch_r2:.3f}', color='red', fontsize=10, verticalalignment='bottom')\n",
    "    \n",
    "    # Plot RMSE for train and validation sets\n",
    "    axs[1].plot(range(epochs), rmse_train_list, label='Train RMSE', color='red', linestyle='-', marker='x')\n",
    "    axs[1].plot(range(epochs), rmse_val_list, label='Validation RMSE', color='purple', linestyle='--', marker='v')\n",
    "    axs[1].set_title(f'RMSE per Epoch - {site_name}')\n",
    "    axs[1].set_xlabel('Epochs')\n",
    "    axs[1].set_ylabel('RMSE')\n",
    "    axs[1].legend()\n",
    "    \n",
    "    # Mark best epoch for RMSE and display RMSE value\n",
    "    axs[1].scatter(best_epoch_rmse_index, best_epoch_rmse, color='blue', zorder=5, label=f'Best Epoch: {best_epoch_rmse_index}')\n",
    "    axs[1].text(best_epoch_rmse_index, best_epoch_rmse, f' RMSE: {best_epoch_rmse:.3f}', color='blue', fontsize=10, verticalalignment='top')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming your split data frames are named as follows:\n",
    "# (train_resampled_df_50, val_resampled_df_50)\n",
    "# (train_resampled_df_130, val_resampled_df_130)\n",
    "# (train_resampled_df_35, val_resampled_df_35)\n",
    "# (train_resampled_df_30, val_resampled_df_30)\n",
    "\n",
    "# List of split data frames and site names\n",
    "split_resampled_dfs = [\n",
    "    (normalized_train_df_50, normalized_val_df_50, 'Site_50MV')\n",
    "]\n",
    "\n",
    "# Define the min and max values used for normalization\n",
    "min_train_50 = normalized_train_df_50.min().min()  # Get the minimum value from the training data\n",
    "max_train_50 = normalized_train_df_50.max().max()  # Get the maximum value from the training data\n",
    "\n",
    "# Apply the LSTM model to each set of split data frames and calculate metrics\n",
    "for i, (train_df, val_df, site_name) in enumerate(split_resampled_dfs):\n",
    "    print(f\"Evaluating GRU model for resampled dataset {site_name} ({i+1})\")\n",
    "    train_evaluate_gru(\n",
    "        train_df=train_df,\n",
    "        val_df=val_df,\n",
    "        target_column='Power(MW)',\n",
    "        lookback_steps=8,\n",
    "        epochs=128,\n",
    "        batch_size=64,\n",
    "        site_name=site_name,\n",
    "        min_train=min_train_50,  # Provide the min value for reversing normalization\n",
    "        max_train=max_train_50   # Provide the max value for reversing normalization\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a389e143",
   "metadata": {},
   "source": [
    "Number of Units: 32-16-20, Epochs=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6736aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout,GRU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "# Set the random seeds for reproducibility\n",
    "np.random.seed(42)  # For numpy\n",
    "\n",
    "# Function to create look-back features for time series forecasting\n",
    "def create_lookback_features(df, target_column, lookback_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - lookback_steps):\n",
    "        X.append(df.iloc[i:i + lookback_steps].values)\n",
    "        y.append(df.iloc[i + lookback_steps][target_column])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Function to reverse normalization\n",
    "def reverse_normalization(normalized_values, min_val, max_val):\n",
    "    return normalized_values * (max_val - min_val) + min_val\n",
    "\n",
    "# Function to calculate and print R squared and RMSE\n",
    "def calculate_metrics(y_true_train, y_pred_train, y_true_val, y_pred_val, title, best_epoch=None):\n",
    "    r2_train = r2_score(y_true_train, y_pred_train)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_true_train, y_pred_train))\n",
    "    \n",
    "    r2_val = r2_score(y_true_val, y_pred_val)\n",
    "    rmse_val = np.sqrt(mean_squared_error(y_true_val, y_pred_val))\n",
    "    \n",
    "    print(f\"{title} - Train R squared: {r2_train}, RMSE: {rmse_train}\")\n",
    "    print(f\"{title} - Validation R squared: {r2_val}, RMSE: {rmse_val}\")\n",
    "    \n",
    "    if best_epoch is not None:\n",
    "        print(f\"Best Epoch: {best_epoch}\")\n",
    "\n",
    "# Function to train and evaluate the LSTM model\n",
    "def train_evaluate_gru(train_df, val_df, target_column='Power(MW)', lookback_steps=8, epochs=100, batch_size=64, site_name='Site', min_train=None, max_train=None):\n",
    "    # Create look-back features\n",
    "    X_train, y_train = create_lookback_features(train_df, target_column, lookback_steps)\n",
    "    X_val, y_val = create_lookback_features(val_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape data for GRU: [samples, time steps, features]\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2])\n",
    "    X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], X_val.shape[2])\n",
    "    \n",
    "    # Build the GRU model\n",
    "    model = Sequential()\n",
    "    model.add(GRU(units=32, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                   kernel_regularizer=regularizers.l2(0.001)))  # GRU Layer with L2 regularization\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(GRU(units=16, return_sequences=False, kernel_regularizer=regularizers.l2(0.001)))  # Second GRU Layer\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    # Adding Dense layers with ReLU activation\n",
    "    model.add(Dense(20, activation='relu', kernel_regularizer=regularizers.l2(0.001)))  # Hidden Dense layer with ReLU\n",
    "    model.add(Dropout(0.2))  # Dropout layer after Dense layer\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))  # Output layer with Linear activation (default for regression)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    # Initialize lists to store metrics for each epoch\n",
    "    r2_train_list = []\n",
    "    rmse_train_list = []\n",
    "    r2_val_list = []  # List to store R² for validation set\n",
    "    rmse_val_list = []  # List to store RMSE for validation set\n",
    "    \n",
    "    # Initialize variables to track the best epoch\n",
    "    best_epoch_r2 = -np.inf\n",
    "    best_epoch_rmse = np.inf\n",
    "    best_epoch_r2_index = None\n",
    "    best_epoch_rmse_index = None\n",
    "    \n",
    "    # Train the model and track history\n",
    "    for epoch in range(epochs):\n",
    "        # Train for one epoch\n",
    "        history = model.fit(X_train, y_train, epochs=1, batch_size=batch_size, verbose=0,shuffle=False)\n",
    "        \n",
    "        # Get predictions for train and validation data\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_val = model.predict(X_val)\n",
    "        \n",
    "        # Reverse normalization for both predictions and true values\n",
    "        y_pred_train = reverse_normalization(y_pred_train, min_train, max_train)\n",
    "        y_pred_val = reverse_normalization(y_pred_val, min_train, max_train)\n",
    "        y_train_orig = reverse_normalization(y_train, min_train, max_train)\n",
    "        y_val_orig = reverse_normalization(y_val, min_train, max_train)\n",
    "        \n",
    "        # Calculate R-squared and RMSE for the current epoch for both train and validation data\n",
    "        r2_train = r2_score(y_train_orig, y_pred_train)\n",
    "        rmse_train = np.sqrt(mean_squared_error(y_train_orig, y_pred_train))\n",
    "        \n",
    "        r2_val = r2_score(y_val_orig, y_pred_val)\n",
    "        rmse_val = np.sqrt(mean_squared_error(y_val_orig, y_pred_val))\n",
    "        \n",
    "        # Append metrics to lists\n",
    "        r2_train_list.append(r2_train)\n",
    "        rmse_train_list.append(rmse_train)\n",
    "        r2_val_list.append(r2_val)  # Append to validation list\n",
    "        rmse_val_list.append(rmse_val)  # Append to validation list\n",
    "        \n",
    "        # Track best epoch for R² (max)\n",
    "        if r2_val > best_epoch_r2:\n",
    "            best_epoch_r2 = r2_val\n",
    "            best_epoch_r2_index = epoch\n",
    "        \n",
    "        # Track best epoch for RMSE (min)\n",
    "        if rmse_val < best_epoch_rmse:\n",
    "            best_epoch_rmse = rmse_val\n",
    "            best_epoch_rmse_index = epoch\n",
    "    \n",
    "    # Predict on the training and validation sets after training\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    \n",
    "    # Reverse normalization for both predictions and true values\n",
    "    y_pred_train = reverse_normalization(y_pred_train, min_train, max_train)\n",
    "    y_pred_val = reverse_normalization(y_pred_val, min_train, max_train)\n",
    "    y_train_orig = reverse_normalization(y_train, min_train, max_train)\n",
    "    y_val_orig = reverse_normalization(y_val, min_train, max_train)\n",
    "    \n",
    "    # Calculate and print final R squared and RMSE for train and validation sets\n",
    "    calculate_metrics(y_train_orig, y_pred_train, y_val_orig, y_pred_val, title=f'GRU Model - {site_name}', best_epoch=best_epoch_r2_index)\n",
    "    \n",
    "    # Plot the loss, R squared, and RMSE metrics for train and validation sets\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(10, 12))\n",
    "    \n",
    "    # Plot R2 for train and validation sets\n",
    "    axs[0].plot(range(epochs), r2_train_list, label='Train R²', color='blue', linestyle='-', marker='o')\n",
    "    axs[0].plot(range(epochs), r2_val_list, label='Validation R²', color='green', linestyle='--', marker='s')\n",
    "    axs[0].set_title(f'R² Score per Epoch - {site_name}')\n",
    "    axs[0].set_xlabel('Epochs')\n",
    "    axs[0].set_ylabel('R²')\n",
    "    axs[0].legend()\n",
    "    \n",
    "    # Mark best epoch for R² and display R² value\n",
    "    axs[0].scatter(best_epoch_r2_index, best_epoch_r2, color='red', zorder=5, label=f'Best Epoch: {best_epoch_r2_index}')\n",
    "    axs[0].text(best_epoch_r2_index, best_epoch_r2, f' R²: {best_epoch_r2:.3f}', color='red', fontsize=10, verticalalignment='bottom')\n",
    "    \n",
    "    # Plot RMSE for train and validation sets\n",
    "    axs[1].plot(range(epochs), rmse_train_list, label='Train RMSE', color='red', linestyle='-', marker='x')\n",
    "    axs[1].plot(range(epochs), rmse_val_list, label='Validation RMSE', color='purple', linestyle='--', marker='v')\n",
    "    axs[1].set_title(f'RMSE per Epoch - {site_name}')\n",
    "    axs[1].set_xlabel('Epochs')\n",
    "    axs[1].set_ylabel('RMSE')\n",
    "    axs[1].legend()\n",
    "    \n",
    "    # Mark best epoch for RMSE and display RMSE value\n",
    "    axs[1].scatter(best_epoch_rmse_index, best_epoch_rmse, color='blue', zorder=5, label=f'Best Epoch: {best_epoch_rmse_index}')\n",
    "    axs[1].text(best_epoch_rmse_index, best_epoch_rmse, f' RMSE: {best_epoch_rmse:.3f}', color='blue', fontsize=10, verticalalignment='top')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming your split data frames are named as follows:\n",
    "# (train_resampled_df_50, val_resampled_df_50)\n",
    "# (train_resampled_df_130, val_resampled_df_130)\n",
    "# (train_resampled_df_35, val_resampled_df_35)\n",
    "# (train_resampled_df_30, val_resampled_df_30)\n",
    "\n",
    "# List of split data frames and site names\n",
    "split_resampled_dfs = [\n",
    "    (normalized_train_df_50, normalized_val_df_50, 'Site_50MV')\n",
    "]\n",
    "\n",
    "# Define the min and max values used for normalization\n",
    "min_train_50 = normalized_train_df_50.min().min()  # Get the minimum value from the training data\n",
    "max_train_50 = normalized_train_df_50.max().max()  # Get the maximum value from the training data\n",
    "\n",
    "# Apply the LSTM model to each set of split data frames and calculate metrics\n",
    "for i, (train_df, val_df, site_name) in enumerate(split_resampled_dfs):\n",
    "    print(f\"Evaluating GRU model for resampled dataset {site_name} ({i+1})\")\n",
    "    train_evaluate_gru(\n",
    "        train_df=train_df,\n",
    "        val_df=val_df,\n",
    "        target_column='Power(MW)',\n",
    "        lookback_steps=8,\n",
    "        epochs=64,\n",
    "        batch_size=64,\n",
    "        site_name=site_name,\n",
    "        min_train=min_train_50,  # Provide the min value for reversing normalization\n",
    "        max_train=max_train_50   # Provide the max value for reversing normalization\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdcad0a",
   "metadata": {},
   "source": [
    "# Build GRU model with best combination of Hyperparameters, then forcast On Test Data for all sites and plot actual vs predicted# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8bc97f",
   "metadata": {},
   "source": [
    "# Site_50MW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea461702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "# Function to create look-back features for time series forecasting\n",
    "def create_lookback_features(df, target_column, lookback_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - lookback_steps):\n",
    "        X.append(df.iloc[i:i + lookback_steps].values)\n",
    "        y.append(df.iloc[i + lookback_steps][target_column])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Function to calculate and print R squared and RMSE\n",
    "def calculate_metrics(y_true, y_pred, title):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    \n",
    "    print(f\"{title} - R squared: {r2}, RMSE: {rmse}\")\n",
    "\n",
    "# Function to reverse normalization\n",
    "def reverse_normalization(normalized_values, min_val, max_val):\n",
    "    return normalized_values * (max_val - min_val) + min_val\n",
    "\n",
    "# Function to train the GRU model and predict on test data\n",
    "def train_evaluate_gru(train_df, test_df, target_column='Power(MW)', lookback_steps=8, epochs=100, batch_size=64, site_name='Site', min_val=None, max_val=None):\n",
    "    # Create look-back features for training data\n",
    "    X_train, y_train = create_lookback_features(train_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape data for GRU: [samples, time steps, features]\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2])\n",
    "    \n",
    "    # Build the GRU model\n",
    "    model = Sequential()\n",
    "    model.add(GRU(units=64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                   kernel_regularizer=regularizers.l2(0.001)))  # GRU Layer with L2 regularization\n",
    "    model.add(BatchNormalization())  # BatchNormalization after first GRU\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(GRU(units=32, return_sequences=False, kernel_regularizer=regularizers.l2(0.001)))  # Second GRU Layer\n",
    "    model.add(BatchNormalization())  # BatchNormalization after second GRU\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    # Adding Dense layers with ReLU activation\n",
    "    model.add(Dense(24, activation='relu', kernel_regularizer=regularizers.l2(0.001)))  # Hidden Dense layer with ReLU\n",
    "    model.add(BatchNormalization())  # BatchNormalization after Dense layer\n",
    "    model.add(Dropout(0.2))  # Dropout layer after Dense layer\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))  # Output layer with Linear activation (default for regression)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1, shuffle=False)\n",
    "    \n",
    "    # Create look-back features for test data\n",
    "    X_test, y_test = create_lookback_features(test_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape test data for GRU\n",
    "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2])\n",
    "    \n",
    "    # Predict on the test data\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Reverse normalization on both actual and predicted values\n",
    "    y_test_reversed = reverse_normalization(y_test, min_val, max_val)\n",
    "    y_pred_test_reversed = reverse_normalization(y_pred_test, min_val, max_val)\n",
    "    \n",
    "    # Calculate and print R squared and RMSE for the test set\n",
    "    calculate_metrics(y_test_reversed, y_pred_test_reversed, title=f'GRU Model - {site_name} (Test Data)')\n",
    "\n",
    "    # Define the range of days you want to plot (90th to 100th days)\n",
    "    start_day = 90\n",
    "    end_day = 95\n",
    "    records_per_day = 48  # 24 hours * 2 records per hour (30-minute intervals)\n",
    "\n",
    "    # Calculate the indices corresponding to the 90th and 100th days\n",
    "    start_index = (start_day - 1) * records_per_day  # Start index for 95th day\n",
    "    end_index = end_day * records_per_day  # End index for 100th day\n",
    "\n",
    "    # Slice the test data and predictions for the range from the 90th to 95th day\n",
    "    y_test_range = y_test_reversed[start_index:end_index]\n",
    "    y_pred_test_range = y_pred_test_reversed[start_index:end_index]\n",
    "\n",
    "    # Plot the comparison between actual and predicted values for the 90th to 95th days\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(start_index, end_index), y_test_range, label='Actual', color='blue', linestyle='-', marker='o')\n",
    "    plt.plot(range(start_index, end_index), y_pred_test_range, label='Predicted', color='red', linestyle='--', marker='x')\n",
    "    plt.title(f'Actual vs Predicted Power (GRU: LB=8, Epoch=128, Unit=64-32-24) - {site_name}')\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('Power (MW)')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming your split data frames are named as follows:\n",
    "# (train_resampled_df_50, test_resampled_df_50)\n",
    "\n",
    "# Define the min and max values used for normalization\n",
    "min_train_50 = normalized_train_df_50.min().min()  # Get the minimum value from the training data\n",
    "max_train_50 = normalized_train_df_50.max().max()  # Get the maximum value from the training data\n",
    "\n",
    "# List of split data frames and site names\n",
    "split_resampled_dfs = [\n",
    "    (normalized_train_df_50, normalized_test_df_50, 'Site_50MV')\n",
    "]\n",
    "\n",
    "# Apply the GRU model to each set of split data frames and calculate metrics\n",
    "for i, (train_df, test_df, site_name) in enumerate(split_resampled_dfs):\n",
    "    print(f\"Evaluating GRU model for resampled dataset {site_name} ({i+1})\")\n",
    "    train_evaluate_gru(train_df=train_df,\n",
    "                        test_df=test_df,\n",
    "                        target_column='Power(MW)',\n",
    "                        lookback_steps=8, epochs=128, batch_size=64,\n",
    "                        site_name=site_name,\n",
    "                        min_val=min_train_50,\n",
    "                        max_val=max_train_50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0bce81",
   "metadata": {},
   "source": [
    "# Site_130MW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202d32d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "# Function to create look-back features for time series forecasting\n",
    "def create_lookback_features(df, target_column, lookback_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - lookback_steps):\n",
    "        X.append(df.iloc[i:i + lookback_steps].values)\n",
    "        y.append(df.iloc[i + lookback_steps][target_column])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Function to calculate and print R squared and RMSE\n",
    "def calculate_metrics(y_true, y_pred, title):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    \n",
    "    print(f\"{title} - R squared: {r2}, RMSE: {rmse}\")\n",
    "\n",
    "# Function to reverse normalization\n",
    "def reverse_normalization(normalized_values, min_val, max_val):\n",
    "    return normalized_values * (max_val - min_val) + min_val\n",
    "\n",
    "# Function to train the GRU model and predict on test data\n",
    "def train_evaluate_gru(train_df, test_df, target_column='Power(MW)', lookback_steps=8, epochs=100, batch_size=64, site_name='Site', min_val=None, max_val=None):\n",
    "    # Create look-back features for training data\n",
    "    X_train, y_train = create_lookback_features(train_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape data for GRU: [samples, time steps, features]\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2])\n",
    "    \n",
    "    # Build the GRU model\n",
    "    model = Sequential()\n",
    "    model.add(GRU(units=64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                   kernel_regularizer=regularizers.l2(0.001)))  # GRU Layer with L2 regularization\n",
    "    model.add(BatchNormalization())  # BatchNormalization after first GRU\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(GRU(units=32, return_sequences=False, kernel_regularizer=regularizers.l2(0.001)))  # Second GRU Layer\n",
    "    model.add(BatchNormalization())  # BatchNormalization after second GRU\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    # Adding Dense layers with ReLU activation\n",
    "    model.add(Dense(24, activation='relu', kernel_regularizer=regularizers.l2(0.001)))  # Hidden Dense layer with ReLU\n",
    "    model.add(BatchNormalization())  # BatchNormalization after Dense layer\n",
    "    model.add(Dropout(0.2))  # Dropout layer after Dense layer\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))  # Output layer with Linear activation (default for regression)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1, shuffle=False)\n",
    "    \n",
    "    # Create look-back features for test data\n",
    "    X_test, y_test = create_lookback_features(test_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape test data for GRU\n",
    "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2])\n",
    "    \n",
    "    # Predict on the test data\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Reverse normalization on both actual and predicted values\n",
    "    y_test_reversed = reverse_normalization(y_test, min_val, max_val)\n",
    "    y_pred_test_reversed = reverse_normalization(y_pred_test, min_val, max_val)\n",
    "    \n",
    "    # Calculate and print R squared and RMSE for the test set\n",
    "    calculate_metrics(y_test_reversed, y_pred_test_reversed, title=f'GRU Model - {site_name} (Test Data)')\n",
    "\n",
    "    # Define the range of days you want to plot (90th to 100th days)\n",
    "    start_day = 90\n",
    "    end_day = 95\n",
    "    records_per_day = 48  # 24 hours * 2 records per hour (30-minute intervals)\n",
    "\n",
    "    # Calculate the indices corresponding to the 90th and 100th days\n",
    "    start_index = (start_day - 1) * records_per_day  # Start index for 95th day\n",
    "    end_index = end_day * records_per_day  # End index for 100th day\n",
    "\n",
    "    # Slice the test data and predictions for the range from the 90th to 95th day\n",
    "    y_test_range = y_test_reversed[start_index:end_index]\n",
    "    y_pred_test_range = y_pred_test_reversed[start_index:end_index]\n",
    "\n",
    "    # Plot the comparison between actual and predicted values for the 90th to 95th days\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(start_index, end_index), y_test_range, label='Actual', color='blue', linestyle='-', marker='o')\n",
    "    plt.plot(range(start_index, end_index), y_pred_test_range, label='Predicted', color='red', linestyle='--', marker='x')\n",
    "    plt.title(f'Actual vs Predicted Power (GRU: LB=8, Epoch=128, Unit=64-32-24) - {site_name}')\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('Power (MW)')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming your split data frames are named as follows:\n",
    "# (train_resampled_df_50, test_resampled_df_50)\n",
    "\n",
    "# Define the min and max values used for normalization\n",
    "min_train_130 = normalized_train_df_130.min().min()  # Get the minimum value from the training data\n",
    "max_train_130 = normalized_train_df_130.max().max()  # Get the maximum value from the training data\n",
    "\n",
    "# List of split data frames and site names\n",
    "split_resampled_dfs = [\n",
    "    (normalized_train_df_130, normalized_test_df_130, 'Site_130MV')\n",
    "]\n",
    "\n",
    "# Apply the GRU model to each set of split data frames and calculate metrics\n",
    "for i, (train_df, test_df, site_name) in enumerate(split_resampled_dfs):\n",
    "    print(f\"Evaluating GRU model for resampled dataset {site_name} ({i+1})\")\n",
    "    train_evaluate_gru(train_df=train_df,\n",
    "                        test_df=test_df,\n",
    "                        target_column='Power(MW)',\n",
    "                        lookback_steps=8, epochs=128, batch_size=64,\n",
    "                        site_name=site_name,\n",
    "                        min_val=min_train_130,\n",
    "                        max_val=max_train_130)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5de8c7",
   "metadata": {},
   "source": [
    "# Site_30MW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a66e722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "# Function to create look-back features for time series forecasting\n",
    "def create_lookback_features(df, target_column, lookback_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - lookback_steps):\n",
    "        X.append(df.iloc[i:i + lookback_steps].values)\n",
    "        y.append(df.iloc[i + lookback_steps][target_column])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Function to calculate and print R squared and RMSE\n",
    "def calculate_metrics(y_true, y_pred, title):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    \n",
    "    print(f\"{title} - R squared: {r2}, RMSE: {rmse}\")\n",
    "\n",
    "# Function to reverse normalization\n",
    "def reverse_normalization(normalized_values, min_val, max_val):\n",
    "    return normalized_values * (max_val - min_val) + min_val\n",
    "\n",
    "# Function to train the GRU model and predict on test data\n",
    "def train_evaluate_gru(train_df, test_df, target_column='Power(MW)', lookback_steps=8, epochs=100, batch_size=64, site_name='Site', min_val=None, max_val=None):\n",
    "    # Create look-back features for training data\n",
    "    X_train, y_train = create_lookback_features(train_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape data for GRU: [samples, time steps, features]\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2])\n",
    "    \n",
    "    # Build the GRU model\n",
    "    model = Sequential()\n",
    "    model.add(GRU(units=64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                   kernel_regularizer=regularizers.l2(0.001)))  # GRU Layer with L2 regularization\n",
    "    model.add(BatchNormalization())  # BatchNormalization after first GRU\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(GRU(units=32, return_sequences=False, kernel_regularizer=regularizers.l2(0.001)))  # Second GRU Layer\n",
    "    model.add(BatchNormalization())  # BatchNormalization after second GRU\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    # Adding Dense layers with ReLU activation\n",
    "    model.add(Dense(24, activation='relu', kernel_regularizer=regularizers.l2(0.001)))  # Hidden Dense layer with ReLU\n",
    "    model.add(BatchNormalization())  # BatchNormalization after Dense layer\n",
    "    model.add(Dropout(0.2))  # Dropout layer after Dense layer\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))  # Output layer with Linear activation (default for regression)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1, shuffle=False)\n",
    "    \n",
    "    # Create look-back features for test data\n",
    "    X_test, y_test = create_lookback_features(test_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape test data for GRU\n",
    "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2])\n",
    "    \n",
    "    # Predict on the test data\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Reverse normalization on both actual and predicted values\n",
    "    y_test_reversed = reverse_normalization(y_test, min_val, max_val)\n",
    "    y_pred_test_reversed = reverse_normalization(y_pred_test, min_val, max_val)\n",
    "    \n",
    "    # Calculate and print R squared and RMSE for the test set\n",
    "    calculate_metrics(y_test_reversed, y_pred_test_reversed, title=f'GRU Model - {site_name} (Test Data)')\n",
    "\n",
    "    # Define the range of days you want to plot (90th to 100th days)\n",
    "    start_day = 90\n",
    "    end_day = 95\n",
    "    records_per_day = 48  # 24 hours * 2 records per hour (30-minute intervals)\n",
    "\n",
    "    # Calculate the indices corresponding to the 90th and 100th days\n",
    "    start_index = (start_day - 1) * records_per_day  # Start index for 95th day\n",
    "    end_index = end_day * records_per_day  # End index for 100th day\n",
    "\n",
    "    # Slice the test data and predictions for the range from the 90th to 95th day\n",
    "    y_test_range = y_test_reversed[start_index:end_index]\n",
    "    y_pred_test_range = y_pred_test_reversed[start_index:end_index]\n",
    "\n",
    "    # Plot the comparison between actual and predicted values for the 90th to 95th days\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(start_index, end_index), y_test_range, label='Actual', color='blue', linestyle='-', marker='o')\n",
    "    plt.plot(range(start_index, end_index), y_pred_test_range, label='Predicted', color='red', linestyle='--', marker='x')\n",
    "    plt.title(f'Actual vs Predicted Power (GRU: LB=8, Epoch=128, Unit=64-32-24) - {site_name}')\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('Power (MW)')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming your split data frames are named as follows:\n",
    "# (train_resampled_df_50, test_resampled_df_50)\n",
    "\n",
    "# Define the min and max values used for normalization\n",
    "min_train_30 = normalized_train_df_30.min().min()  # Get the minimum value from the training data\n",
    "max_train_30 = normalized_train_df_30.max().max()  # Get the maximum value from the training data\n",
    "\n",
    "# List of split data frames and site names\n",
    "split_resampled_dfs = [\n",
    "    (normalized_train_df_30, normalized_test_df_30, 'Site_30MV')\n",
    "]\n",
    "\n",
    "# Apply the GRU model to each set of split data frames and calculate metrics\n",
    "for i, (train_df, test_df, site_name) in enumerate(split_resampled_dfs):\n",
    "    print(f\"Evaluating GRU model for resampled dataset {site_name} ({i+1})\")\n",
    "    train_evaluate_gru(train_df=train_df,\n",
    "                        test_df=test_df,\n",
    "                        target_column='Power(MW)',\n",
    "                        lookback_steps=8, epochs=128, batch_size=64,\n",
    "                        site_name=site_name,\n",
    "                        min_val=min_train_30,\n",
    "                        max_val=max_train_30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad391ae",
   "metadata": {},
   "source": [
    "# Site_35MW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0272454a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "# Function to create look-back features for time series forecasting\n",
    "def create_lookback_features(df, target_column, lookback_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - lookback_steps):\n",
    "        X.append(df.iloc[i:i + lookback_steps].values)\n",
    "        y.append(df.iloc[i + lookback_steps][target_column])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Function to calculate and print R squared and RMSE\n",
    "def calculate_metrics(y_true, y_pred, title):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    \n",
    "    print(f\"{title} - R squared: {r2}, RMSE: {rmse}\")\n",
    "\n",
    "# Function to reverse normalization\n",
    "def reverse_normalization(normalized_values, min_val, max_val):\n",
    "    return normalized_values * (max_val - min_val) + min_val\n",
    "\n",
    "# Function to train the GRU model and predict on test data\n",
    "def train_evaluate_gru(train_df, test_df, target_column='Power(MW)', lookback_steps=8, epochs=100, batch_size=64, site_name='Site', min_val=None, max_val=None):\n",
    "    # Create look-back features for training data\n",
    "    X_train, y_train = create_lookback_features(train_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape data for GRU: [samples, time steps, features]\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2])\n",
    "    \n",
    "    # Build the GRU model\n",
    "    model = Sequential()\n",
    "    model.add(GRU(units=64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                   kernel_regularizer=regularizers.l2(0.001)))  # GRU Layer with L2 regularization\n",
    "    model.add(BatchNormalization())  # BatchNormalization after first GRU\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(GRU(units=32, return_sequences=False, kernel_regularizer=regularizers.l2(0.001)))  # Second GRU Layer\n",
    "    model.add(BatchNormalization())  # BatchNormalization after second GRU\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    # Adding Dense layers with ReLU activation\n",
    "    model.add(Dense(24, activation='relu', kernel_regularizer=regularizers.l2(0.001)))  # Hidden Dense layer with ReLU\n",
    "    model.add(BatchNormalization())  # BatchNormalization after Dense layer\n",
    "    model.add(Dropout(0.2))  # Dropout layer after Dense layer\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))  # Output layer with Linear activation (default for regression)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1, shuffle=False)\n",
    "    \n",
    "    # Create look-back features for test data\n",
    "    X_test, y_test = create_lookback_features(test_df, target_column, lookback_steps)\n",
    "    \n",
    "    # Reshape test data for GRU\n",
    "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2])\n",
    "    \n",
    "    # Predict on the test data\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Reverse normalization on both actual and predicted values\n",
    "    y_test_reversed = reverse_normalization(y_test, min_val, max_val)\n",
    "    y_pred_test_reversed = reverse_normalization(y_pred_test, min_val, max_val)\n",
    "    \n",
    "    # Calculate and print R squared and RMSE for the test set\n",
    "    calculate_metrics(y_test_reversed, y_pred_test_reversed, title=f'GRU Model - {site_name} (Test Data)')\n",
    "\n",
    "    # Define the range of days you want to plot (90th to 100th days)\n",
    "    start_day = 90\n",
    "    end_day = 95\n",
    "    records_per_day = 48  # 24 hours * 2 records per hour (30-minute intervals)\n",
    "\n",
    "    # Calculate the indices corresponding to the 90th and 100th days\n",
    "    start_index = (start_day - 1) * records_per_day  # Start index for 95th day\n",
    "    end_index = end_day * records_per_day  # End index for 100th day\n",
    "\n",
    "    # Slice the test data and predictions for the range from the 90th to 95th day\n",
    "    y_test_range = y_test_reversed[start_index:end_index]\n",
    "    y_pred_test_range = y_pred_test_reversed[start_index:end_index]\n",
    "\n",
    "    # Plot the comparison between actual and predicted values for the 90th to 95th days\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(start_index, end_index), y_test_range, label='Actual', color='blue', linestyle='-', marker='o')\n",
    "    plt.plot(range(start_index, end_index), y_pred_test_range, label='Predicted', color='red', linestyle='--', marker='x')\n",
    "    plt.title(f'Actual vs Predicted Power (GRU: LB=8, Epoch=128, Unit=64-32-24) - {site_name}')\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('Power (MW)')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Assuming your split data frames are named as follows:\n",
    "# (train_resampled_df_50, test_resampled_df_50)\n",
    "\n",
    "# Define the min and max values used for normalization\n",
    "min_train_35 = normalized_train_df_35.min().min()  # Get the minimum value from the training data\n",
    "max_train_35 = normalized_train_df_35.max().max()  # Get the maximum value from the training data\n",
    "\n",
    "# List of split data frames and site names\n",
    "split_resampled_dfs = [\n",
    "    (normalized_train_df_35, normalized_test_df_35, 'Site_35MV')\n",
    "]\n",
    "\n",
    "# Apply the GRU model to each set of split data frames and calculate metrics\n",
    "for i, (train_df, test_df, site_name) in enumerate(split_resampled_dfs):\n",
    "    print(f\"Evaluating GRU model for resampled dataset {site_name} ({i+1})\")\n",
    "    train_evaluate_gru(train_df=train_df,\n",
    "                        test_df=test_df,\n",
    "                        target_column='Power(MW)',\n",
    "                        lookback_steps=8, epochs=128, batch_size=64,\n",
    "                        site_name=site_name,\n",
    "                        min_val=min_train_35,\n",
    "                        max_val=max_train_35)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200f07f4",
   "metadata": {},
   "source": [
    "# LSTM Vs GRU On Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d245087f",
   "metadata": {},
   "source": [
    "# Site_50MW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c7adfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Function to create look-back features for time series forecasting\n",
    "def create_lookback_features(df, target_column, lookback_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - lookback_steps):\n",
    "        X.append(df.iloc[i:i + lookback_steps].values)  # Slice features\n",
    "        y.append(df.iloc[i + lookback_steps][target_column])  # Target is the next value\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Function to reverse normalization\n",
    "def reverse_normalization(normalized_values, min_val, max_val):\n",
    "    return normalized_values * (max_val - min_val) + min_val\n",
    "\n",
    "# Function to calculate and print R squared and RMSE\n",
    "def calculate_metrics(y_true_train, y_pred_train, y_true_val, y_pred_val, title, best_epoch=None):\n",
    "    r2_train = r2_score(y_true_train, y_pred_train)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_true_train, y_pred_train))\n",
    "    \n",
    "    r2_val = r2_score(y_true_val, y_pred_val)\n",
    "    rmse_val = np.sqrt(mean_squared_error(y_true_val, y_pred_val))\n",
    "    \n",
    "    print(f\"{title} - Train R squared: {r2_train}, RMSE: {rmse_train}\")\n",
    "    print(f\"{title} - Validation R squared: {r2_val}, RMSE: {rmse_val}\")\n",
    "    \n",
    "    if best_epoch is not None:\n",
    "        print(f\"Best Epoch: {best_epoch}\")\n",
    "\n",
    "# Function to build and train LSTM model\n",
    "def build_train_lstm(X_train, y_train, X_val, y_val, lookback_steps, epochs, batch_size, min_train, max_train):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                   kernel_regularizer=regularizers.l2(0.001)))  # LSTM Layer with L2 regularization\n",
    "    model.add(BatchNormalization())  # BatchNormalization after LSTM\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(LSTM(units=32, return_sequences=False, kernel_regularizer=regularizers.l2(0.001)))  # Second LSTM Layer\n",
    "    model.add(BatchNormalization())  # BatchNormalization after second LSTM\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(Dense(24, activation='relu', kernel_regularizer=regularizers.l2(0.001)))  # Dense layer with ReLU\n",
    "    model.add(Dropout(0.2))  # Dropout layer after Dense layer\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))  # Output layer\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    r2_train_list, rmse_train_list, r2_val_list, rmse_val_list = [], [], [], []\n",
    "    best_epoch_r2, best_epoch_rmse = -np.inf, np.inf\n",
    "    best_epoch_r2_index, best_epoch_rmse_index = None, None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.fit(X_train, y_train, epochs=1, batch_size=batch_size, verbose=0,shuffle=False)\n",
    "        \n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_val = model.predict(X_val)\n",
    "        \n",
    "        # Reverse normalization for both predictions and true values\n",
    "        y_pred_train = reverse_normalization(y_pred_train, min_train, max_train)\n",
    "        y_pred_val = reverse_normalization(y_pred_val, min_train, max_train)\n",
    "        y_train_orig = reverse_normalization(y_train, min_train, max_train)\n",
    "        y_val_orig = reverse_normalization(y_val, min_train, max_train)\n",
    "        \n",
    "        # Calculate R² and RMSE\n",
    "        r2_train = r2_score(y_train_orig, y_pred_train)\n",
    "        rmse_train = np.sqrt(mean_squared_error(y_train_orig, y_pred_train))\n",
    "        \n",
    "        r2_val = r2_score(y_val_orig, y_pred_val)\n",
    "        rmse_val = np.sqrt(mean_squared_error(y_val_orig, y_pred_val))\n",
    "        \n",
    "        r2_train_list.append(r2_train)\n",
    "        rmse_train_list.append(rmse_train)\n",
    "        r2_val_list.append(r2_val)\n",
    "        rmse_val_list.append(rmse_val)\n",
    "        \n",
    "        # Track best epoch for R² (max) and RMSE (min)\n",
    "        if r2_val > best_epoch_r2:\n",
    "            best_epoch_r2 = r2_val\n",
    "            best_epoch_r2_index = epoch\n",
    "        \n",
    "        if rmse_val < best_epoch_rmse:\n",
    "            best_epoch_rmse = rmse_val\n",
    "            best_epoch_rmse_index = epoch\n",
    "    \n",
    "    return r2_train_list, rmse_train_list, r2_val_list, rmse_val_list, best_epoch_r2_index, best_epoch_rmse_index\n",
    "\n",
    "# Function to build and train GRU model\n",
    "def build_train_gru(X_train, y_train, X_val, y_val, lookback_steps, epochs, batch_size, min_train, max_train):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(units=64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                  kernel_regularizer=regularizers.l2(0.001)))  # GRU Layer with L2 regularization\n",
    "    model.add(BatchNormalization())  # BatchNormalization after GRU\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(GRU(units=32, return_sequences=False, kernel_regularizer=regularizers.l2(0.001)))  # Second GRU Layer\n",
    "    model.add(BatchNormalization())  # BatchNormalization after second GRU\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(Dense(24, activation='relu', kernel_regularizer=regularizers.l2(0.001)))  # Dense layer with ReLU\n",
    "    model.add(Dropout(0.2))  # Dropout layer after Dense layer\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))  # Output layer\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    r2_train_list, rmse_train_list, r2_val_list, rmse_val_list = [], [], [], []\n",
    "    best_epoch_r2, best_epoch_rmse = -np.inf, np.inf\n",
    "    best_epoch_r2_index, best_epoch_rmse_index = None, None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.fit(X_train, y_train, epochs=1, batch_size=batch_size, verbose=0,shuffle=False)\n",
    "        \n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_val = model.predict(X_val)\n",
    "        \n",
    "        # Reverse normalization for both predictions and true values\n",
    "        y_pred_train = reverse_normalization(y_pred_train, min_train, max_train)\n",
    "        y_pred_val = reverse_normalization(y_pred_val, min_train, max_train)\n",
    "        y_train_orig = reverse_normalization(y_train, min_train, max_train)\n",
    "        y_val_orig = reverse_normalization(y_val, min_train, max_train)\n",
    "        \n",
    "        # Calculate R² and RMSE\n",
    "        r2_train = r2_score(y_train_orig, y_pred_train)\n",
    "        rmse_train = np.sqrt(mean_squared_error(y_train_orig, y_pred_train))\n",
    "        \n",
    "        r2_val = r2_score(y_val_orig, y_pred_val)\n",
    "        rmse_val = np.sqrt(mean_squared_error(y_val_orig, y_pred_val))\n",
    "        \n",
    "        r2_train_list.append(r2_train)\n",
    "        rmse_train_list.append(rmse_train)\n",
    "        r2_val_list.append(r2_val)\n",
    "        rmse_val_list.append(rmse_val)\n",
    "        \n",
    "        # Track best epoch for R² (max) and RMSE (min)\n",
    "        if r2_val > best_epoch_r2:\n",
    "            best_epoch_r2 = r2_val\n",
    "            best_epoch_r2_index = epoch\n",
    "        \n",
    "        if rmse_val < best_epoch_rmse:\n",
    "            best_epoch_rmse = rmse_val\n",
    "            best_epoch_rmse_index = epoch\n",
    "    \n",
    "    return r2_train_list, rmse_train_list, r2_val_list, rmse_val_list, best_epoch_r2_index, best_epoch_rmse_index\n",
    "\n",
    "# Function to plot metrics for both LSTM and GRU models with different reds for RMSE\n",
    "def plot_metrics(r2_train_lstm, rmse_train_lstm, r2_val_lstm, rmse_val_lstm, \n",
    "                 r2_train_gru, rmse_train_gru, r2_val_gru, rmse_val_gru, \n",
    "                 best_epoch_r2_lstm, best_epoch_rmse_lstm,\n",
    "                 best_epoch_r2_gru, best_epoch_rmse_gru, epochs):\n",
    "    \n",
    "    fig, axs = plt.subplots(2, 1, figsize=(10, 12))\n",
    "    \n",
    "    # Plot R² for train and validation sets (keeping previous colors)\n",
    "    axs[0].plot(range(epochs), r2_train_lstm, label='LSTM Train R²', color='blue', linestyle='-', marker='o')\n",
    "    axs[0].plot(range(epochs), r2_val_lstm, label='LSTM Test R²', color='blue', linestyle='--')\n",
    "    axs[0].plot(range(epochs), r2_train_gru, label='GRU Train R²', color='green', linestyle='-', marker='o')\n",
    "    axs[0].plot(range(epochs), r2_val_gru, label='GRU Test R²', color='green', linestyle='--')\n",
    "    axs[0].set_title('R² for LSTM and GRU Models (LB=8, Epochs=128, Units=64-32-24) - Site_50MV')\n",
    "    axs[0].set_xlabel('Epochs')\n",
    "    axs[0].set_ylabel('R²')\n",
    "    axs[0].legend(loc='lower right')\n",
    "    axs[0].grid(True)\n",
    "    \n",
    "    # Plot RMSE for train and validation sets with different red and orange shades\n",
    "    axs[1].plot(range(epochs), rmse_train_lstm, label='LSTM Train RMSE', color='darkorange', linestyle='-', marker='o')\n",
    "    axs[1].plot(range(epochs), rmse_val_lstm, label='LSTM Test RMSE', color='orangered', linestyle='--')\n",
    "    axs[1].plot(range(epochs), rmse_train_gru, label='GRU Train RMSE', color='darkred', linestyle='-', marker='o')\n",
    "    axs[1].plot(range(epochs), rmse_val_gru, label='GRU Test RMSE', color='firebrick', linestyle='--')\n",
    "    axs[1].set_title('RMSE for LSTM and GRU Models (LB=8, Epochs=128, Units=64-32-24) - Site_50MV')\n",
    "    axs[1].set_xlabel('Epochs')\n",
    "    axs[1].set_ylabel('RMSE')\n",
    "    axs[1].legend(loc='upper right')\n",
    "    axs[1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Load your training and validation data (assuming they are already normalized and available)\n",
    "# Train and validate LSTM model\n",
    "lookback_steps = 8\n",
    "epochs = 128\n",
    "batch_size = 64\n",
    "\n",
    "train_df = normalized_train_df_50 # This is the training DataFrame\n",
    "val_df = normalized_test_df_50 \n",
    "X_train, y_train = create_lookback_features(train_df, 'Power(MW)', lookback_steps)\n",
    "X_val, y_val = create_lookback_features(val_df, 'Power(MW)', lookback_steps)\n",
    "\n",
    "# Normalize features and target for LSTM/GRU\n",
    "min_train, max_train = np.min(y_train), np.max(y_train)\n",
    "X_train = (X_train - X_train.min(axis=0)) / (X_train.max(axis=0) - X_train.min(axis=0))\n",
    "X_val = (X_val - X_val.min(axis=0)) / (X_val.max(axis=0) - X_val.min(axis=0))\n",
    "\n",
    "# Train LSTM\n",
    "r2_train_lstm, rmse_train_lstm, r2_val_lstm, rmse_val_lstm, best_epoch_r2_lstm, best_epoch_rmse_lstm = build_train_lstm(\n",
    "    X_train, y_train, X_val, y_val, lookback_steps, epochs, batch_size, min_train, max_train\n",
    ")\n",
    "\n",
    "# Train GRU\n",
    "r2_train_gru, rmse_train_gru, r2_val_gru, rmse_val_gru, best_epoch_r2_gru, best_epoch_rmse_gru = build_train_gru(\n",
    "    X_train, y_train, X_val, y_val, lookback_steps, epochs, batch_size, min_train, max_train\n",
    ")\n",
    "\n",
    "# Plot metrics for both models\n",
    "plot_metrics(r2_train_lstm, rmse_train_lstm, r2_val_lstm, rmse_val_lstm,\n",
    "             r2_train_gru, rmse_train_gru, r2_val_gru, rmse_val_gru,\n",
    "             best_epoch_r2_lstm, best_epoch_rmse_lstm,\n",
    "             best_epoch_r2_gru, best_epoch_rmse_gru, epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e76d944",
   "metadata": {},
   "source": [
    "# Site_130MW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a51955",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Function to create look-back features for time series forecasting\n",
    "def create_lookback_features(df, target_column, lookback_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - lookback_steps):\n",
    "        X.append(df.iloc[i:i + lookback_steps].values)  # Slice features\n",
    "        y.append(df.iloc[i + lookback_steps][target_column])  # Target is the next value\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Function to reverse normalization\n",
    "def reverse_normalization(normalized_values, min_val, max_val):\n",
    "    return normalized_values * (max_val - min_val) + min_val\n",
    "\n",
    "# Function to calculate and print R squared and RMSE\n",
    "def calculate_metrics(y_true_train, y_pred_train, y_true_val, y_pred_val, title, best_epoch=None):\n",
    "    r2_train = r2_score(y_true_train, y_pred_train)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_true_train, y_pred_train))\n",
    "    \n",
    "    r2_val = r2_score(y_true_val, y_pred_val)\n",
    "    rmse_val = np.sqrt(mean_squared_error(y_true_val, y_pred_val))\n",
    "    \n",
    "    print(f\"{title} - Train R squared: {r2_train}, RMSE: {rmse_train}\")\n",
    "    print(f\"{title} - Validation R squared: {r2_val}, RMSE: {rmse_val}\")\n",
    "    \n",
    "    if best_epoch is not None:\n",
    "        print(f\"Best Epoch: {best_epoch}\")\n",
    "\n",
    "# Function to build and train LSTM model\n",
    "def build_train_lstm(X_train, y_train, X_val, y_val, lookback_steps, epochs, batch_size, min_train, max_train):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                   kernel_regularizer=regularizers.l2(0.001)))  # LSTM Layer with L2 regularization\n",
    "    model.add(BatchNormalization())  # BatchNormalization after LSTM\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(LSTM(units=32, return_sequences=False, kernel_regularizer=regularizers.l2(0.001)))  # Second LSTM Layer\n",
    "    model.add(BatchNormalization())  # BatchNormalization after second LSTM\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(Dense(24, activation='relu', kernel_regularizer=regularizers.l2(0.001)))  # Dense layer with ReLU\n",
    "    model.add(Dropout(0.2))  # Dropout layer after Dense layer\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))  # Output layer\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    r2_train_list, rmse_train_list, r2_val_list, rmse_val_list = [], [], [], []\n",
    "    best_epoch_r2, best_epoch_rmse = -np.inf, np.inf\n",
    "    best_epoch_r2_index, best_epoch_rmse_index = None, None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.fit(X_train, y_train, epochs=1, batch_size=batch_size, verbose=0,shuffle=False)\n",
    "        \n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_val = model.predict(X_val)\n",
    "        \n",
    "        # Reverse normalization for both predictions and true values\n",
    "        y_pred_train = reverse_normalization(y_pred_train, min_train, max_train)\n",
    "        y_pred_val = reverse_normalization(y_pred_val, min_train, max_train)\n",
    "        y_train_orig = reverse_normalization(y_train, min_train, max_train)\n",
    "        y_val_orig = reverse_normalization(y_val, min_train, max_train)\n",
    "        \n",
    "        # Calculate R² and RMSE\n",
    "        r2_train = r2_score(y_train_orig, y_pred_train)\n",
    "        rmse_train = np.sqrt(mean_squared_error(y_train_orig, y_pred_train))\n",
    "        \n",
    "        r2_val = r2_score(y_val_orig, y_pred_val)\n",
    "        rmse_val = np.sqrt(mean_squared_error(y_val_orig, y_pred_val))\n",
    "        \n",
    "        r2_train_list.append(r2_train)\n",
    "        rmse_train_list.append(rmse_train)\n",
    "        r2_val_list.append(r2_val)\n",
    "        rmse_val_list.append(rmse_val)\n",
    "        \n",
    "        # Track best epoch for R² (max) and RMSE (min)\n",
    "        if r2_val > best_epoch_r2:\n",
    "            best_epoch_r2 = r2_val\n",
    "            best_epoch_r2_index = epoch\n",
    "        \n",
    "        if rmse_val < best_epoch_rmse:\n",
    "            best_epoch_rmse = rmse_val\n",
    "            best_epoch_rmse_index = epoch\n",
    "    \n",
    "    return r2_train_list, rmse_train_list, r2_val_list, rmse_val_list, best_epoch_r2_index, best_epoch_rmse_index\n",
    "\n",
    "# Function to build and train GRU model\n",
    "def build_train_gru(X_train, y_train, X_val, y_val, lookback_steps, epochs, batch_size, min_train, max_train):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(units=64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                  kernel_regularizer=regularizers.l2(0.001)))  # GRU Layer with L2 regularization\n",
    "    model.add(BatchNormalization())  # BatchNormalization after GRU\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(GRU(units=32, return_sequences=False, kernel_regularizer=regularizers.l2(0.001)))  # Second GRU Layer\n",
    "    model.add(BatchNormalization())  # BatchNormalization after second GRU\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(Dense(24, activation='relu', kernel_regularizer=regularizers.l2(0.001)))  # Dense layer with ReLU\n",
    "    model.add(Dropout(0.2))  # Dropout layer after Dense layer\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))  # Output layer\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    r2_train_list, rmse_train_list, r2_val_list, rmse_val_list = [], [], [], []\n",
    "    best_epoch_r2, best_epoch_rmse = -np.inf, np.inf\n",
    "    best_epoch_r2_index, best_epoch_rmse_index = None, None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.fit(X_train, y_train, epochs=1, batch_size=batch_size, verbose=0,shuffle=False)\n",
    "        \n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_val = model.predict(X_val)\n",
    "        \n",
    "        # Reverse normalization for both predictions and true values\n",
    "        y_pred_train = reverse_normalization(y_pred_train, min_train, max_train)\n",
    "        y_pred_val = reverse_normalization(y_pred_val, min_train, max_train)\n",
    "        y_train_orig = reverse_normalization(y_train, min_train, max_train)\n",
    "        y_val_orig = reverse_normalization(y_val, min_train, max_train)\n",
    "        \n",
    "        # Calculate R² and RMSE\n",
    "        r2_train = r2_score(y_train_orig, y_pred_train)\n",
    "        rmse_train = np.sqrt(mean_squared_error(y_train_orig, y_pred_train))\n",
    "        \n",
    "        r2_val = r2_score(y_val_orig, y_pred_val)\n",
    "        rmse_val = np.sqrt(mean_squared_error(y_val_orig, y_pred_val))\n",
    "        \n",
    "        r2_train_list.append(r2_train)\n",
    "        rmse_train_list.append(rmse_train)\n",
    "        r2_val_list.append(r2_val)\n",
    "        rmse_val_list.append(rmse_val)\n",
    "        \n",
    "        # Track best epoch for R² (max) and RMSE (min)\n",
    "        if r2_val > best_epoch_r2:\n",
    "            best_epoch_r2 = r2_val\n",
    "            best_epoch_r2_index = epoch\n",
    "        \n",
    "        if rmse_val < best_epoch_rmse:\n",
    "            best_epoch_rmse = rmse_val\n",
    "            best_epoch_rmse_index = epoch\n",
    "    \n",
    "    return r2_train_list, rmse_train_list, r2_val_list, rmse_val_list, best_epoch_r2_index, best_epoch_rmse_index\n",
    "\n",
    "# Function to plot metrics for both LSTM and GRU models with different reds for RMSE\n",
    "def plot_metrics(r2_train_lstm, rmse_train_lstm, r2_val_lstm, rmse_val_lstm, \n",
    "                 r2_train_gru, rmse_train_gru, r2_val_gru, rmse_val_gru, \n",
    "                 best_epoch_r2_lstm, best_epoch_rmse_lstm,\n",
    "                 best_epoch_r2_gru, best_epoch_rmse_gru, epochs):\n",
    "    \n",
    "    fig, axs = plt.subplots(2, 1, figsize=(10, 12))\n",
    "    \n",
    "    # Plot R² for train and validation sets (keeping previous colors)\n",
    "    axs[0].plot(range(epochs), r2_train_lstm, label='LSTM Train R²', color='blue', linestyle='-', marker='o')\n",
    "    axs[0].plot(range(epochs), r2_val_lstm, label='LSTM Test R²', color='blue', linestyle='--')\n",
    "    axs[0].plot(range(epochs), r2_train_gru, label='GRU Train R²', color='green', linestyle='-', marker='o')\n",
    "    axs[0].plot(range(epochs), r2_val_gru, label='GRU Test R²', color='green', linestyle='--')\n",
    "    axs[0].set_title('R² for LSTM and GRU Models (LB=8, Epochs=128, Units=64-32-24) - Site_130MV')\n",
    "    axs[0].set_xlabel('Epochs')\n",
    "    axs[0].set_ylabel('R²')\n",
    "    axs[0].legend(loc='lower right')\n",
    "    axs[0].grid(True)\n",
    "    \n",
    "    # Plot RMSE for train and validation sets with different red and orange shades\n",
    "    axs[1].plot(range(epochs), rmse_train_lstm, label='LSTM Train RMSE', color='darkorange', linestyle='-', marker='o')\n",
    "    axs[1].plot(range(epochs), rmse_val_lstm, label='LSTM Test RMSE', color='orangered', linestyle='--')\n",
    "    axs[1].plot(range(epochs), rmse_train_gru, label='GRU Train RMSE', color='darkred', linestyle='-', marker='o')\n",
    "    axs[1].plot(range(epochs), rmse_val_gru, label='GRU Test RMSE', color='firebrick', linestyle='--')\n",
    "    axs[1].set_title('RMSE for LSTM and GRU Models (LB=8, Epochs=128, Units=64-32-24) - Site_130MV')\n",
    "    axs[1].set_xlabel('Epochs')\n",
    "    axs[1].set_ylabel('RMSE')\n",
    "    axs[1].legend(loc='upper right')\n",
    "    axs[1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Load your training and validation data (assuming they are already normalized and available)\n",
    "# Train and validate LSTM model\n",
    "lookback_steps = 8\n",
    "epochs = 128\n",
    "batch_size = 64\n",
    "\n",
    "train_df = normalized_train_df_130 # This is the training DataFrame\n",
    "val_df = normalized_test_df_130 \n",
    "X_train, y_train = create_lookback_features(train_df, 'Power(MW)', lookback_steps)\n",
    "X_val, y_val = create_lookback_features(val_df, 'Power(MW)', lookback_steps)\n",
    "\n",
    "# Normalize features and target for LSTM/GRU\n",
    "min_train, max_train = np.min(y_train), np.max(y_train)\n",
    "X_train = (X_train - X_train.min(axis=0)) / (X_train.max(axis=0) - X_train.min(axis=0))\n",
    "X_val = (X_val - X_val.min(axis=0)) / (X_val.max(axis=0) - X_val.min(axis=0))\n",
    "\n",
    "# Train LSTM\n",
    "r2_train_lstm, rmse_train_lstm, r2_val_lstm, rmse_val_lstm, best_epoch_r2_lstm, best_epoch_rmse_lstm = build_train_lstm(\n",
    "    X_train, y_train, X_val, y_val, lookback_steps, epochs, batch_size, min_train, max_train\n",
    ")\n",
    "\n",
    "# Train GRU\n",
    "r2_train_gru, rmse_train_gru, r2_val_gru, rmse_val_gru, best_epoch_r2_gru, best_epoch_rmse_gru = build_train_gru(\n",
    "    X_train, y_train, X_val, y_val, lookback_steps, epochs, batch_size, min_train, max_train\n",
    ")\n",
    "\n",
    "# Plot metrics for both models\n",
    "plot_metrics(r2_train_lstm, rmse_train_lstm, r2_val_lstm, rmse_val_lstm,\n",
    "             r2_train_gru, rmse_train_gru, r2_val_gru, rmse_val_gru,\n",
    "             best_epoch_r2_lstm, best_epoch_rmse_lstm,\n",
    "             best_epoch_r2_gru, best_epoch_rmse_gru, epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0a5562",
   "metadata": {},
   "source": [
    "# Site_30MW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e5d9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Function to create look-back features for time series forecasting\n",
    "def create_lookback_features(df, target_column, lookback_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - lookback_steps):\n",
    "        X.append(df.iloc[i:i + lookback_steps].values)  # Slice features\n",
    "        y.append(df.iloc[i + lookback_steps][target_column])  # Target is the next value\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Function to reverse normalization\n",
    "def reverse_normalization(normalized_values, min_val, max_val):\n",
    "    return normalized_values * (max_val - min_val) + min_val\n",
    "\n",
    "# Function to calculate and print R squared and RMSE\n",
    "def calculate_metrics(y_true_train, y_pred_train, y_true_val, y_pred_val, title, best_epoch=None):\n",
    "    r2_train = r2_score(y_true_train, y_pred_train)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_true_train, y_pred_train))\n",
    "    \n",
    "    r2_val = r2_score(y_true_val, y_pred_val)\n",
    "    rmse_val = np.sqrt(mean_squared_error(y_true_val, y_pred_val))\n",
    "    \n",
    "    print(f\"{title} - Train R squared: {r2_train}, RMSE: {rmse_train}\")\n",
    "    print(f\"{title} - Validation R squared: {r2_val}, RMSE: {rmse_val}\")\n",
    "    \n",
    "    if best_epoch is not None:\n",
    "        print(f\"Best Epoch: {best_epoch}\")\n",
    "\n",
    "# Function to build and train LSTM model\n",
    "def build_train_lstm(X_train, y_train, X_val, y_val, lookback_steps, epochs, batch_size, min_train, max_train):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                   kernel_regularizer=regularizers.l2(0.001)))  # LSTM Layer with L2 regularization\n",
    "    model.add(BatchNormalization())  # BatchNormalization after LSTM\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(LSTM(units=32, return_sequences=False, kernel_regularizer=regularizers.l2(0.001)))  # Second LSTM Layer\n",
    "    model.add(BatchNormalization())  # BatchNormalization after second LSTM\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(Dense(24, activation='relu', kernel_regularizer=regularizers.l2(0.001)))  # Dense layer with ReLU\n",
    "    model.add(Dropout(0.2))  # Dropout layer after Dense layer\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))  # Output layer\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    r2_train_list, rmse_train_list, r2_val_list, rmse_val_list = [], [], [], []\n",
    "    best_epoch_r2, best_epoch_rmse = -np.inf, np.inf\n",
    "    best_epoch_r2_index, best_epoch_rmse_index = None, None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.fit(X_train, y_train, epochs=1, batch_size=batch_size, verbose=0,shuffle=False)\n",
    "        \n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_val = model.predict(X_val)\n",
    "        \n",
    "        # Reverse normalization for both predictions and true values\n",
    "        y_pred_train = reverse_normalization(y_pred_train, min_train, max_train)\n",
    "        y_pred_val = reverse_normalization(y_pred_val, min_train, max_train)\n",
    "        y_train_orig = reverse_normalization(y_train, min_train, max_train)\n",
    "        y_val_orig = reverse_normalization(y_val, min_train, max_train)\n",
    "        \n",
    "        # Calculate R² and RMSE\n",
    "        r2_train = r2_score(y_train_orig, y_pred_train)\n",
    "        rmse_train = np.sqrt(mean_squared_error(y_train_orig, y_pred_train))\n",
    "        \n",
    "        r2_val = r2_score(y_val_orig, y_pred_val)\n",
    "        rmse_val = np.sqrt(mean_squared_error(y_val_orig, y_pred_val))\n",
    "        \n",
    "        r2_train_list.append(r2_train)\n",
    "        rmse_train_list.append(rmse_train)\n",
    "        r2_val_list.append(r2_val)\n",
    "        rmse_val_list.append(rmse_val)\n",
    "        \n",
    "        # Track best epoch for R² (max) and RMSE (min)\n",
    "        if r2_val > best_epoch_r2:\n",
    "            best_epoch_r2 = r2_val\n",
    "            best_epoch_r2_index = epoch\n",
    "        \n",
    "        if rmse_val < best_epoch_rmse:\n",
    "            best_epoch_rmse = rmse_val\n",
    "            best_epoch_rmse_index = epoch\n",
    "    \n",
    "    return r2_train_list, rmse_train_list, r2_val_list, rmse_val_list, best_epoch_r2_index, best_epoch_rmse_index\n",
    "\n",
    "# Function to build and train GRU model\n",
    "def build_train_gru(X_train, y_train, X_val, y_val, lookback_steps, epochs, batch_size, min_train, max_train):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(units=64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                  kernel_regularizer=regularizers.l2(0.001)))  # GRU Layer with L2 regularization\n",
    "    model.add(BatchNormalization())  # BatchNormalization after GRU\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(GRU(units=32, return_sequences=False, kernel_regularizer=regularizers.l2(0.001)))  # Second GRU Layer\n",
    "    model.add(BatchNormalization())  # BatchNormalization after second GRU\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(Dense(24, activation='relu', kernel_regularizer=regularizers.l2(0.001)))  # Dense layer with ReLU\n",
    "    model.add(Dropout(0.2))  # Dropout layer after Dense layer\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))  # Output layer\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    r2_train_list, rmse_train_list, r2_val_list, rmse_val_list = [], [], [], []\n",
    "    best_epoch_r2, best_epoch_rmse = -np.inf, np.inf\n",
    "    best_epoch_r2_index, best_epoch_rmse_index = None, None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.fit(X_train, y_train, epochs=1, batch_size=batch_size, verbose=0,shuffle=False)\n",
    "        \n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_val = model.predict(X_val)\n",
    "        \n",
    "        # Reverse normalization for both predictions and true values\n",
    "        y_pred_train = reverse_normalization(y_pred_train, min_train, max_train)\n",
    "        y_pred_val = reverse_normalization(y_pred_val, min_train, max_train)\n",
    "        y_train_orig = reverse_normalization(y_train, min_train, max_train)\n",
    "        y_val_orig = reverse_normalization(y_val, min_train, max_train)\n",
    "        \n",
    "        # Calculate R² and RMSE\n",
    "        r2_train = r2_score(y_train_orig, y_pred_train)\n",
    "        rmse_train = np.sqrt(mean_squared_error(y_train_orig, y_pred_train))\n",
    "        \n",
    "        r2_val = r2_score(y_val_orig, y_pred_val)\n",
    "        rmse_val = np.sqrt(mean_squared_error(y_val_orig, y_pred_val))\n",
    "        \n",
    "        r2_train_list.append(r2_train)\n",
    "        rmse_train_list.append(rmse_train)\n",
    "        r2_val_list.append(r2_val)\n",
    "        rmse_val_list.append(rmse_val)\n",
    "        \n",
    "        # Track best epoch for R² (max) and RMSE (min)\n",
    "        if r2_val > best_epoch_r2:\n",
    "            best_epoch_r2 = r2_val\n",
    "            best_epoch_r2_index = epoch\n",
    "        \n",
    "        if rmse_val < best_epoch_rmse:\n",
    "            best_epoch_rmse = rmse_val\n",
    "            best_epoch_rmse_index = epoch\n",
    "    \n",
    "    return r2_train_list, rmse_train_list, r2_val_list, rmse_val_list, best_epoch_r2_index, best_epoch_rmse_index\n",
    "\n",
    "# Function to plot metrics for both LSTM and GRU models with different reds for RMSE\n",
    "def plot_metrics(r2_train_lstm, rmse_train_lstm, r2_val_lstm, rmse_val_lstm, \n",
    "                 r2_train_gru, rmse_train_gru, r2_val_gru, rmse_val_gru, \n",
    "                 best_epoch_r2_lstm, best_epoch_rmse_lstm,\n",
    "                 best_epoch_r2_gru, best_epoch_rmse_gru, epochs):\n",
    "    \n",
    "    fig, axs = plt.subplots(2, 1, figsize=(10, 12))\n",
    "    \n",
    "    # Plot R² for train and validation sets (keeping previous colors)\n",
    "    axs[0].plot(range(epochs), r2_train_lstm, label='LSTM Train R²', color='blue', linestyle='-', marker='o')\n",
    "    axs[0].plot(range(epochs), r2_val_lstm, label='LSTM Test R²', color='blue', linestyle='--')\n",
    "    axs[0].plot(range(epochs), r2_train_gru, label='GRU Train R²', color='green', linestyle='-', marker='o')\n",
    "    axs[0].plot(range(epochs), r2_val_gru, label='GRU Test R²', color='green', linestyle='--')\n",
    "    axs[0].set_title('R² for LSTM and GRU Models (LB=8, Epochs=128, Units=64-32-24) - Site_30MV')\n",
    "    axs[0].set_xlabel('Epochs')\n",
    "    axs[0].set_ylabel('R²')\n",
    "    axs[0].legend(loc='lower right')\n",
    "    axs[0].grid(True)\n",
    "    \n",
    "    # Plot RMSE for train and validation sets with different red and orange shades\n",
    "    axs[1].plot(range(epochs), rmse_train_lstm, label='LSTM Train RMSE', color='darkorange', linestyle='-', marker='o')\n",
    "    axs[1].plot(range(epochs), rmse_val_lstm, label='LSTM Test RMSE', color='orangered', linestyle='--')\n",
    "    axs[1].plot(range(epochs), rmse_train_gru, label='GRU Train RMSE', color='darkred', linestyle='-', marker='o')\n",
    "    axs[1].plot(range(epochs), rmse_val_gru, label='GRU Test RMSE', color='firebrick', linestyle='--')\n",
    "    axs[1].set_title('RMSE for LSTM and GRU Models (LB=8, Epochs=128, Units=64-32-24) - Site_30MV')\n",
    "    axs[1].set_xlabel('Epochs')\n",
    "    axs[1].set_ylabel('RMSE')\n",
    "    axs[1].legend(loc='upper right')\n",
    "    axs[1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Load your training and validation data (assuming they are already normalized and available)\n",
    "# Train and validate LSTM model\n",
    "lookback_steps = 8\n",
    "epochs = 128\n",
    "batch_size = 64\n",
    "\n",
    "train_df = normalized_train_df_30 # This is the training DataFrame\n",
    "val_df = normalized_test_df_30 \n",
    "X_train, y_train = create_lookback_features(train_df, 'Power(MW)', lookback_steps)\n",
    "X_val, y_val = create_lookback_features(val_df, 'Power(MW)', lookback_steps)\n",
    "\n",
    "# Normalize features and target for LSTM/GRU\n",
    "min_train, max_train = np.min(y_train), np.max(y_train)\n",
    "X_train = (X_train - X_train.min(axis=0)) / (X_train.max(axis=0) - X_train.min(axis=0))\n",
    "X_val = (X_val - X_val.min(axis=0)) / (X_val.max(axis=0) - X_val.min(axis=0))\n",
    "\n",
    "# Train LSTM\n",
    "r2_train_lstm, rmse_train_lstm, r2_val_lstm, rmse_val_lstm, best_epoch_r2_lstm, best_epoch_rmse_lstm = build_train_lstm(\n",
    "    X_train, y_train, X_val, y_val, lookback_steps, epochs, batch_size, min_train, max_train\n",
    ")\n",
    "\n",
    "# Train GRU\n",
    "r2_train_gru, rmse_train_gru, r2_val_gru, rmse_val_gru, best_epoch_r2_gru, best_epoch_rmse_gru = build_train_gru(\n",
    "    X_train, y_train, X_val, y_val, lookback_steps, epochs, batch_size, min_train, max_train\n",
    ")\n",
    "\n",
    "# Plot metrics for both models\n",
    "plot_metrics(r2_train_lstm, rmse_train_lstm, r2_val_lstm, rmse_val_lstm,\n",
    "             r2_train_gru, rmse_train_gru, r2_val_gru, rmse_val_gru,\n",
    "             best_epoch_r2_lstm, best_epoch_rmse_lstm,\n",
    "             best_epoch_r2_gru, best_epoch_rmse_gru, epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7a4916",
   "metadata": {},
   "source": [
    "# Site_35MW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e83551b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Function to create look-back features for time series forecasting\n",
    "def create_lookback_features(df, target_column, lookback_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - lookback_steps):\n",
    "        X.append(df.iloc[i:i + lookback_steps].values)  # Slice features\n",
    "        y.append(df.iloc[i + lookback_steps][target_column])  # Target is the next value\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Function to reverse normalization\n",
    "def reverse_normalization(normalized_values, min_val, max_val):\n",
    "    return normalized_values * (max_val - min_val) + min_val\n",
    "\n",
    "# Function to calculate and print R squared and RMSE\n",
    "def calculate_metrics(y_true_train, y_pred_train, y_true_val, y_pred_val, title, best_epoch=None):\n",
    "    r2_train = r2_score(y_true_train, y_pred_train)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_true_train, y_pred_train))\n",
    "    \n",
    "    r2_val = r2_score(y_true_val, y_pred_val)\n",
    "    rmse_val = np.sqrt(mean_squared_error(y_true_val, y_pred_val))\n",
    "    \n",
    "    print(f\"{title} - Train R squared: {r2_train}, RMSE: {rmse_train}\")\n",
    "    print(f\"{title} - Validation R squared: {r2_val}, RMSE: {rmse_val}\")\n",
    "    \n",
    "    if best_epoch is not None:\n",
    "        print(f\"Best Epoch: {best_epoch}\")\n",
    "\n",
    "# Function to build and train LSTM model\n",
    "def build_train_lstm(X_train, y_train, X_val, y_val, lookback_steps, epochs, batch_size, min_train, max_train):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                   kernel_regularizer=regularizers.l2(0.001)))  # LSTM Layer with L2 regularization\n",
    "    model.add(BatchNormalization())  # BatchNormalization after LSTM\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(LSTM(units=32, return_sequences=False, kernel_regularizer=regularizers.l2(0.001)))  # Second LSTM Layer\n",
    "    model.add(BatchNormalization())  # BatchNormalization after second LSTM\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(Dense(24, activation='relu', kernel_regularizer=regularizers.l2(0.001)))  # Dense layer with ReLU\n",
    "    model.add(Dropout(0.2))  # Dropout layer after Dense layer\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))  # Output layer\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    r2_train_list, rmse_train_list, r2_val_list, rmse_val_list = [], [], [], []\n",
    "    best_epoch_r2, best_epoch_rmse = -np.inf, np.inf\n",
    "    best_epoch_r2_index, best_epoch_rmse_index = None, None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.fit(X_train, y_train, epochs=1, batch_size=batch_size, verbose=0,shuffle=False)\n",
    "        \n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_val = model.predict(X_val)\n",
    "        \n",
    "        # Reverse normalization for both predictions and true values\n",
    "        y_pred_train = reverse_normalization(y_pred_train, min_train, max_train)\n",
    "        y_pred_val = reverse_normalization(y_pred_val, min_train, max_train)\n",
    "        y_train_orig = reverse_normalization(y_train, min_train, max_train)\n",
    "        y_val_orig = reverse_normalization(y_val, min_train, max_train)\n",
    "        \n",
    "        # Calculate R² and RMSE\n",
    "        r2_train = r2_score(y_train_orig, y_pred_train)\n",
    "        rmse_train = np.sqrt(mean_squared_error(y_train_orig, y_pred_train))\n",
    "        \n",
    "        r2_val = r2_score(y_val_orig, y_pred_val)\n",
    "        rmse_val = np.sqrt(mean_squared_error(y_val_orig, y_pred_val))\n",
    "        \n",
    "        r2_train_list.append(r2_train)\n",
    "        rmse_train_list.append(rmse_train)\n",
    "        r2_val_list.append(r2_val)\n",
    "        rmse_val_list.append(rmse_val)\n",
    "        \n",
    "        # Track best epoch for R² (max) and RMSE (min)\n",
    "        if r2_val > best_epoch_r2:\n",
    "            best_epoch_r2 = r2_val\n",
    "            best_epoch_r2_index = epoch\n",
    "        \n",
    "        if rmse_val < best_epoch_rmse:\n",
    "            best_epoch_rmse = rmse_val\n",
    "            best_epoch_rmse_index = epoch\n",
    "    \n",
    "    return r2_train_list, rmse_train_list, r2_val_list, rmse_val_list, best_epoch_r2_index, best_epoch_rmse_index\n",
    "\n",
    "# Function to build and train GRU model\n",
    "def build_train_gru(X_train, y_train, X_val, y_val, lookback_steps, epochs, batch_size, min_train, max_train):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(units=64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                  kernel_regularizer=regularizers.l2(0.001)))  # GRU Layer with L2 regularization\n",
    "    model.add(BatchNormalization())  # BatchNormalization after GRU\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(GRU(units=32, return_sequences=False, kernel_regularizer=regularizers.l2(0.001)))  # Second GRU Layer\n",
    "    model.add(BatchNormalization())  # BatchNormalization after second GRU\n",
    "    model.add(Dropout(0.2))  # Dropout Layer\n",
    "    \n",
    "    model.add(Dense(24, activation='relu', kernel_regularizer=regularizers.l2(0.001)))  # Dense layer with ReLU\n",
    "    model.add(Dropout(0.2))  # Dropout layer after Dense layer\n",
    "    \n",
    "    model.add(Dense(1, activation='linear'))  # Output layer\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    \n",
    "    r2_train_list, rmse_train_list, r2_val_list, rmse_val_list = [], [], [], []\n",
    "    best_epoch_r2, best_epoch_rmse = -np.inf, np.inf\n",
    "    best_epoch_r2_index, best_epoch_rmse_index = None, None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.fit(X_train, y_train, epochs=1, batch_size=batch_size, verbose=0,shuffle=False)\n",
    "        \n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_val = model.predict(X_val)\n",
    "        \n",
    "        # Reverse normalization for both predictions and true values\n",
    "        y_pred_train = reverse_normalization(y_pred_train, min_train, max_train)\n",
    "        y_pred_val = reverse_normalization(y_pred_val, min_train, max_train)\n",
    "        y_train_orig = reverse_normalization(y_train, min_train, max_train)\n",
    "        y_val_orig = reverse_normalization(y_val, min_train, max_train)\n",
    "        \n",
    "        # Calculate R² and RMSE\n",
    "        r2_train = r2_score(y_train_orig, y_pred_train)\n",
    "        rmse_train = np.sqrt(mean_squared_error(y_train_orig, y_pred_train))\n",
    "        \n",
    "        r2_val = r2_score(y_val_orig, y_pred_val)\n",
    "        rmse_val = np.sqrt(mean_squared_error(y_val_orig, y_pred_val))\n",
    "        \n",
    "        r2_train_list.append(r2_train)\n",
    "        rmse_train_list.append(rmse_train)\n",
    "        r2_val_list.append(r2_val)\n",
    "        rmse_val_list.append(rmse_val)\n",
    "        \n",
    "        # Track best epoch for R² (max) and RMSE (min)\n",
    "        if r2_val > best_epoch_r2:\n",
    "            best_epoch_r2 = r2_val\n",
    "            best_epoch_r2_index = epoch\n",
    "        \n",
    "        if rmse_val < best_epoch_rmse:\n",
    "            best_epoch_rmse = rmse_val\n",
    "            best_epoch_rmse_index = epoch\n",
    "    \n",
    "    return r2_train_list, rmse_train_list, r2_val_list, rmse_val_list, best_epoch_r2_index, best_epoch_rmse_index\n",
    "\n",
    "# Function to plot metrics for both LSTM and GRU models with different reds for RMSE\n",
    "def plot_metrics(r2_train_lstm, rmse_train_lstm, r2_val_lstm, rmse_val_lstm, \n",
    "                 r2_train_gru, rmse_train_gru, r2_val_gru, rmse_val_gru, \n",
    "                 best_epoch_r2_lstm, best_epoch_rmse_lstm,\n",
    "                 best_epoch_r2_gru, best_epoch_rmse_gru, epochs):\n",
    "    \n",
    "    fig, axs = plt.subplots(2, 1, figsize=(10, 12))\n",
    "    \n",
    "    # Plot R² for train and validation sets (keeping previous colors)\n",
    "    axs[0].plot(range(epochs), r2_train_lstm, label='LSTM Train R²', color='blue', linestyle='-', marker='o')\n",
    "    axs[0].plot(range(epochs), r2_val_lstm, label='LSTM Test R²', color='blue', linestyle='--')\n",
    "    axs[0].plot(range(epochs), r2_train_gru, label='GRU Train R²', color='green', linestyle='-', marker='o')\n",
    "    axs[0].plot(range(epochs), r2_val_gru, label='GRU Test R²', color='green', linestyle='--')\n",
    "    axs[0].set_title('R² for LSTM and GRU Models (LB=8, Epochs=128, Units=64-32-24) - Site_35MV')\n",
    "    axs[0].set_xlabel('Epochs')\n",
    "    axs[0].set_ylabel('R²')\n",
    "    axs[0].legend(loc='lower right')\n",
    "    axs[0].grid(True)\n",
    "    \n",
    "    # Plot RMSE for train and validation sets with different red and orange shades\n",
    "    axs[1].plot(range(epochs), rmse_train_lstm, label='LSTM Train RMSE', color='darkorange', linestyle='-', marker='o')\n",
    "    axs[1].plot(range(epochs), rmse_val_lstm, label='LSTM Test RMSE', color='orangered', linestyle='--')\n",
    "    axs[1].plot(range(epochs), rmse_train_gru, label='GRU Train RMSE', color='darkred', linestyle='-', marker='o')\n",
    "    axs[1].plot(range(epochs), rmse_val_gru, label='GRU Test RMSE', color='firebrick', linestyle='--')\n",
    "    axs[1].set_title('RMSE for LSTM and GRU Models (LB=8, Epochs=128, Units=64-32-24) - Site_35MV')\n",
    "    axs[1].set_xlabel('Epochs')\n",
    "    axs[1].set_ylabel('RMSE')\n",
    "    axs[1].legend(loc='upper right')\n",
    "    axs[1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Load your training and validation data (assuming they are already normalized and available)\n",
    "# Train and validate LSTM model\n",
    "lookback_steps = 8\n",
    "epochs = 128\n",
    "batch_size = 64\n",
    "\n",
    "train_df = normalized_train_df_35 # This is the training DataFrame\n",
    "val_df = normalized_test_df_35 \n",
    "X_train, y_train = create_lookback_features(train_df, 'Power(MW)', lookback_steps)\n",
    "X_val, y_val = create_lookback_features(val_df, 'Power(MW)', lookback_steps)\n",
    "\n",
    "# Normalize features and target for LSTM/GRU\n",
    "min_train, max_train = np.min(y_train), np.max(y_train)\n",
    "X_train = (X_train - X_train.min(axis=0)) / (X_train.max(axis=0) - X_train.min(axis=0))\n",
    "X_val = (X_val - X_val.min(axis=0)) / (X_val.max(axis=0) - X_val.min(axis=0))\n",
    "\n",
    "# Train LSTM\n",
    "r2_train_lstm, rmse_train_lstm, r2_val_lstm, rmse_val_lstm, best_epoch_r2_lstm, best_epoch_rmse_lstm = build_train_lstm(\n",
    "    X_train, y_train, X_val, y_val, lookback_steps, epochs, batch_size, min_train, max_train\n",
    ")\n",
    "\n",
    "# Train GRU\n",
    "r2_train_gru, rmse_train_gru, r2_val_gru, rmse_val_gru, best_epoch_r2_gru, best_epoch_rmse_gru = build_train_gru(\n",
    "    X_train, y_train, X_val, y_val, lookback_steps, epochs, batch_size, min_train, max_train\n",
    ")\n",
    "\n",
    "# Plot metrics for both models\n",
    "plot_metrics(r2_train_lstm, rmse_train_lstm, r2_val_lstm, rmse_val_lstm,\n",
    "             r2_train_gru, rmse_train_gru, r2_val_gru, rmse_val_gru,\n",
    "             best_epoch_r2_lstm, best_epoch_rmse_lstm,\n",
    "             best_epoch_r2_gru, best_epoch_rmse_gru, epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1271d43f",
   "metadata": {},
   "source": [
    "# Plot R2 for Different Sites In Different Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c67ab2d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data from the table\n",
    "methods = ['LSTM', 'GRU', 'RF']\n",
    "site_50mv_r2 = [0.9413, 0.9444, 0.9760]\n",
    "site_130mv_r2 = [0.9536, 0.9329, 0.9765]\n",
    "site_35mv_r2 = [0.9566, 0.9575, 0.9761]\n",
    "site_30mv_r2 = [0.8770, 0.8270, 0.9762]\n",
    "\n",
    "# Plotting the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(methods, site_50mv_r2, marker='o', label='Site_50MW')\n",
    "plt.plot(methods, site_130mv_r2, marker='o', label='Site_130MW')\n",
    "plt.plot(methods, site_35mv_r2, marker='o', label='Site_35MW')\n",
    "plt.plot(methods, site_30mv_r2, marker='o', label='Site_30MW')\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('R² of Different Models for Each Site')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Test R²')\n",
    "plt.legend()\n",
    "\n",
    "# Save the plot as an image file\n",
    "plt.grid(True)\n",
    "plt.savefig('r2_plot.png')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5650295",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
